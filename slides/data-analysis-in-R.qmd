---
title: "Data Analysis in R"
institute: "Department of Political Science at Georgia State University"
author: "Josh Allen"
date: "2022-10-07"
format: 
 revealjs:
   theme: ["allen.scss"]
knitr:
  opts_chunk:
    dev: "ragg_png"
    retina: 1
    dpi: 200
execute:
  echo: true
  fig-width: 5
  fig-height: 6
---



```{r setup, include = F, warnings = FALSE}



library(AllenMisc)
library(sandwich)
library(lmtest)
library(marginaleffects)
library(emmeans)
library(modelsummary)
library(broom)
library(palmerpenguins)
library(fixest)
library(nullabor)
library(ggfortify)
library(plm)
library(tidyverse)
library(MetBrewer)
library(colorspace)

clrs <- met.brewer(name = "Demuth")
clrs_lt <- lighten(clrs, 0.9)




penguins = palmerpenguins::penguins 


```


# Research Data Services {background-color="`r clrs[3]`" .white}


## Our Team 

```{r echo = FALSE}
knitr::include_graphics("figs/rds-team-members.png")
```

:::{.notes}

 Also, we do not have the capacity to provide pre-scheduled frequent software tutoring that is divorced from a specific assignment or software troubleshooting task at hand -- in other words, we are also not generalized software tutors. 
 
 Our one-on-one software assistance is available to help you troubleshoot specific tasks or assignments. If you are seeking generalized software help, we direct you to our live workshops and our recorded workshops to gain the foundational skills for using various analytical software. Then, when or if the time comes that you have targeted software questions or specific issues to tackle related to a course assignment or research project, please feel free to contact us for one-on-one support. 
:::


## Get Ready Badges


```{r echo = FALSE}
knitr::include_graphics("figs/get-ready-badges.png")
```


:::{.notes}


https://research.library.gsu.edu/dataservices/data-ready

These are awesome to share on social media i.e. linkedin which is a good signal
to potential employers that you know this stuff

:::

## How To Get the Badges

```{r echo = FALSE}
knitr::include_graphics("figs/how-to-badge.png")
```

## Packages You Will Need 


```{r eval=FALSE}
install.packages("pacman")

pacman::p_load("sandwich", "lmtest", "marginaleffects", "emmeans", "modelsummary", "broom",
               "caret", "tidyverse", "fixest", "kableExtra", "nullabor", "plm", "ggfortify", install = TRUE)
set.seed(1994)

penguins = palmerpenguins::penguins 

```




:::{.notes}
Note this workshop is not a workshop that will teach you how these methods work. I presume that most of you will be familiar with linear regression and maximum likelihood estimation. If any of you are here with more of a machine learning background this includes logit and what not. 

I also assume you know the basics of indexing I will go through a bit of it but you should probably know that before doing data analysis 

This workshop is going to sort of through building the results section of a paper. This will generally be geared toward a social science audience. But the fundamentals are virtually the same if you are not. 


:::







# Exploratory Data Analysis in R {background-color="`r clrs[3]`" .white}





## Describing Variables 

- This depends on what kind of variable it is i.e. continuous, categorical etc

- It also depends on what story you need to tell
  - Is this confounder a big deal?
  - Do we see anticipation of treatment?
  - Are there any outliers?
  - etc?
  
- Remember R is just a toolbox. 



:::{.notes}
Exploratory data analysis is the first step of any data analysis pipeline. You need to understand your data in order to understand how to handle things. What does the distribution look like of your outcome variable. If you are like me and ended up taking a lot of causal inference classes and or read a lot of causal inference papers lots of the first parts of papers are just presenting EDA results to convince the reader that the assumptions are met. 

Today we will be asking what is the relationship between bill length and body mass

hopefully that seems like a sensible research question. If not well the goal is just to show you how to generate descriptive statistics in R 
:::


## First Cut


```{r base-summary}
summary(penguins) 

```




:::{.notes}
If we just want like the quickest and dirtiest look at all the variables in our data we can use summary. Functionally this works because palmerpenguins is a small toy dataset. Thankfully for me we can access the raw data that the penguins dataset is based on. This is just the penguins raw function in the palmer penguins packagte. 
:::



## Summary with a bigger data frame 

```{r}
penguins_bigger = palmerpenguins::penguins_raw

summary(penguins_bigger)

```

:::{.notes}
As you can see you can that if you like a somewhat big data frame it can get a little unruly quickly because it is going to spit out a description for every column in your data set. If you want a single variable than you can just the dollar sign to index by name. We can also get each measure we want individually  
:::



## Getting Some Descriptive Statistics 
### The Mean
:::columns
:::column
### Base 

```{r}
mean(penguins$bill_depth_mm, na.rm = TRUE)
```

:::

:::column
### Tidyverse(overkill in this application)

```{r}
summarise(penguins, mean(body_mass_g,
                      na.rm = TRUE))
```

:::
:::




## Quartiles

:::columns 
:::column
### Base 

```{r}
quantile(penguins$bill_depth_mm,
         na.rm = TRUE,
         probs = c(.25, .50, .75))
```

:::

:::column
### Tidyverse(overkill in this instance)
```{r}
penguins |> 
  summarise(quartile_length = quantile(bill_length_mm,
                          na.rm = TRUE,
                          probs = c(.25,
                                    .50,
                                    .75)))



```


:::
:::



## T-tests

::::{.columns}
:::{.column width="45%"}
### Index
```{r eval=FALSE}
penguins_t_test = na.omit(penguins) # can also use na.action in t.test

penguins_t_test$gentoo = ifelse(penguins_t_test$species == "Gentoo", TRUE, FALSE)

t.test(penguins_t_test$body_mass_g,
       penguins_t_test$bill_length_mm,
        var.equal = TRUE)
# this is just to show you 
# some of the options
```

```{r echo = FALSE}
penguins_t_test = na.omit(penguins) 

penguins_t_test$gentoo = ifelse(penguins_t_test$species == "Gentoo", TRUE, FALSE)

t.test(penguins_t_test$body_mass_g, penguins_t_test$bill_length_mm, var.equal = TRUE)  
```

:::

:::{.column width="45%"}

### Formula

```{r eval=FALSE}
penguins_t_test = penguins |> 
  drop_na() |> 
  mutate(gentoo = ifelse(species == "Gentoo",
                         TRUE,
                         FALSE))

t.test(flipper_length_mm ~ gentoo,
     data = penguins_t_test) 
```


```{r echo = FALSE}
penguins_t_test = penguins |> 
  drop_na() |> 
  mutate(gentoo = ifelse(species == "Gentoo",
                         TRUE,
                         FALSE))

t.test(flipper_length_mm ~ gentoo, data = penguins_t_test) 
```


:::
::::

:::{.notes}
In this case we have our first example of using of the formula. To do statsy stuff in R we generally have to use the tilde. This is just a function of how R does statistical analysis. Would and equal sign make sense here sure but like we have to use equal for lots of other stuff.

On the left we are just using the default t.test in with two vectors. R assumes that the variances are unequal and that the it is a two sided test. 


his version of the test is actually the Welch Student’s test, used when the variances of the populations are unknown and unequal
:::


## Correlations 

:::columns
:::column
### Base 

```{r eval=FALSE}

penguins_df = penguins[,c(3:4)]  |> ## lets you do it by column position
  na.omit() 

cor(penguins_df) |> 
  knitr::kable(format = "html")

## this lets you do it by class
 penguins[, sapply(penguins, is.numeric)]  |> 
  na.omit() |> 
   cor() # alternatively you could just specify various corr options 

```


```{r echo=FALSE}
penguins_df = penguins[, c(3:4)] |> 
  na.omit()
  

cor(penguins_df) |> 
  knitr::kable(format = "html")

```


:::

:::column
### Tidyverse 
```{r eval=FALSE}

penguins_df = penguins |> 
  drop_na() |> 
  select(where(is.numeric), -year, - body_mass_g) #### Just cutting out year for presentation 

cor(penguins_df)

```


```{r echo=FALSE}
penguins_df = penguins |> 
  drop_na() |> 
  select(contains("bill")) 

cor(penguins_df) |> 
  knitr::kable(format = "html")
```

:::
:::



## It is Also Important to plot your data! 


<figure>
  <img src="https://raw.githubusercontent.com/andrewheiss/datavizs21.classes.andrewheiss.com/main/static/slides/img/01/DinoSequentialSmaller.gif" alt="Datasaurus Dozen" title="Datasaurus Dozen" width="100%">
  <figcaption><a href="https://www.autodeskresearch.com/publications/samestats" target="_blank">The Datasaurus Dozen</a></figcaption>
</figure>


:::{.notes}
Remember numbers hide lots of things from our ggplot workshop I showed this figure. These all have roughly the same mean and standard deviation 
:::



## Some Basic Graphs 


```{r penguins-bill-length-scatter}
#| output-location: column
ggplot(penguins,
       aes(x = bill_length_mm ,
           y = body_mass_g,
           color = species)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(x = "Bill Length(milimeters)",
       y = "Body mass(grams)") +
  guides(color = guide_legend(title = "Species",
                              override.aes = list(fill = NA))) +
  theme_minimal()
```




## Some Basic Graphs Cont


```{r bill-depth-scatter}
#| output-location: column
ggplot(penguins,
   aes(x = bill_depth_mm,
       y = body_mass_g)) +
  geom_point(aes(color = species)) +
  geom_smooth(aes(color = species),
              method = "lm") +
  geom_smooth(method = "lm") + 
  labs(x = "Bill Depth(milimeters)",
        y = "Body mass(grams)") +
  guides(color = guide_legend(title = "Species",
        override.aes = list(fill = NA))) +
  theme_minimal()

```


:::{.notes}
Notice that when we plot by subgroup we see evidence of simpson's pardox 


Simpson’s Paradox is a statistical phenomenon where an association between two variables in a population emerges, disappears or reverses when the population is divided into subpopulations. For instance, two variables may be positively associated in a population, but be independent or even negatively associated in all subpopulations

In this case the subgroup differences tell a much different story than the overall population. So that is something that we need to be aware of 

:::

## Distributions 


```{r flipper-hist}
#| output-location: column
ggplot(penguins,
  aes(x = flipper_length_mm,
      fill = species)) +
  geom_histogram(alpha = 0.6) +
  labs(x = "Flipper Length(milimeters)") +
  guides(fill = guide_legend(title = "Species")) +
  theme_minimal()
```





## Distributions(cont)


```{r body-mas-density}
#| output-location: column
## you can turn off scientific notion 
# using option(scipen = 999)

ggplot(penguins,
  aes(x = body_mass_g,
     fill = species)) +
  geom_density(alpha = 0.7) +
  labs(x = "Body Mass(grams)") +
  guides(fill = guide_legend(title = "Species")) +
  theme_minimal()


```





## User Written Extensions {.scrollable}


```{r ggdist-out}
#| output-location: column

library(gghalves)
library(MetBrewer)
library(ggdist)

ggplot(penguins,
  aes(x = species,
      y = bill_depth_mm,
     fill = species)) +
  geom_boxplot(
    width = .2, fill = "white",
    size = 1.5, outlier.shape = NA
  ) +
  stat_halfeye(
    adjust = .33,
    width = .67, 
    color = NA,
    position = position_nudge(x = .15)
  ) +
  geom_half_point(
    side = "l", 
    range_scale = .3, 
    alpha = .25, size = 1
  ) +
  scale_fill_met_d(name = "Veronese") +
  labs(x = NULL,
      fill = "Species",
      y = "Bill Depth(millimeters)") +
  coord_flip() +
  theme_minimal()
```




:::{.aside}
Code and example are from Cédric Scherer's [Beyond the Bar plot talk](https://twitter.com/CedScherer/status/1438836874630545412)
:::

## Correlation Plots {.scrollable}


```{r}
#| output-location: column
cor_plot_dat = penguins |>
drop_na() |>
select(where(is.numeric), -year) |>
cor()

cor_plot_dat[lower.tri(cor_plot_dat)] <- NA



long_cor_dat = cor_plot_dat |>
as.data.frame() |>
rownames_to_column("measure2") |>
pivot_longer(cols = -measure2,
             names_to = "measure1",
            values_to = "cor") |>
mutate(nice_cor = round(cor,2)) |>
filter(measure2 != measure1) |>
filter(!is.na(cor)) |>
mutate(measure1 = fct_inorder(measure1),
       measure2 = fct_inorder(measure2))


ggplot(long_cor_dat, 
     aes(x = measure2,
     y = measure1,
     fill = cor)) +
  geom_tile() +
  geom_text(aes(label = nice_cor)) +
  scale_fill_gradient2(low = "#e76254",
       mid = "white",
       high = "#1e466e",
       limits = c(-1, 1)) +
  labs(x = NULL, y = NULL) +
  coord_equal() +
  theme_minimal() +
  theme(panel.grid = element_blank())




```




## Generating Table 1

```{r}
penguins_df = as.data.frame(penguins) |>
  select(-year)

datasummary(All(penguins_df) ~ Mean + SD + Max + Min + Median + Histogram,
            data = penguins, output = "html",
            title = "Descriptive Statistics") 
  

```

## Getting Just A Few Stats

```{r}

datasummary(body_mass_g + bill_length_mm ~ Mean + SD, data = penguins_df, output = "html")


```





## Generating a Balance Table {.scrollable}


```{r out.width="90%"}
datasummary_balance(~gentoo,
                    data = penguins_t_test)
```










# Data Analysis in R {background-color="`r clrs[3]`" .white}



## Some Basics
  
```{r eval = FALSE}
estimator(Outcome ~ IV1 + IV2, data = your_data)

```
 

- If you want to include all the variables in your data set than you can do that with `.`

- Remember R can hold lots of datasets so we have to be explicit with where the data is coming from
  - note that we have several different datasets named `penguins_blah`
  - making sure you have kept track of the using dataset is important







## Univariate Regression 

```{r}
peng_naive = lm(body_mass_g ~ bill_length_mm, data = penguins)

peng_naive
```

:::{.notes}
notice that the output is fairly sparse but in the environment you have a list object in the environment. R creates these as lists because there are tons of things of various classes. So to grab them you can use broom or you can index them.
:::





## 

```{r}
peng_naive = lm(body_mass_g ~ bill_length_mm, data = penguins)

summary(peng_naive)
```

:::{.notes}
This is probably what you are used to seeing if you are coming over from another language 
:::



## Anova 


```{r}

anova_example = aov(body_mass_g ~ species, data = penguins)

#aov(peng_naive) also works

summary(anova_example)

TukeyHSD(anova_example)

```

:::{.notes}
Here we can see that at least one group is different. 

::::


## Multiple Regression 


```{r}
peng_adjust = lm(body_mass_g ~  bill_length_mm + flipper_length_mm + species, data = penguins)

summary(peng_adjust)

```



## Adding Things in The Formula

```{r}
peng_adjust_sq = lm(body_mass_g ~  bill_length_mm + flipper_length_mm + I(bill_depth_mm^2) + species, data = penguins)

summary(peng_adjust_sq)

```

:::{.notes}
The I is the isolates or inhibit function. R will parse forumulas differently than regualr R code. so ^ has a special meaning in forumla. Which can lead to things that you don't anticipate. Normally if you wanted to get the sum of two terms you would use the addition symbol to achieve this. But in the formula syntax you are now adding them as terms to the model

 from the gaze of R's formula parsing code. It allows the standard R operators to work as they would if you used them outside of a formula, rather than being treated as special formula operators.
:::



## Your Turn {background-color="`r clrs_lt[3]`"}


- Estimate a model that looks something like this

```{r eval = FALSE}

lm(flipper_length_mm ~ body_mass_g + other_vars)

```


- Add any other variables

- what happens if you do this in the equation 

```{r eval = FALSE}

lm(flipper_length_mm ~ body_mass_g + log(somevar))

```

- *Bonus*: change the factor levels of species so Gentoo is the reference group
  - hint: use fct_relevel or relevel


```{r echo = FALSE}

countdown::countdown(minutes = 5)

```



:::{.notes}
penguins = penguins |>
mutate(species = fct_relevel(species, "Gentoo", "Chinstrap", "Adelie"))

:::

## Getting Diagnostic Statistics 

:::columns
:::column
```{r echo=FALSE}

peng_sans_miss_base = penguins |> drop_na()

peng_sans_miss_tidy = penguins |> drop_na()
```


### Base R
```{r}

peng_adjust = lm(body_mass_g ~  bill_length_mm + flipper_length_mm + species,
                 data = peng_sans_miss_base)
## remember you need to open the train car door or it will return a listy thing
peng_sans_miss_base$.fitted_vals_brack = peng_adjust[[5]] 

peng_sans_miss_base$.fitted_vals_dollar = peng_adjust$fitted.values

peng_sans_miss_base$.predicted_vals = predict(peng_adjust, interval = "prediction")

peng_sans_miss_base$.residuals_vals = peng_adjust$residuals

peng_sans_miss_base$.studentized_resids = rstudent(peng_adjust)

peng_sans_miss_base$.cooks_distance = cooks.distance(peng_adjust)

```
:::

:::column
### Tidyverse 

```{r }
peng_adjust = lm(body_mass_g ~  bill_length_mm + flipper_length_mm + species,
   data = peng_sans_miss_tidy) 
peng_diag_tidy = augment(peng_adjust,
                         data = peng_sans_miss_tidy)
```


:::
:::


:::{.aside}
I just dropped the missing values to make my life easier
:::

:::{.notes}
As good data analysts we should always check that our assumptions are met. One really good test is to just plot our fitted values against our residuals. So lets go ahead and grab them. In base you just index them via the dollar sign or brackets. In the tidyverse can get them via broom augment. They will be returned with the name of the thing you want starting with a dot. 

The reason is that the team behind broom did not want to override existing stuff
:::


## Plotting {.scrollable}

:::columns 
:::column
### Base 
```{r}
plot(peng_sans_miss_base$.fitted_vals_dollar,
     peng_sans_miss_base$.residuals_vals,
     col = peng_sans_miss_base$species)
abline(lm(peng_sans_miss_base$.residuals_vals ~ peng_sans_miss_base$.fitted_vals_dollar))
```
:::


:::column
### Tidyverse 
```{r}
ggplot(peng_diag_tidy, aes(x = .fitted,
                           y = .resid)) +
  geom_point(aes(color = species)) +
  geom_smooth(method = "lm") +
  theme_minimal()
```

:::
:::




## Checking for Normality {.scrollable}



```{r}
#| layout-ncol: 2
hist(peng_sans_miss_base$.residuals_vals)

ggplot(peng_diag_tidy, aes(x = .resid)) +
  geom_histogram(binwidth = 100) +
  theme_minimal()
```

## Base R diagnostic plots {.scrollable}

```{r eval = FALSE}
par(mfrow = c(2, 2))
plot(peng_adjust)
```

```{r echo = FALSE}
#| layout-ncol: 2
 plot(peng_adjust)
```


## ggplot2 extension for same thing 

```{r}
#| fig-align: center

autoplot(peng_adjust)

```





## Tests 
```{r eval= FALSE}
bptest(peng_adjust)
```

```{r echo=FALSE}
bptest(peng_adjust) |> ## Breush Pagan test for hetroskedasticity
  tidy() |> 
  knitr::kable(format = "html")## 
```


```{r eval=FALSE}
bgtest(peng_adjust)
```

```{r echo=FALSE}
bgtest(peng_adjust) |> ## Breusch-Godfrey test for autocorrelation
  tidy() |> 
  knitr::kable(format = "html")
```

:::{.notes}
null for Breusch Pagan there is no hetrosckedasticity 

fail to reject the null hypothesis. This means that there is no heterosckedasticity. Thats awesome

Null for Breusch Godfrey is there is no serial correlation of order up to 1

We can reject the null hypothesis meaning that there is some autocorrelation in our data. 

Okay it looks like something we have violated some of our assumptions 
:::

## Your Turn(optional) {background-color="`r clrs_lt[3]`"}

- Check the assumptions of your model

- Are the regression assumptions met? 

- Look through the `lmtest` documentation to see what other tests are available.



## Fixing Our Standard Errors

- We can do this "on the fly" in R 

- There are ways to do this in the model formula with various packages 
  - But it is better to do this ["on the fly"](https://grantmcdermott.com/better-way-adjust-SEs/)





## Adjusting Our Standard Errors for Real This time 

```{r}
coeftest(peng_adjust, vcov= vcovHC)
```

:::{.notes}

Using sandwich we can adjust our standard errors like this 
:::


## A Whole Host of Standard Errors 

```{r}
vc <- list(
  "Standard"              = vcov(peng_adjust),
  "Sandwich (basic)"      = sandwich(peng_adjust),
  "Clustered"             = vcovCL(peng_adjust, cluster = ~ species),
  "Clustered (two-way)"   = vcovCL(peng_adjust, cluster = ~ species + year),
  "HC3"                   = vcovHC(peng_adjust),
  "Andrews' kernel HAC"   = kernHAC(peng_adjust),
  "Newey-West"            = NeweyWest(peng_adjust),
  "Bootstrap"             = vcovBS(peng_adjust),
  "Bootstrap (clustered)" = vcovBS(peng_adjust, cluster = ~ species)
)

```


:::columns
:::column 

### Base 

```{r}
adjusted_models = lapply(vc, function(x) coeftest(peng_adjust, vcov = x))
```

:::


:::column

### Tidy

```{r}
map_version = map(vc, ~coeftest(peng_adjust, vcov = .x))
```

:::
:::


:::{.notes}
We can feed a huge list of standard errors fairly quickly it this took virtually no time. Instead of going back and adjusting a huge set of models as would be the case in other softwares where you specify standard error fixes in the model here you can simply specify it once and forget it.

:::

## What does it look like?

```{r echo=FALSE}
#| fig-align: center

modelplot(map_version, coef_omit = c("flipper_length_mm|species|Interc")) +
  coord_flip()


```



## Making Table 2 

- Model Summary is my favorite table making package
  - if you run `modelsummary::supported_models()` you will see it fits most needs
  - alternatively if you don't want to read the list and fit your model than do `modelsummary::get_estimates(model_I_fitted)` it will tell if the output is supported

- Supports a variety of formats including word
  

```{r eval= FALSE}
se_info = tibble(term = "Standard errors","iid", "robust", "bootstrap", "stata", "clustered by sex")
modelsummary(peng_adjust,
             stars = TRUE,
             coef_omit = "(Intercept)|flipper_length_mm|.*species",
             add_rows = se_info,
             vcov =  list("iid", "robust", "bootstrap", "stata", cluster = ~ sex ),
             gof_map = c("nobs", "r.squared"))
```

- As you can see it also lets you adjust your standard errors 
  - You can supply it one kind of standard error
  - Or a List like I did


## The Results 

```{r echo=FALSE}
se_info = tibble(term = "Standard errors","iid", "robust", "bootstrap", "stata", "clustered by sex")
modelsummary(peng_adjust,
             stars = TRUE,
             coef_omit = "(Intercept)|flipper_length_mm|.*species",
             add_rows = se_info,
             vcov =  list("iid", "robust", "bootstrap", "stata", cluster = ~ sex ),
             gof_map = c("nobs", "r.squared"))
```



## Handling Missing Data 

:::incremental
- The default in R is to use listwise deletion 

- This requires some assumptions about why the data is missing

- Sometimes we may need to impute data for missing observations
  - Multiple imputation takes guesses about what would be there using the data
  - Two of the most popular in my home discipline are MICE and AMELIA 

```{r}

mods <- list()

pengs_impute_mice = mice::mice(penguins, m = 5, printFlag = FALSE)

pengs_amelia = Amelia::amelia(as.data.frame(penguins), m = 5, p2s = 0,
                               idvars = c("species", "island", "sex"))$imputations

mods[['List Wise Deletion']] = lm(body_mass_g ~  bill_length_mm + flipper_length_mm + species,
   data = penguins)

mods[['Mice']] = with(pengs_impute_mice,lm(body_mass_g ~ bill_length_mm + flipper_length_mm + species))
mods[['Amelia']] = lapply(pengs_amelia, \(x) lm(body_mass_g ~  bill_length_mm + flipper_length_mm + species, data = x))

mods[['Mice']] <- mice::pool(mods[['Mice']])

mods[['Amelia']] <- mice::pool(mods[['Amelia']])

```

:::

:::{.notes}
Missing data is a fact of life when you work with real data. So we have to deal with it. Just letting R drop it makes very strong assumptions about why the data is missing. Often times we may want to 

:::


## Lets see it in a table 

```{r}
#| echo: false
modelsummary(mods,
             gof_map = c("r.squared", "nobs", "rmse" ))

```



## Interactions 

- To include a multiplicative term we can use `*`, `:` or `/` 

- The constitutive terms will appear automatically with `*` and `/`

- In the case of `/` it returns the marginal effect 
  - As well as the correct standard errors and p-values 

:::{.notes}
The nice part about R is that you can include interactions in a few different ways. One thing is that R will include the constuitive terms for you. The slash will sort of trick R into including the full marginal effects for you. 
:::


```{r}
pengs_interact_stand = lm(body_mass_g ~ bill_length_mm * sex + species + flipper_length_mm,
                           data = peng_sans_miss_tidy)

summary(pengs_interact_stand)
```





## Full Marginal Effects way 

```{r}
pengs_interact_marg = lm(body_mass_g ~ bill_length_mm / factor(sex) + flipper_length_mm + species, data = peng_sans_miss_tidy)

summary(pengs_interact_marg)
```

:::{.notes}
So this will give you the correct marginal effects but if you want the full range of values we need our marginal effects package 

:::

## Getting Marginal Effects for Interactions 


```{r}
#| output-location: column
pengs_interact_effect = lm(body_mass_g ~ bill_length_mm  + flipper_length_mm * sex + species, data = peng_sans_miss_tidy)

plot_cco(
  pengs_interact_effect,
  effect =  "sex", 
  condition = "flipper_length_mm")



```



:::{.notes}
We may be interested in what happens when we shift the values of our continous variable. One awesome way to do this is through our marginal effects package. In this case we can look at the effect as bill_length_mm and the conditonal contrasts. plot cco  plots the contrasts (y-axis) against values of predictor(s) variable(s) (x-axis and colors). 

essentially what happens when we have an increase of some value


 So in this case this is just showing the effect when penguins take on the value of male and flipper length in increases by x amount mm


:::

## Interflex 

```{r results = "hide", warning = FALSE}
#| output-location: column

library(interflex)

peng_interflex = as.data.frame(peng_sans_miss_tidy)


interflex(Y = "body_mass_g",
          D = "sex",
          X = "flipper_length_mm",
          Z = c("bill_length_mm", "species"),
         estimator = "kernel",
         data = peng_interflex,
         theme.bw = TRUE,
         parallel = FALSE) # recommend that you set this to true


```

:::{.notes}
Interflex is also a package that I would be remiss not to bring attention to. It is by a group of political scientists. This implements some tests to ensure that there is a linear interaction effects and whether there is common support. Basically if you start to plug in values your self across the full range of flipper lenghh you may be calculating effects where there is not a ton of data making those estimates super fragile. I would probably opt for interflex in this case but I think that is just a discipline specific thing 


The kernal estimator is what they reccommend. It is more flexible because we are not imposing a strong parametric form of the relationship. It is also nice because we include any confounders. So it looks like when we increase our flipper length in mm to about 190 mm the body mass of the penguins increase to about 400 g 

:::

## Tables with multiple models 

```{r}
modelsummary(list(peng_naive, peng_adjust, pengs_interact_stand),
             stars = TRUE,
             gof_omit = ".*", ## omitting all goodness of fit for space
             coef_map = c("bill_length_mm" = "Bill Length(mm)",
                          "flipper_length_mm" = "Flipper Length(mm)",
                          "bill_length_mm:sexmale" = "Bill Length(mm) x Male Penguin"),
             vcov = "robust",
             note = "Robust Standard errors in Parenthsis")

```




## Fancy Tables 

- Often times we have multiple dv's 

- A popular strategy in Pols and Econ is to make a table with two panels
  - Where outcome 1 is in one panel and outcome two is in the bottom panel

```{r}
#| eval: false
gm <- c("r.squared", "nobs", "rmse")

panels <- list(
  list(
    lm(body_mass_g ~ bill_length_mm + species, data = peng_sans_miss_tidy),
    lm(body_mass_g ~ bill_length_mm, data = peng_sans_miss_tidy)
  ),
  list(
    lm(flipper_length_mm ~ species + bill_length_mm, data = peng_sans_miss_tidy),
    lm(flipper_length_mm ~ species + bill_length_mm + bill_depth_mm, data = peng_sans_miss_tidy)
  )
)

modelsummary(
  panels,
  shape = "rbind",
  gof_map = gm)

```

## Output{.scrollable}

```{r}
#| echo: false
gm <- c("r.squared", "nobs", "rmse")

panels <- list(
  list(
    lm(body_mass_g ~ bill_length_mm + species, data = peng_sans_miss_tidy),
    lm(body_mass_g ~ bill_length_mm, data = peng_sans_miss_tidy)
  ),
  list(
    lm(flipper_length_mm ~ species + bill_length_mm, data = peng_sans_miss_tidy),
    lm(flipper_length_mm ~ species + bill_length_mm + bill_depth_mm, data = peng_sans_miss_tidy)
  )
)

modelsummary(
  panels,
  shape = "rbind",
  gof_map = gm)
```


## Fixed Effects

- Mega popular estimation strategy in Econ and Political Science

```{r eval = FALSE}
library(plm)
library(vdemdata)

vdem_feols = feols(v2x_polyarchy ~ v2x_gender + v2x_corr | year + country_id, data = vdem)

vdem_stand = lm(v2x_polyarchy ~ v2x_gender + v2x_corr + factor(year) + factor(country_id), data = vdem)

vdem_plm = plm(v2x_polyarchy ~ v2x_gender + v2x_corr + factor(year), 
  data = vdem,
  index = c("country_id"),
  model = "within")
```


:::{.notes}
There are more than a few ways to get a fixed effects regression in R my prefered way is using fixest. Because a species and island fixed effect would be a bit ridicoulous we can use star wars to demonstrate our point a bit. Feols does this in a really intuitive way. All your fixed effectgs are behi

To sort of get on with the rest of the workshop I will not spend to much time on this.  But it is stupid fast and lets you do a whole lot.
It does not have random effects tho. PLM is really useful for lots of other common panel estimators. But fixest is still the winner when it comes to]
fixed effects models.

::::


## Performance

- Vdem has 202 country ids and 233 years 
  - Lets time them to see 

```{r}
#| echo: false

library(microbenchmark)

vdem  = vdemdata::vdem

times_fix = microbenchmark(vdem_feols = feols(v2x_polyarchy ~ v2x_gender + v2x_corr | year + country_id, data = vdem),

vdem_stand = lm(v2x_polyarchy ~ v2x_gender + v2x_corr + factor(year) + factor(country_id), data = vdem),

vdem_plm = plm(v2x_polyarchy ~ v2x_gender + v2x_corr + factor(year), 
  data = vdem,
  index = c("country_id"),
  model = "within"), times = 10)



```

```{r}
#| echo: false
times_fix |>
group_by(expr) |>
summarise(`Mean Time in Seconds` = mean(time)/6e9)



```


## Generating Predictions 



```{r}
set.seed(1994)

penguins_prediction = mutate(penguins, id = row_number()) |> 
drop_na()

peng_train = penguins_prediction |> 
  sample_frac(0.7) # sample 70% of the dataset 

peng_test = anti_join(penguins_prediction, peng_train, by = "id")

peng_test$data_set = "Test" # add an indicator for what data set we have

penguins_training_model = lm(body_mass_g ~ bill_length_mm, data = peng_train)

penguins_train_plot = augment(penguins_training_model, 
                      interval = "prediction") |>
                      select(.fitted, .lower, .upper, body_mass_g, bill_length_mm) |>
                      mutate(data_set = "train")

# alterntively this works
# predict(penguins_train_plot, newdata = peng_test)

peng_predict_test = augment(penguins_training_model,
                           interval = "prediction",
                           newdata = peng_test) |> 
 select(.fitted, .lower, .upper,  body_mass_g, bill_length_mm, data_set)

all_together = rbind(penguins_train_plot,
                    peng_predict_test)
```

:::{.notes}

 It involves randomly dividing the available set of observations into two parts, a training set and a validation set or hold-out set. The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. The resulting validation set error rate — typically assessed using MSE in the case of a quantitative response—provides an estimate of the test error rate

:::

##  Visual Inspection


```{r }
#| output-location: column


ggplot(all_together,
  aes(x = bill_length_mm,
    y = body_mass_g,
   color = data_set,
    fill = data_set)) +
  geom_point(alpha = 0.7) +
  geom_line(aes(y = .fitted)) +
  geom_ribbon(aes(ymin = .lower,
             ymax = .upper),
               alpha = 0.3,
               col = NA) +
  scale_color_discrete(name = "Training sample?",
                       aesthetics = c("color",
                                      "fill")) +
  theme_minimal()
```




## ~~Maximum Likelihood Estimation~~ <br> Machine Learning </br>

- R comes with a whole host of maximum likelihood estimators(MLE)
  - As well as user written packages 
  
- You will also probably want to load `marginaleffects` to get marginal effects of your model
  - Remember the coefficients of a MLE model are not directly interpretable

- `emmeans` is also a solid package for getting marginal effects

- Note: They do differ on what they can do and how they generate marginal effects
  - Andrew Heiss has a nice summary of this [at the bottom of this blog post](https://www.andrewheiss.com/blog/2022/05/20/marginalia/##tldr-overall-summary-of-all-these-marginal-effects-approaches)





## The data we are using 

```{r}
penguins_glm = penguins |> 
  drop_na() |> 
  mutate(big_penguin = ifelse(body_mass_g > median(body_mass_g), 1,0),
         big_penguin_logic = ifelse(body_mass_g > median(body_mass_g), TRUE, FALSE),
         female = ifelse(sex == "female", TRUE, FALSE))


```







## Lets See What this looks like 


```{r }
#| output-location: column

ggplot(penguins_glm,
    aes(x = bill_length_mm,
       y = big_penguin)) +
  stat_dots(aes(side = ifelse(big_penguin == 0,
       "top",
       "bottom")),
       scale = 1/3) +
  geom_smooth(method = "glm",
  method.args = list(family = binomial(link = "logit"))) +
  theme_minimal()
```






## Our Model 
```{r}
base_model = glm(big_penguin ~ bill_length_mm + flipper_length_mm + species + female,
                 data = penguins_glm,
                 family = binomial(link = "logit"))

summary(base_model)
```







## Getting Interpretable Quantities 

```{r }
#| output-location: column

# get odds ratio

odds_ration = exp(coef(base_model))

avg_marginal_effect = base_model |> 
  marginaleffects()  |> 
  tidy()

ggplot(data = filter(avg_marginal_effect,
       !term == "species"),
       aes(x = estimate,
           y = str_to_title(term))) +
  geom_pointrange(aes(xmin = conf.low,
                     xmax = conf.high)) +
  geom_vline(xintercept = 0) +
  labs(y = NULL,
       x = "Average Marginal Effect") +
  theme_minimal()


```

:::{.notes}
Since our coefficients in our logit model are uninterpretable we need to get some interpretable quantities. We can either use odds ratios by just doing exponeniate. 

This is fine if your discipline does this. Political scientists especially prefer using average marginal effects. 

Under the hood what is going on is that marginal effects is going to plug in the each row into the model and generate predictions and calculates the slope. Then will basically average across them. So the effect is basically just the average across the fitted results.

:::

## User Specified Values

:::columns
:::column

### emmeans
```{r eval=FALSE}

predictions_emmeans = base_model |> 
 emmeans(~ bill_length_mm + species, var = "bill_length_mm",
                   at = list(bill_length_mm = seq(0, 60,1)),
                   regrid = "response", delta = 0.001) |> 
  as_tibble()

colors_plot = c("#f0be3d", "#931e18", "#247d3f")

ggplot(predictions_emmeans, aes(x = bill_length_mm, y = prob, color = species)) +
  geom_line() + 
  labs(x = "Bill Length(millimeters)", y = "Predicted Probablity of Being a Big Penguin",
       color = NULL) +
  theme(legend.position = "bottom") +
  theme_minimal() +
  scale_color_manual(values = colors_plot)
  




```
:::

:::column

###  Marginal Effects

```{r eval=FALSE}

base_model_use = base_model |>
predictions(newdata = datagrid(bill_length_mm = c(0,60)))


p  = plot_cap(base_model, condition = c("bill_length_mm", "species"))## can be saved and changed with ggplot options 

p + 
  labs(x = "Bill Length(millimeters)", y = "Predicted Probablity of Being a Big Penguin",
       color = NULL, fill = NULL,
       caption = "By Default plot_cap plots confidence intervals \nso you have you make them really small") +
  scale_x_continuous(breaks = seq(0,60, 20)) +
  theme(legend.position = "bottom") +
  theme_minimal() +
  scale_color_manual(values = colors_plot)
```
:::
:::





## Results

::::{.columns}
:::{.column width="45%"}
```{r echo=FALSE}

predictions_emmeans = base_model |> 
  emmeans(~ bill_length_mm + species, var = "bill_length_mm",
                   at = list(bill_length_mm = seq(0, 60,1)),
                   regrid = "response", delta = 0.001) |> 
  as_tibble()

colors_plot = c("#f0be3d", "#931e18", "#247d3f")

ggplot(predictions_emmeans, aes(x = bill_length_mm, y = prob, color = species)) +
  geom_line() + 
  labs(x = "Bill Length(millimeters)", y = "Predicted Probablity of Being a Big Penguin",
       color = NULL) +
  theme(legend.position = "bottom") +
  theme_minimal() +
  scale_color_manual(values = colors_plot)
  




```
:::

:::{.column width="45%"}
```{r echo=FALSE}
colors_plot = c("#f0be3d", "#931e18", "#247d3f")

p  = plot_cap(base_model,
   condition = c("bill_length_mm", "species"),
    conf_level = 0.01) ## can be saved and changed with ggplot options 

p + 
  labs(x = "Bill Length(millimeters)",
       y = "Predicted Probability of Being a Big Penguin",
       color = NULL, fill = NULL,
       caption = "By Default plot_cap plots confidence intervals \nso you have you make them really small") +
  theme_minimal() +
  scale_color_manual(values = colors_plot)
```
:::
::::







#  Machine Learning(basics) {background-color="`r clrs[3]`" .white}


## Packages You Will Need 


```{r}
set.seed(1994) # make things reproducible
library(ISLR2)
library(caret)
library(leaps)
library(mlbench)
library(tidymodels)
library(vip)

```



- Note caret and tidymodels have some namespace conflicts. 
    - I personally would prefer to load one or the other


:::{.callout-important}
The Elements of Statistical Learning has all the R and Python code [located at this website](https://www.statlearning.com/resources-second-edition). 

:::

:::{.notes}
I generally am not a machine learning person. So I am just going to show you the basics of a machine learning workflow in R.

:::



## Creating Training and Test Sets 


:::columns 
:::column 
### Caret 
```{r}

# Preprocess data  
peng =  penguins |> drop_na()

peng_caret = as.data.frame(peng)  

train_index_caret = caret::createDataPartition(peng_caret$body_mass_g, p = 0.7, list = FALSE, times = 1)

train_caret = peng_caret[train_index_caret,]

test_caret = peng_caret[-train_index_caret,]

```


:::


:::column

### Tidymodels 

```{r}

peng_models =  rsample::initial_split(peng, prop = 0.7)

peng_tidy_train = rsample::training(peng_models)

peng_tidy_test = rsample::testing(peng_models)


```

:::
:::



## Create Cross Validations


:::columns
:::column 

### Caret 



```{r}


peng_caret_cv = trainControl(method = "cv",
      number = 10)

```


:::
:::column 

### Tidymodels

```{r}
peng_cv_tidy = vfold_cv(peng_tidy_train, v = 10, strata = body_mass_g)
```

:::
:::


##  Fit a lasso regression





```{r}

lasso_caret = train(body_mass_g ~ .,
                 data = train_caret,
                 method = "lasso",
                 trControl = peng_caret_cv)



tuning_grid_caret = data.frame(.fraction = 10^seq(-2, -1, length.out = 10))



lasso_caret_tune = train(body_mass_g ~ .,
                 data = train_caret,
                 method = "lasso",
                 trControl = peng_caret_cv,
                 tuneGrid = tuning_grid_caret)

predictions_caret = predict(lasso_caret_tune, newdata = test_caret)


```








:::{.notes}
All in all machine learning in R works. I just have not done it all that much. I have mostly done strucutural topic mo


:::

## Lasso Regression in Tidymodels 


```{r}


penguins_rec = recipe(body_mass_g ~ ., data = peng_tidy_test)  |>
step_center(all_predictors(), -all_nominal()) |>
step_dummy(all_nominal())

lasso_mod = linear_reg(mode = "regression",
                       penalty = tune(),
                       mixture = 1) |>
            set_engine("glmnet") # other package that fits lasso

wf = workflow() |>
add_model(lasso_mod) |>
add_recipe(penguins_rec)


my_grid = tibble(penalty = 10^seq(-2, -1, length.out = 10))


my_res = wf |>
tune_grid(resamples = peng_cv_tidy,
          grid = my_grid,
          control = control_grid(verbose = FALSE, save_pred = TRUE),
          metrics = metric_set(rmse))



good_mod = my_res |> select_best("rmse", maximize = FALSE)


final_fit = finalize_workflow(wf, good_mod) |>
fit(data = peng_tidy_train)


predict(final_fit, peng_tidy_test)

```

:::{.notes }
recipes, which is designed to help you preprocess your data before training your model. Recipes are built as a series of preprocessing steps, such as:

defines a model that can predict numeric values from predictors using a linear function. This function can fit regression models

 is used to specify which package or system will be used to fit the model, along with any arguments specific to that software.


 A workflow is a container object that aggregates information required to fit and predict from a model. 


 specifies the terms of the model and any preprocessing that is required through the usage of a recipe.



:::

## Variable Importance Plot 



```{r}
#| output-location: column
final_fit |>
fit(peng_tidy_train) |>
extract_fit_parsnip() |>
vi(lambda = good_mod$penalty) |>
mutate(
    Importance = abs(Importance),
    Variable = fct_reorder(Variable, Importance)
  ) |>
ggplot(aes(x = Importance,
          y = Variable,
         fill = Sign)) +
geom_col() +
scale_x_continuous(expand = c(0,0)) + 
labs(y = NULL) +
theme_minimal()

```





