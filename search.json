[
  {
    "objectID": "supplemental/log-transformations.html",
    "href": "supplemental/log-transformations.html",
    "title": "Log Transformations in Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides details about the model interpretation when the predictor and/or response variables are log-transformed. For simplicity, we will discuss transformations for the simple linear regression model as shown in Equation 1.\n\\[\n\\label{orig}\ny = \\beta_0 + \\beta_1 x\n\\tag{1}\\]\nAll results and interpretations can be easily extended to transformations in multiple regression models.\nNote: log refers to the natural logarithm."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the response variable",
    "text": "Log-transformation on the response variable\nSuppose we fit a linear regression model with \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(x\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 x, \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(x\\) and \\(\\log(y)\\) using the model in Equation 2.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 x\n\\tag{2}\\]\nIf we interpret the model in terms of \\(\\log(y)\\), then we can use the usual interpretations for slope and intercept. When reporting results, however, it is best to give all interpretations in terms of the original response variable \\(y\\), since interpretations using log-transformed variables are often more difficult to truly understand.\nIn order to get back on the original scale, we need to use the exponential function (also known as the anti-log), \\(\\exp\\{x\\} = e^x\\). Therefore, we use the model in Equation 2 for interpretations and predictions, we will use Equation 3 to state our conclusions in terms of \\(y\\).\n\\[\n\\begin{aligned}\n&\\exp\\{\\log(y)\\} = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\n\\end{aligned}\n\\tag{3}\\]\nIn order to interpret the slope and intercept, we need to first understand the relationship between the mean, median and log transformations.\n\nMean, Median, and Log Transformations\nSuppose we have a dataset y that contains the following observations:\n\n\n[1] 3 5 6 7 8\n\n\nIf we log-transform the values of y then calculate the mean and median, we have\n\n\n\n\n\nmean_log_y\nmedian_log_y\n\n\n\n\n1.70503\n1.79176\n\n\n\n\n\nIf we calculate the mean and median of y, then log-transform the mean and median, we have\n\n\n\n\n\nlog_mean\nlog_median\n\n\n\n\n1.75786\n1.79176\n\n\n\n\n\nThis is a simple illustration to show\n\n\\(\\text{Mean}[{\\log(y)}] \\neq \\log[\\text{Mean}(y)]\\) - the mean and log are not commutable\n\\(\\text{Median}[\\log(y)] = \\log[\\text{Median}(y)]\\) - the median and log are commutable\n\n\n\nInterpretaton of model coefficients\nUsing Equation 2, the mean \\(\\log(y)\\) for any given value of \\(x\\) is \\(\\beta_0 + \\beta_1 x\\); however, this does not indicate that the mean of \\(y = \\exp\\{\\beta_0 + \\beta_1 x\\}\\) (see previous section). From the assumptions of linear regression, we assume that for any given value of \\(x\\), the distribution of \\(\\log(y)\\) is Normal, and therefore symmetric. Thus the median of \\(\\log(y)\\) is equal to the mean of \\(\\log(y)\\), i.e \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x\\).\nSince the log and the median are commutable, \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x \\Rightarrow \\text{Median}(y) = \\exp\\{\\beta_0 + \\beta_1 x\\}\\). Thus, when we log-transform the response variable, the interpretation of the intercept and slope are in terms of the effect on the median of \\(y\\).\nIntercept: The intercept is expected median of \\(y\\) when the predictor variable equals 0. Therefore, when \\(x=0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x=0\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is the expected change in the median of \\(y\\) when \\(x\\) increases by 1 unit. The change in the median of \\(y\\) is\n\\[\n\\exp\\{[\\beta_0 + \\beta_1 (x+1)] - [\\beta_0 + \\beta_1 x]\\} = \\frac{\\exp\\{\\beta_0 + \\beta_1 (x+1)\\}}{\\exp\\{\\beta_0 + \\beta_1 x\\}} = \\frac{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\\exp\\{\\beta_1\\}}{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}} = \\exp\\{\\beta_1\\}\n\\]\nThus, the median of \\(y\\) for \\(x+1\\) is \\(\\exp\\{\\beta_1\\}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) increases by one unit, the median of \\(y\\) is expected to multiply by a factor of \\(\\exp\\{\\beta_1\\}\\)."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the predictor variable",
    "text": "Log-transformation on the predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(y\\), such that \\(y \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(y\\) using the model in #eq-log-x.\n\\[\ny = \\beta_0 + \\beta_1 \\log(x)\n\\tag{4}\\]\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\).\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the mean of \\(y\\) is expected to be \\(\\beta_0\\).\nSlope: The slope is interpreted in terms of the change in the mean of \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the mean of \\(y\\) is\n\\[\n\\begin{aligned}\n[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)] &= \\beta_1 [\\log(Cx) - \\log(x)] \\\\[10pt]\n& = \\beta_1[\\log(C) + \\log(x) - \\log(x)] \\\\[10pt]\n& = \\beta_1 \\log(C)\n\\end{aligned}\n\\]\nThus the mean of \\(y\\) changes by \\(\\beta_1 \\log(C)\\) units.\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(C)\\) units. For example, if \\(x\\) is doubled, then the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(2)\\) units."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the the response and predictor variable",
    "text": "Log-transformation on the the response and predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable and \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(\\log(y)\\) using the model in Equation 5.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 \\log(x)\n\\tag{5}\\]\nBecause the response variable is log-transformed, the interpretations on the original scale will be in terms of the median of \\(y\\) (see the section on the log-transformed response variable for more detail).\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\). Therefore, when \\(\\log(x) = 0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is interpreted in terms of the change in the median \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the median of \\(y\\) is\n\\[\n\\begin{aligned}\n\\exp\\{[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)]\\} &=\n\\exp\\{\\beta_1 [\\log(Cx) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1[\\log(C) + \\log(x) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1 \\log(C)\\} = C^{\\beta_1}\n\\end{aligned}\n\\]\nThus, the median of \\(y\\) for \\(Cx\\) is \\(C^{\\beta_1}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the median of \\(y\\) is expected to multiple by a factor of \\(C^{\\beta_1}\\). For example, if \\(x\\) is doubled, then the median of \\(y\\) is expected to multiply by \\(2^{\\beta_1}\\)."
  },
  {
    "objectID": "supplemental/slr-derivations.html",
    "href": "supplemental/slr-derivations.html",
    "title": "Deriving the Least-Squares Estimates for Simple Linear Regression",
    "section": "",
    "text": "This document contains the mathematical details for deriving the least-squares estimates for slope (\\(\\beta_1\\)) and intercept (\\(\\beta_0\\)). We obtain the estimates, \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) by finding the values that minimize the sum of squared residuals, as shown in Equation 1.\n\\[\nSSR = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 = [y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i]^2\n\\tag{1}\\]\nRecall that we can find the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize /eq-ssr by taking the partial derivatives of Equation 1 and setting them to 0. Thus, the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize the respective partial derivative also minimize the sum of squared residuals. The partial derivatives are shown in Equation 2.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} &= -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)  \\\\\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\n\\end{aligned}\n\\tag{2}\\]\nThe derivation of deriving \\(\\hat{\\beta}_0\\) is shown in Equation 3.\n\\[\n\\begin{aligned}\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}(y_i + \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow - \\sum\\limits_{i=1}^{n}y_i + n\\hat{\\beta}_0 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i = 0 \\\\&\\Rightarrow n\\hat{\\beta}_0  = \\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i \\\\&\\Rightarrow \\hat{\\beta}_0  = \\frac{1}{n}\\Big(\\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i\\Big)\\\\&\\Rightarrow \\hat{\\beta}_0  = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\\\end{aligned}\n\\tag{3}\\]\nThe derivation of \\(\\hat{\\beta}_1\\) using the \\(\\hat{\\beta}_0\\) we just derived is shown in Equation 4.\n\\[\n\\begin{aligned}&\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} = -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0  \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + \\hat{\\beta}_0\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\\\text{(Fill in }\\hat{\\beta}_0\\text{)}&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\&\\Rightarrow  (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\bar{y}\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\bar{x}\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow n\\bar{y}\\bar{x} - \\hat{\\beta}_1n\\bar{x}^2 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 - \\hat{\\beta}_1n\\bar{x}^2  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\&\\Rightarrow \\hat{\\beta}_1\\Big(\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2\\Big)  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\ &\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2}\\end{aligned}\n\\tag{4}\\]\nTo write \\(\\hat{\\beta}_1\\) in a form that’s more recognizable, we will use the following:\n\\[\n\\sum x_iy_i - n\\bar{y}\\bar{x} = \\sum(x - \\bar{x})(y - \\bar{y}) = (n-1)\\text{Cov}(x,y)\n\\tag{5}\\]\n\\[\n\\sum x_i^2 - n\\bar{x}^2 - \\sum(x - \\bar{x})^2 = (n-1)s_x^2\n\\tag{6}\\]\nwhere \\(\\text{Cov}(x,y)\\) is the covariance of \\(x\\) and \\(y\\), and \\(s_x^2\\) is the sample variance of \\(x\\) (\\(s_x\\) is the sample standard deviation).\nThus, applying Equation 5 and Equation 6, we have\n\\[\n\\begin{aligned}\\hat{\\beta}_1 &= \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} \\\\&= \\frac{\\sum\\limits_{i=1}^{n}(x-\\bar{x})(y-\\bar{y})}{\\sum\\limits_{i=1}^{n}(x-\\bar{x})^2}\\\\&= \\frac{(n-1)\\text{Cov}(x,y)}{(n-1)s_x^2}\\\\&= \\frac{\\text{Cov}(x,y)}{s_x^2}\\end{aligned}\n\\tag{7}\\]\nThe correlation between \\(x\\) and \\(y\\) is \\(r = \\frac{\\text{Cov}(x,y)}{s_x s_y}\\). Thus, \\(\\text{Cov}(x,y) = r s_xs_y\\). Plugging this into Equation 7, we have\n\\[\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_ys_x}{s_x^2} = r\\frac{s_y}{s_x}\n\\tag{8}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html",
    "href": "supplemental/model-diagnostics-matrix.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of the model diagnostics - leverage, standardized residuals, and Cook’s distance. We assume the reader knowledge of the matrix form for multiple linear regression. Please see Matrix Form of Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#introduction",
    "href": "supplemental/model-diagnostics-matrix.html#introduction",
    "title": "Model Diagnostics",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation 2.\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "href": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "title": "Model Diagnostics",
    "section": "Matrix Form for the Regression Model",
    "text": "Matrix Form for the Regression Model\nWe can represent the Equation 1 and Equation 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "href": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "title": "Model Diagnostics",
    "section": "Hat Matrix & Leverage",
    "text": "Hat Matrix & Leverage\nRecall from the notes Matrix Form of Linear Regression that \\(\\hat{\\boldsymbol{\\beta}}\\) can be written as the following:\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\n\\tag{5}\\]\nCombining Equation 4 and Equation 5, we can write \\(\\hat{\\mathbf{Y}}\\) as the following:\n\\[\n\\begin{aligned}\n\\hat{\\mathbf{Y}} &= \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt]\n&= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\\\\n\\end{aligned}\n\\tag{6}\\]\nWe define the hat matrix as an \\(n \\times n\\) matrix of the form \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\). Thus Equation 6 becomes\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y}\n\\tag{7}\\]\nThe diagonal elements of the hat matrix are a measure of how far the predictor variables of each observation are from the means of the predictor variables. For example, \\(h_{ii}\\) is a measure of how far the values of the predictor variables for the \\(i^{th}\\) observation, \\(x_{i1}, x_{i2}, \\ldots, x_{ip}\\), are from the mean values of the predictor variables, \\(\\bar{x}_1, \\bar{x}_2, \\ldots, \\bar{x}_p\\). In the case of simple linear regression, the \\(i^{th}\\) diagonal, \\(h_{ii}\\), can be written as\n\\[\nh_{ii} =  \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n}(x_j-\\bar{x})^2}\n\\]\nWe call these diagonal elements, the leverage of each observation.\nThe diagonal elements of the hat matrix have the following properties:\n\n\\(0 \\leq h_ii \\leq 1\\)\n\\(\\sum\\limits_{i=1}^{n} h_{ii} = p+1\\), where \\(p\\) is the number of predictor variables in the model.\nThe mean hat value is \\(\\bar{h} = \\frac{\\sum\\limits_{i=1}^{n} h_{ii}}{n} = \\frac{p+1}{n}\\).\n\nUsing these properties, we consider a point to have high leverage if it has a leverage value that is more than 2 times the average. In other words, observations with leverage greater than \\(\\frac{2(p+1)}{n}\\) are considered to be high leverage points, i.e. outliers in the predictor variables. We are interested in flagging high leverage points, because they may have an influence on the regression coefficients.\nWhen there are high leverage points in the data, the regression line will tend towards those points; therefore, one property of high leverage points is that they tend to have small residuals. We will show this by rewriting the residuals from Equation 4 using Equation 7.\n\\[\n\\begin{aligned}\n\\mathbf{e} &= \\mathbf{Y} - \\hat{\\mathbf{Y}} \\\\[10pt]\n& = \\mathbf{Y} - \\mathbf{H}\\mathbf{Y} \\\\[10pt]\n&= (1-\\mathbf{H})\\mathbf{Y}\n\\end{aligned}\n\\tag{8}\\]\nNote that the identity matrix and hat matrix are idempotent, i.e. \\(\\mathbf{I}\\mathbf{I} = \\mathbf{I}\\), \\(\\mathbf{H}\\mathbf{H} = \\mathbf{H}\\). Thus, \\((\\mathbf{I} - \\mathbf{H})\\) is also idempotent. These matrices are also symmetric. Using these properties and Equation 8, we have that the variance-covariance matrix of the residuals \\(\\boldsymbol{e}\\), is\n\\[\n\\begin{aligned}\nVar(\\mathbf{e}) &= \\mathbf{e}\\mathbf{e}^T \\\\[10pt]\n&=  (1-\\mathbf{H})Var(\\mathbf{Y})^T(1-\\mathbf{H})^T \\\\[10pt]\n&= (1-\\mathbf{H})\\hat{\\sigma}^2(1-\\mathbf{H})^T  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})(1-\\mathbf{H})  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})\n\\end{aligned}\n\\tag{9}\\]\nwhere \\(\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^{n}e_i^2}{n-p-1}\\) is the estimated regression variance. Thus, the variance of the \\(i^{th}\\) residual is \\(Var(e_i) = \\hat{\\sigma}^2(1-h_{ii})\\). Therefore, the higher the leverage, the smaller the variance of the residual. Because the expected value of the residuals is 0, we conclude that points with high leverage tend to have smaller residuals than points with lower leverage."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "href": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "title": "Model Diagnostics",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\nIn general, we standardize a value by shifting by the expected value and rescaling by the standard deviation (or standard error). Thus, the \\(i^{th}\\) standardized residual takes the form\n\\[\nstd.res_i = \\frac{e_i - E(e_i)}{SE(e_i)}\n\\]\nThe expected value of the residuals is 0, i.e. \\(E(e_i) = 0\\). From Equation 9), the standard error of the residual is \\(SE(e_i) = \\hat{\\sigma}\\sqrt{1-h_{ii}}\\). Therefore,\n\\[\nstd.res_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\n\\tag{10}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "href": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "title": "Model Diagnostics",
    "section": "Cook’s Distance",
    "text": "Cook’s Distance\nCook’s distance is a measure of how much each observation influences the model coefficients, and thus the predicted values. The Cook’s distance for the \\(i^{th}\\) observation can be written as\n\\[\nD_i = \\frac{(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})^T(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})}{(p+1)\\hat{\\sigma}}\n\\tag{11}\\]\nwhere \\(\\hat{\\mathbf{Y}}_{(i)}\\) is the vector of predicted values from the model fitted when the \\(i^{th}\\) observation is deleted. Cook’s Distance can be calculated without deleting observations one at a time, since Equation 12 below is mathematically equivalent to Equation 11.\n\\[\nD_i = \\frac{1}{p+1}std.res_i^2\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg] = \\frac{e_i^2}{(p+1)\\hat{\\sigma}^2(1-h_{ii})}\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg]\n\\tag{12}\\]"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html",
    "href": "supplemental/model-selection-criteria.html",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of Akaike’s Information Criterion (AIC) and Schwarz’s Bayesian Information Criterion (BIC). We assume the reader knowledge of the matrix form for multiple linear regression.Please see Matrix Notation for Multiple Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "href": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)",
    "text": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)\nTo understand the formulas for AIC and BIC, we will first briefly explain the likelihood function and maximum likelihood estimates for regression.\nLet \\(\\mathbf{Y}\\) be \\(n \\times 1\\) matrix of responses, \\(\\mathbf{X}\\), the \\(n \\times (p+1)\\) matrix of predictors, and \\(\\boldsymbol{\\beta}\\), \\((p+1) \\times 1\\) matrix of coefficients. If the multiple linear regression model is correct then,\n\\[\n\\mathbf{Y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2)\n\\tag{1}\\]\nWhen we do linear regression, our goal is to estimate the unknown parameters \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) from Equation 1. In Matrix Notation for Multiple Linear Regression, we showed a way to estimate these parameters using matrix alegbra. Another approach for estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is using maximum likelihood estimation.\nA likelihood function is used to summarise the evidence from the data in support of each possible value of a model parameter. Using Equation 1, we will write the likelihood function for linear regression as\n\\[\nL(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) = \\prod\\limits_{i=1}^n (2\\pi \\sigma^2)^{-\\frac{1}{2}} \\exp\\bigg\\{-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\bigg\\}\n\\tag{2}\\]\nwhere \\(Y_i\\) is the \\(i^{th}\\) response and \\(\\mathbf{X}_i\\) is the vector of predictors for the \\(i^{th}\\) observation. One approach estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is to find the values of those parameters that maximize the likelihood in Equation 2, i.e. maximum likelhood estimation. To make the calculations more manageable, instead of maximizing the likelihood function, we will instead maximize its logarithm, i.e. the log-likelihood function. The values of the parameters that maximize the log-likelihood function are those that maximize the likelihood function. The log-likelihood function we will maximize is\n\\[\n\\begin{aligned}\n\\log L(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) &= \\sum\\limits_{i=1}^n -\\frac{1}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta}) \\\\\n&= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\\\\n\\end{aligned}\n\\tag{3}\\]\n\nThe maximum likelihood estimate of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are \\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\hspace{10mm} \\hat{\\sigma}^2 = \\frac{1}{n}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta}) = \\frac{1}{n}RSS\n\\tag{4}\\]\nwhere \\(RSS\\) is the residual sum of squares. Note that the maximum likelihood estimate is not exactly equal to the estimate of \\(\\sigma^2\\) we typically use \\(\\frac{RSS}{n-p-1}\\). This is because the maximum likelihood estimate of \\(\\sigma^2\\) in Equation 4 is a biased estimator of \\(\\sigma^2\\). When \\(n\\) is much larger than the number of predictors \\(p\\), then the differences in these two estimates are trivial."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#aic",
    "href": "supplemental/model-selection-criteria.html#aic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "AIC",
    "text": "AIC\nAkaike’s Information Criterion (AIC) is\n\\[\nAIC = -2 \\log L + 2(p+1)\n\\tag{5}\\]\nwhere \\(\\log L\\) is the log-likelihood. This is the general form of AIC that can be applied to a variety of models, but for now, let’s focus on AIC for mutliple linear regression.\n\\[\n\\begin{aligned}\nAIC &= -2 \\log L + 2(p+1) \\\\\n&= -2\\bigg[-\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\bigg] + 2(p+1) \\\\\n&= n\\log\\big(2\\pi\\frac{RSS}{n}\\big) + \\frac{1}{RSS/n}RSS \\\\\n&= n\\log(2\\pi) + n\\log(RSS) - n\\log(n) + 2(p+1)\n\\end{aligned}\n\\tag{6}\\]"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#bic",
    "href": "supplemental/model-selection-criteria.html#bic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "BIC",
    "text": "BIC\n[To be added.]"
  },
  {
    "objectID": "supplemental/mlr-matrix.html",
    "href": "supplemental/mlr-matrix.html",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides the details for the matrix notation for multiple linear regression. We assume the reader has familiarity with some linear algebra. Please see Chapter 1 of An Introduction to Statistical Learning for a brief review of linear algebra."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#introduction",
    "href": "supplemental/mlr-matrix.html#introduction",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation 2\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "href": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Matrix Representation for the Regression Model",
    "text": "Matrix Representation for the Regression Model\nWe can represent the Equation 1 and Equation 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "href": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Estimating the Coefficients",
    "text": "Estimating the Coefficients\nThe least-squares model is the one that minimizes the sum of the squared residuals. Therefore, we want to find the coefficients, \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes\n\\[\n\\sum\\limits_{i=1}^{n} e_{i}^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})\n\\tag{5}\\]\nwhere \\(\\mathbf{e}^T\\), the transpose of the matrix \\(\\mathbf{e}\\).\n\\[\n(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{Y}^T\\mathbf{Y} -\n\\mathbf{Y}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}} - (\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y} +\n\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}})\n\\tag{6}\\]\nNote that \\((\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Since these are both constants (i.e. \\(1\\times 1\\) vectors), \\(\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Thus, Equation 7 becomes\n\\[\n\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}}\n\\tag{7}\\]\nSince we want to find the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes Equation 5, will find the value of \\(\\hat{\\boldsymbol{\\beta}}\\) such that the derivative with respect to \\(\\hat{\\boldsymbol{\\beta}}\\) is equal to 0.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\mathbf{e}^T\\mathbf{e}}{\\partial \\hat{\\boldsymbol{\\beta}}} & = \\frac{\\partial}{\\partial \\hat{\\boldsymbol{\\beta}}}(\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^T\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0 \\\\\n&\\Rightarrow - 2 \\mathbf{X}^T\\mathbf{Y} + 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = 0 \\\\\n& \\Rightarrow 2 \\mathbf{X}^T\\mathbf{Y} = 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow \\mathbf{X}^T\\mathbf{Y} = \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{I}\\hat{\\boldsymbol{\\beta}}\n\\end{aligned}\n\\tag{8}\\]\nThus, the estimate of the model coefficients is \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "href": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Variance-covariance matrix of the coefficients",
    "text": "Variance-covariance matrix of the coefficients\nWe will use two properties to derive the form of the variance-covariance matrix of the coefficients:\n\n\\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\)\n\nFirst, we will show that \\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\[\n\\begin{aligned}\nE[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] &= E \\begin{bmatrix}\\epsilon_1  & \\epsilon_2 & \\dots & \\epsilon_n \\end{bmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}  \\\\\n& = E \\begin{bmatrix} \\epsilon_1^2  & \\epsilon_1 \\epsilon_2 & \\dots & \\epsilon_1 \\epsilon_n \\\\\n\\epsilon_2 \\epsilon_1 & \\epsilon_2^2 & \\dots & \\epsilon_2 \\epsilon_n \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\epsilon_n \\epsilon_1 & \\epsilon_n \\epsilon_2 & \\dots & \\epsilon_n^2\n\\end{bmatrix} \\\\\n& = \\begin{bmatrix} E[\\epsilon_1^2]  & E[\\epsilon_1 \\epsilon_2] & \\dots & E[\\epsilon_1 \\epsilon_n] \\\\\nE[\\epsilon_2 \\epsilon_1] & E[\\epsilon_2^2] & \\dots & E[\\epsilon_2 \\epsilon_n] \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n \\epsilon_1] & E[\\epsilon_n \\epsilon_2] & \\dots & E[\\epsilon_n^2]\n\\end{bmatrix}\n\\end{aligned}\n\\tag{9}\\]\nRecall, the regression assumption that the errors \\(\\epsilon_i's\\) are Normally distributed with mean 0 and variance \\(\\sigma^2\\). Thus, \\(E(\\epsilon_i^2) = Var(\\epsilon_i) = \\sigma^2\\) for all \\(i\\). Additionally, recall the regression assumption that the errors are uncorrelated, i.e. \\(E(\\epsilon_i \\epsilon_j) = Cov(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i,j\\). Using these assumptions, we can write Equation 9 as\n\\[\nE[\\mathbf{\\epsilon}\\mathbf{\\epsilon}^T]  = \\begin{bmatrix} \\sigma^2  & 0 & \\dots & 0 \\\\\n0 & \\sigma^2  & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{bmatrix} = \\sigma^2 \\mathbf{I}\n\\tag{10}\\]\nwhere \\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix.\nNext, we show that \\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\).\nRecall that the \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) and \\(\\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}\\). Then,\n\\[\n\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\\n&= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\\n&= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{\\epsilon} \\\\\n\\end{aligned}\n\\tag{11}\\]\nUsing these two properties, we derive the form of the variance-covariance matrix for the coefficients. Note that the covariance matrix is \\(E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T]\\)\n\\[\n\\begin{aligned}\nE[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T] &= E[(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})^T]\\\\\n& = E[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}] \\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T]\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T (\\sigma^2\\mathbf{I})\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&= \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&  = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n\\end{aligned}\n\\tag{12}\\]"
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week 4",
    "section": "",
    "text": "Important\n\n\n\nThe recording for this workshop is posted on the RDS website just click on the link to review"
  },
  {
    "objectID": "weeks/week-4.html#slides",
    "href": "weeks/week-4.html#slides",
    "title": "Week 4",
    "section": "Slides",
    "text": "Slides\n🖥️ Data Analysis in R"
  },
  {
    "objectID": "weeks/week-4.html#resources-used",
    "href": "weeks/week-4.html#resources-used",
    "title": "Week 4",
    "section": "Resources Used",
    "text": "Resources Used\nThis blog post by Julia Silge on tidymodels was extremely helpful\nAdjusting your standard errors on the fly, this blog post and the prediction portion of this lecture by Grant McDermott was invaluble.\nFor the marginal effects portion of the workshop much of the code is based on this blog post by Andrew Heiss."
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2",
    "section": "",
    "text": "Important\n\n\n\nThe recording for this workshop is posted on the RDS website just click on the link to review"
  },
  {
    "objectID": "weeks/week-2.html#slides",
    "href": "weeks/week-2.html#slides",
    "title": "Week 2",
    "section": "Slides",
    "text": "Slides\n🖥️ Data Cleaning in R With The Tidyverse\n🖥️ Data Cleaning in R With data.table"
  },
  {
    "objectID": "weeks/week-2.html#practice",
    "href": "weeks/week-2.html#practice",
    "title": "Week 2",
    "section": "Practice",
    "text": "Practice\n📋 Practice Materials"
  },
  {
    "objectID": "weeks/week-2.html#resources-used",
    "href": "weeks/week-2.html#resources-used",
    "title": "Week 2",
    "section": "Resources Used",
    "text": "Resources Used\nIntroduction to the tidyverse + data visualization with ggplot2 by Andrew Heiss\nLecture 5: Data cleaning & wrangling:(1) Tidyverse by Grant McDermott\nLecture 5: Data cleaning & wrangling:(1) data.table by Grant McDermott\nR for Data Science by Hadley Wickham & Garrett Grolemund\nTidyexplain by Garrick Aden-Buie, Tyler Grant Smith, Lukas Wallrich, and Kelsey Gonzalez\nRstudio Cheatsheets"
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week 1",
    "section": "",
    "text": "Important\n\n\n\nThe recording for this workshop is posted on the RDS website just click on the link to review"
  },
  {
    "objectID": "weeks/week-1.html#slides",
    "href": "weeks/week-1.html#slides",
    "title": "Week 1",
    "section": "Slides",
    "text": "Slides\n🖥️ Getting Started in R"
  },
  {
    "objectID": "weeks/week-1.html#practice",
    "href": "weeks/week-1.html#practice",
    "title": "Week 1",
    "section": "Practice",
    "text": "Practice\n📋Materials To Practice With"
  },
  {
    "objectID": "weeks/week-1.html#resources-used",
    "href": "weeks/week-1.html#resources-used",
    "title": "Week 1",
    "section": "Resources Used",
    "text": "Resources Used\nGrant McDermot’s excellent Data Science for Economists course especially the R Language Basics lecture that much of the content and structure of this first workshop is based around.\nR Cookbook, 2nd Edition by J.D. Long & Paul Teetor\nFast Lane to Learning R by Norm Matloff\nPalmer Penguins by Allison Horst, Alison Hill, and Kristen Gorman"
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "Important\n\n\n\nThe recording for this workshop is posted on the RDS website just click on the link to review"
  },
  {
    "objectID": "weeks/week-3.html#slides",
    "href": "weeks/week-3.html#slides",
    "title": "Week 3",
    "section": "Slides",
    "text": "Slides\n🖥️ Mapping and Data Visualization"
  },
  {
    "objectID": "weeks/week-3.html#practice",
    "href": "weeks/week-3.html#practice",
    "title": "Week 3",
    "section": "Practice",
    "text": "Practice\n📋 Practice Materials"
  },
  {
    "objectID": "weeks/week-3.html#resources-used",
    "href": "weeks/week-3.html#resources-used",
    "title": "Week 3",
    "section": "Resources Used",
    "text": "Resources Used\nGraphic Design with ggplot2 by Cédric Scherer\nData Visualization PMAP 8921 by Andrew Heiss\nPretty street maps in R with the osmdata package and ggplot by Josh McCrain\nA ggplot2 Tutorial For Beautiful Plotting in R by Cédric Scherer\nA Gentle Guide to the Grammar of Graphics with ggplot2 by Garrick Aden-Buie"
  },
  {
    "objectID": "slides/ggplot-presentation.html#our-team",
    "href": "slides/ggplot-presentation.html#our-team",
    "title": "Getting Started in ggplot",
    "section": "Our Team",
    "text": "Our Team\n\n\nAlso, we do not have the capacity to provide pre-scheduled frequent software tutoring that is divorced from a specific assignment or software troubleshooting task at hand – in other words, we are also not generalized software tutors.\nOur one-on-one software assistance is available to help you troubleshoot specific tasks or assignments. If you are seeking generalized software help, we direct you to our live workshops and our recorded workshops to gain the foundational skills for using various analytical software. Then, when or if the time comes that you have targeted software questions or specific issues to tackle related to a course assignment or research project, please feel free to contact us for one-on-one support."
  },
  {
    "objectID": "slides/ggplot-presentation.html#get-ready-badges",
    "href": "slides/ggplot-presentation.html#get-ready-badges",
    "title": "Getting Started in ggplot",
    "section": "Get Ready Badges",
    "text": "Get Ready Badges\n\n\nhttps://research.library.gsu.edu/dataservices/data-ready\nThese are awesome to share on social media i.e. linkedin which is a good signal to potential employers that you know this stuff"
  },
  {
    "objectID": "slides/ggplot-presentation.html#how-to-get-the-badges",
    "href": "slides/ggplot-presentation.html#how-to-get-the-badges",
    "title": "Getting Started in ggplot",
    "section": "How To Get the Badges",
    "text": "How To Get the Badges"
  },
  {
    "objectID": "slides/ggplot-presentation.html#packages-we-will-need-for-today",
    "href": "slides/ggplot-presentation.html#packages-we-will-need-for-today",
    "title": "Getting Started in ggplot",
    "section": "Packages We Will Need For Today",
    "text": "Packages We Will Need For Today\n\ninstall.packages(c(\"gapminder\", \"palmerpenguins\"))\n\n\nlibrary(gapminder)\nlibrary(palmerpenguins)\nlibrary(tidyverse) # ggplot will automatically be loaded in. \n# library(ggplot2) also works"
  },
  {
    "objectID": "slides/ggplot-presentation.html#why-visualize-your-data",
    "href": "slides/ggplot-presentation.html#why-visualize-your-data",
    "title": "Getting Started in ggplot",
    "section": "Why visualize your data?",
    "text": "Why visualize your data?\n\nmean(graph_dat$x)\n\n[1] 54.2657\n\n\n\nsd(graph_dat$x)\n\n[1] 16.713\n\n\n\nmean(graph_dat$y)\n\n[1] 47.8351\n\n\n\nsd(graph_dat$y)\n\n[1] 26.84777\n\n\n\ncor(graph_dat$x, graph_dat$y)\n\n[1] -0.06601891\n\n\n\nLets take a quick look at the data that I have loaded in. They look pretty similar for the most part and aren’t correlated."
  },
  {
    "objectID": "slides/ggplot-presentation.html#the-dino-strikes",
    "href": "slides/ggplot-presentation.html#the-dino-strikes",
    "title": "Getting Started in ggplot",
    "section": "The Dino Strikes",
    "text": "The Dino Strikes\n\n\n\nThe Datasaurus Dozen"
  },
  {
    "objectID": "slides/ggplot-presentation.html#why-ggplot2",
    "href": "slides/ggplot-presentation.html#why-ggplot2",
    "title": "Getting Started in ggplot",
    "section": "Why ggplot2?",
    "text": "Why ggplot2?\n\n\n\nThe transferable skills from ggplot2 are not the idiosyncrasies of plotting syntax, but a powerful way of thinking about visualization, as a way of mapping between variables and the visual properties of geometric objects that you can perceive.\n– Hadley Wickham"
  },
  {
    "objectID": "slides/ggplot-presentation.html#why-ggplot2-1",
    "href": "slides/ggplot-presentation.html#why-ggplot2-1",
    "title": "Getting Started in ggplot",
    "section": "Why ggplot2?",
    "text": "Why ggplot2?\n\n\nYou have probably heard of it but why use it?\nOnce we understand the “grammar” making figures becomes a lot easier\n\nConveying information becomes easier\nLets you show off your clean, or not so clean, data\nA coherent structure of doing things\n\nFlexibility\n\nTons ways to customize appearance\nLots of functions\nLots of extensions\n\nReproducibility\n\nDoesn’t require you to remember each input from a drop down menu\nDefaults to universally usable formats\nReplaces itself automatically in your directory\n\n\n\n\nggplot2 is probably one of the most well known and event among many of the staunchest base r users well loved an well used libraries in the R ecosystem.\nggplot2 predates most of the tidyverse and the gg part refers to the grammar of graphics, more on that later. Much like writing a sentence there is a proper grammar to it. Once we understand the rules we can construct lots of different “sentences”.\nI think an important theme so to say is that once we start to understand R it benefits your workflow because we can simply tweak a few things and then BAM it will just update. If you are a Latex user then bam the same plot just updates. If you are a word user then those benefits are limited but ggplots play nicely with Word. No having to put replace everywhere or screenshot things or working with propietary formats. If you need to you"
  },
  {
    "objectID": "slides/ggplot-presentation.html#grammar",
    "href": "slides/ggplot-presentation.html#grammar",
    "title": "Getting Started in ggplot",
    "section": "Grammar",
    "text": "Grammar\n\n\n“Good grammar is just the first step of creating a good sentence”\n\nHow is the data related to the figure on the right?"
  },
  {
    "objectID": "slides/ggplot-presentation.html#building-the-plot",
    "href": "slides/ggplot-presentation.html#building-the-plot",
    "title": "Getting Started in ggplot",
    "section": "Building the Plot",
    "text": "Building the Plot\n\n\nBody Weight of Penguins and Bill Length\n\nPenguins\nSpecies\nIsland\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI apologize for some weird plotting stuff. I am still working out some kinks with how the presentation is styled. I wanted to try to give the plots and the code the same size on the slides. However that resulted in some weird decisions from quarto"
  },
  {
    "objectID": "slides/ggplot-presentation.html#building-the-plot-1",
    "href": "slides/ggplot-presentation.html#building-the-plot-1",
    "title": "Getting Started in ggplot",
    "section": "Building the Plot",
    "text": "Building the Plot\n\n\nBody Weight of Penguins and Bill Length\n\nPenguins\nSpecies\nIsland\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere is a grammar to what is going on"
  },
  {
    "objectID": "slides/ggplot-presentation.html#building-the-plot-2",
    "href": "slides/ggplot-presentation.html#building-the-plot-2",
    "title": "Getting Started in ggplot",
    "section": "Building the Plot",
    "text": "Building the Plot\n\n\nBody Weight of Penguins and Bill Length\n\nPenguins\nSpecies\nIsland"
  },
  {
    "objectID": "slides/ggplot-presentation.html#so-how-did-we-go-from",
    "href": "slides/ggplot-presentation.html#so-how-did-we-go-from",
    "title": "Getting Started in ggplot",
    "section": "So How Did We go From?",
    "text": "So How Did We go From?\n\n\nThis\n\n\n\n\n\n\n\n\n\n\nTo This"
  },
  {
    "objectID": "slides/ggplot-presentation.html#the-grammar",
    "href": "slides/ggplot-presentation.html#the-grammar",
    "title": "Getting Started in ggplot",
    "section": "The Grammar",
    "text": "The Grammar\n\n\n\n\n\nComponent\n\n\nFunction\n\n\nExplanation\n\n\n\n\nData\n\n\nggplot(data)         \n\n\nThe raw data that you want to visualise.\n\n\n\n\nAesthetics          \n\n\naes()\n\n\nAesthetic mappings between variables and visual properties.\n\n\n\nGeometries\n\n\ngeom_*()\n\n\nThe geometric shapes representing the data.\n\n\n\n\nStatistics\n\n\nstat_*()\n\n\nThe statistical transformations applied to the data.\n\n\n\n\nScales\n\n\nscale_*()\n\n\nMaps between the data and the aesthetic dimensions.\n\n\n\n\nCoordinate System\n\n\ncoord_*()\n\n\nMaps data into the plane of the data rectangle.\n\n\n\n\nFacets\n\n\nfacet_*()\n\n\nThe arrangement of the data into a grid of plots.\n\n\n\n\nVisual Themes\n\n\ntheme() and theme_*()\n\n\nThe overall visual defaults of a plot.\n\n\n\n\n\nThis is what it looks like when we start to think of the plot. Each of these are columns in our dataset that we plop into aes. Our x goes in the first slot of aes and the second slot is our y. I typically specify this myself just cuz I like to do that but that is a personal decision. The rest you need to tell ggplot to do\nhey take your data and turn it into something that you can see, like size, colour, position or shape. They also provide the tools that let you interpret the plot: the axes and legends. You can generate plots with ggplot2 without knowing how scales work"
  },
  {
    "objectID": "slides/ggplot-presentation.html#where-do-they-go",
    "href": "slides/ggplot-presentation.html#where-do-they-go",
    "title": "Getting Started in ggplot",
    "section": "Where do they go?",
    "text": "Where do they go?\n\n\nggplot() +\n  geom_point(data = penguins,\n   aes(\n  x = bill_length_mm, \n  y = body_mass_g,\n  shape = species, \n  color = island),\n  size = 3)\n\n\n\n\n\n\n\n\n\n\n\nTo actually make the plot you need to specify the columns you want to “map” onto the plot. Map is kind of just the way that it is said in ggplot world. This is just us saying we want these columns in our"
  },
  {
    "objectID": "slides/ggplot-presentation.html#the-data",
    "href": "slides/ggplot-presentation.html#the-data",
    "title": "Getting Started in ggplot",
    "section": "The data",
    "text": "The data\n\n\n\n\n \n  \n    country \n    continent \n    year \n    lifeExp \n    pop \n    gdpPercap \n  \n \n\n  \n    Afghanistan \n    Asia \n    1952 \n    28.801 \n    8425333 \n    779.4453 \n  \n  \n    Afghanistan \n    Asia \n    1957 \n    30.332 \n    9240934 \n    820.8530 \n  \n  \n    Afghanistan \n    Asia \n    1962 \n    31.997 \n    10267083 \n    853.1007 \n  \n  \n    Afghanistan \n    Asia \n    1967 \n    34.020 \n    11537966 \n    836.1971 \n  \n  \n    Afghanistan \n    Asia \n    1972 \n    36.088 \n    13079460 \n    739.9811 \n  \n  \n    Afghanistan \n    Asia \n    1977 \n    38.438 \n    14880372 \n    786.1134 \n  \n  \n    Afghanistan \n    Asia \n    1982 \n    39.854 \n    12881816 \n    978.0114 \n  \n  \n    Afghanistan \n    Asia \n    1987 \n    40.822 \n    13867957 \n    852.3959 \n  \n  \n    Afghanistan \n    Asia \n    1992 \n    41.674 \n    16317921 \n    649.3414 \n  \n  \n    Afghanistan \n    Asia \n    1997 \n    41.763 \n    22227415 \n    635.3414 \n  \n  \n    Afghanistan \n    Asia \n    2002 \n    42.129 \n    25268405 \n    726.7341 \n  \n  \n    Afghanistan \n    Asia \n    2007 \n    43.828 \n    31889923 \n    974.5803 \n  \n  \n    Albania \n    Europe \n    1952 \n    55.230 \n    1282697 \n    1601.0561 \n  \n  \n    Albania \n    Europe \n    1957 \n    59.280 \n    1476505 \n    1942.2842 \n  \n  \n    Albania \n    Europe \n    1962 \n    64.820 \n    1728137 \n    2312.8890 \n  \n\n\n\n\n\n\ntoday we will be using the gapminder dataset. It contains historical (1952-2007) data on various indicators, such as life expectancy and GDP, for countries worldwide."
  },
  {
    "objectID": "slides/ggplot-presentation.html#here-is-your-shell-script",
    "href": "slides/ggplot-presentation.html#here-is-your-shell-script",
    "title": "Getting Started in ggplot",
    "section": "Here is your shell script",
    "text": "Here is your shell script\n\n## be sure you have done \n## install.packages(\"gapminder\")\n## library(gapminder)\n\n\nggplot() +\n  geom_point(data = gapminder,mapping = aes(x = gdpPercap, y = lifeExp))\n\n\nTo help you get your hands dirty I will give y’all a shell script to work with. Press o on your keyboard to navigate to the past slides to help yourself out if you get lost"
  },
  {
    "objectID": "slides/ggplot-presentation.html#activity",
    "href": "slides/ggplot-presentation.html#activity",
    "title": "Getting Started in ggplot",
    "section": "Activity",
    "text": "Activity\n\nAdd color, size, alpha, and shape aesthetics to your graph.\nBe bold be brave! Experiment!\nWhat happens when you add more than one aesthetic?\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/ggplot-presentation.html#how-would-you-make-this-plot",
    "href": "slides/ggplot-presentation.html#how-would-you-make-this-plot",
    "title": "Getting Started in ggplot",
    "section": "How would you make this plot?",
    "text": "How would you make this plot?\n\n\nall we are doing is making each point blue. Should be simple enough. If you are following along please add color = blue where you think it should go!"
  },
  {
    "objectID": "slides/ggplot-presentation.html#section",
    "href": "slides/ggplot-presentation.html#section",
    "title": "Getting Started in ggplot",
    "section": "",
    "text": "ggplot() + \n  geom_point(data = gapminder,\n   aes(x = gdpPercap,\n    y = lifeExp,\n    color = \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\nRemember when we do things in aes R will look for a column in our dataset. Importantly R will just looks for things it can do given the scope of the function. So if we put blue into the aes portion of it will plot it. But it will get confused because blue is not a column in our data set."
  },
  {
    "objectID": "slides/ggplot-presentation.html#section-1",
    "href": "slides/ggplot-presentation.html#section-1",
    "title": "Getting Started in ggplot",
    "section": "",
    "text": "ggplot() +\n  geom_point(data = gapminder,\n    aes(x = gdpPercap,\n        y = lifeExp),\n        color = \"blue\") \n\n\n\n\n\n\n\n\n\n\n\nggplot will take the color argument inside aes and outside aes. If we specify color inside aes than it will color things by a column in the dataset. If you specify color outside of aes ggplot will make everything that color"
  },
  {
    "objectID": "slides/ggplot-presentation.html#same-options-different-stuff",
    "href": "slides/ggplot-presentation.html#same-options-different-stuff",
    "title": "Getting Started in ggplot",
    "section": "Same options different stuff",
    "text": "Same options different stuff\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nthere we can show the same relationship between two variables in a ton of different ways. there are a ton of geoms that we can use to visualize our data. In this case I am just fitting a loess line through it."
  },
  {
    "objectID": "slides/ggplot-presentation.html#what-comes-with-ggplot",
    "href": "slides/ggplot-presentation.html#what-comes-with-ggplot",
    "title": "Getting Started in ggplot",
    "section": "What Comes With ggplot",
    "text": "What Comes With ggplot\n\n\n [1] \"geom_abline\"            \"geom_area\"              \"geom_bar\"              \n [4] \"geom_bin_2d\"            \"geom_bin2d\"             \"geom_blank\"            \n [7] \"geom_boxplot\"           \"geom_col\"               \"geom_contour\"          \n[10] \"geom_contour_filled\"    \"geom_count\"             \"geom_crossbar\"         \n[13] \"geom_curve\"             \"geom_density\"           \"geom_density_2d\"       \n[16] \"geom_density_2d_filled\" \"geom_density2d\"         \"geom_density2d_filled\" \n[19] \"geom_dotplot\"           \"geom_errorbar\"          \"geom_errorbarh\"        \n[22] \"geom_freqpoly\"          \"geom_function\"          \"geom_hex\"              \n[25] \"geom_histogram\"         \"geom_hline\"             \"geom_jitter\"           \n[28] \"geom_label\"             \"geom_line\"              \"geom_linerange\"        \n[31] \"geom_map\"               \"geom_path\"              \"geom_point\"            \n[34] \"geom_pointrange\"        \"geom_polygon\"           \"geom_qq\"               \n[37] \"geom_qq_line\"           \"geom_quantile\"          \"geom_raster\"           \n[40] \"geom_rect\"              \"geom_ribbon\"            \"geom_rug\"              \n[43] \"geom_segment\"           \"geom_sf\"                \"geom_sf_label\"         \n[46] \"geom_sf_text\"           \"geom_smooth\"            \"geom_spoke\"            \n[49] \"geom_step\"              \"geom_text\"              \"geom_tile\"             \n[52] \"geom_violin\"            \"geom_vline\"            \n\n\n\nThere are many more that we can use too. There are tons of different kinds of plots that we can find for our specific plotting needs that people have written for R I have like an unhealthy obsession with ggridges. I will say geom_sf only gets added when you have the sf package loaded and then"
  },
  {
    "objectID": "slides/ggplot-presentation.html#section-2",
    "href": "slides/ggplot-presentation.html#section-2",
    "title": "Getting Started in ggplot",
    "section": "",
    "text": "via GIPHY\n\n\nIt’s not that certain geoms should never be used with certain kinds its that sometimes it doesnt make a whole lot of sense or we aren’t getting the most out of our data visualizations. If it takes the arguments it will plot it."
  },
  {
    "objectID": "slides/ggplot-presentation.html#examplesort-of",
    "href": "slides/ggplot-presentation.html#examplesort-of",
    "title": "Getting Started in ggplot",
    "section": "Example(sort of)",
    "text": "Example(sort of)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoth times we see that we that ggplot will take the arguments and plot them but we arent really getting a lot of useful information. Boxplots would provide a much provide more info"
  },
  {
    "objectID": "slides/ggplot-presentation.html#your-turn",
    "href": "slides/ggplot-presentation.html#your-turn",
    "title": "Getting Started in ggplot",
    "section": "Your Turn",
    "text": "Your Turn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n−+\n02:00\n\n\n\n\ntake the graph on the left and change it to the plot on the right. use the ggplot cheatsheet."
  },
  {
    "objectID": "slides/ggplot-presentation.html#answer",
    "href": "slides/ggplot-presentation.html#answer",
    "title": "Getting Started in ggplot",
    "section": "Answer",
    "text": "Answer\n\n\nggplot() +\n  geom_boxplot(data = gapminder,\n  aes(x = continent, \n  y = lifeExp))"
  },
  {
    "objectID": "slides/ggplot-presentation.html#your-turn-again",
    "href": "slides/ggplot-presentation.html#your-turn-again",
    "title": "Getting Started in ggplot",
    "section": "Your Turn Again",
    "text": "Your Turn Again\nHint do not supply a Y value\n\n\n\n\n−+\n02:00\n\n\n\n\nLets make a histogram of lifeExp. In this case we only need to supply one variable to aes. Take exp and look at the distribution through a histogram"
  },
  {
    "objectID": "slides/ggplot-presentation.html#section-3",
    "href": "slides/ggplot-presentation.html#section-3",
    "title": "Getting Started in ggplot",
    "section": "",
    "text": "ggplot() +\n  geom_histogram( data = gapminder, \n    aes(x = lifeExp))"
  },
  {
    "objectID": "slides/ggplot-presentation.html#your-turn-1",
    "href": "slides/ggplot-presentation.html#your-turn-1",
    "title": "Getting Started in ggplot",
    "section": "Your Turn",
    "text": "Your Turn\nMake This Density Plot filled by continent\n\n\n\n\n−+\n02:00\n\n\n\n\nAgain do not supply a Y column"
  },
  {
    "objectID": "slides/ggplot-presentation.html#section-4",
    "href": "slides/ggplot-presentation.html#section-4",
    "title": "Getting Started in ggplot",
    "section": "",
    "text": "ggplot() +\n  geom_density( data = gapminder,\n  aes(x = lifeExp,\n     fill = continent),\n     alpha = 0.75)"
  },
  {
    "objectID": "slides/ggplot-presentation.html#complex-graph",
    "href": "slides/ggplot-presentation.html#complex-graph",
    "title": "Getting Started in ggplot",
    "section": "Complex graph!",
    "text": "Complex graph!\n\n\nMap wealth to the x-axis, health to the y-axis, add points, color by continent, size by population, scale the x-axis with a log. Please get rid of the default theme."
  },
  {
    "objectID": "slides/ggplot-presentation.html#local",
    "href": "slides/ggplot-presentation.html#local",
    "title": "Getting Started in ggplot",
    "section": "Local",
    "text": "Local\n\n\nggplot() +\n  geom_point(data = gapminder,\n   aes(x = gdpPercap,\n       y = lifeExp, \n       color = continent)) + \n  geom_smooth(data = gapminder,\n   aes(x = gdpPercap, \n       y = lifeExp, \n      color = continent))"
  },
  {
    "objectID": "slides/ggplot-presentation.html#global",
    "href": "slides/ggplot-presentation.html#global",
    "title": "Getting Started in ggplot",
    "section": "Global",
    "text": "Global\n\n\nggplot(gapminder,\n       aes(x = gdpPercap,\n          y = lifeExp, \n          color = continent))  + \n  geom_point() +\n  geom_smooth() \n\n\n\n\n\n\n\n\n\n\n\nSo we have mostly been working inside the individual geoms. However most people do not do this because it can start to get inconvenient quickly. If we work inside each individual geom than the other geom will not know what is going on and ggplot will get confused.\nInstead if you pass off our argument to the first layer that the aes argument will inheret the appropriate stuff"
  },
  {
    "objectID": "slides/ggplot-presentation.html#building-plots",
    "href": "slides/ggplot-presentation.html#building-plots",
    "title": "Getting Started in ggplot",
    "section": "Building Plots",
    "text": "Building Plots\nStarting with Data and aesthics\n\n\nggplot(gapminder,\n aes(x = gdpPercap,\n    y = lifeExp))\n\n\n\n\n\n\n\n\n\n\n\nSo far we have only walked through geoms and aesthics but we need other layers to make graphs that make sense. And are presentable to professional audiences"
  },
  {
    "objectID": "slides/ggplot-presentation.html#add-geom_point",
    "href": "slides/ggplot-presentation.html#add-geom_point",
    "title": "Getting Started in ggplot",
    "section": "Add geom_point",
    "text": "Add geom_point\n\n\nggplot(gapminder,\n    aes(x = gdpPercap,\n        y = lifeExp,\n        color = continent)) +\n  geom_point()"
  },
  {
    "objectID": "slides/ggplot-presentation.html#add-geom_smooth",
    "href": "slides/ggplot-presentation.html#add-geom_smooth",
    "title": "Getting Started in ggplot",
    "section": "Add geom_smooth",
    "text": "Add geom_smooth\n\n\nggplot(gapminder,\n    aes(x = gdpPercap,\n       y = lifeExp,\n       color = continent)) +\n  geom_point() +\n  geom_smooth()"
  },
  {
    "objectID": "slides/ggplot-presentation.html#add-change-transparency",
    "href": "slides/ggplot-presentation.html#add-change-transparency",
    "title": "Getting Started in ggplot",
    "section": "Add Change Transparency",
    "text": "Add Change Transparency\n\n\nggplot(gapminder,\n    aes(x = gdpPercap,\n       y = lifeExp,\n       color = continent)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth()"
  },
  {
    "objectID": "slides/ggplot-presentation.html#adjust-scales-with-scale_x_log10",
    "href": "slides/ggplot-presentation.html#adjust-scales-with-scale_x_log10",
    "title": "Getting Started in ggplot",
    "section": "Adjust scales with scale_x_log10",
    "text": "Adjust scales with scale_x_log10\n\n\nggplot(gapminder,\n     aes(x = gdpPercap,\n         y = lifeExp,\n         color = continent)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth() +\n  scale_x_log10()"
  },
  {
    "objectID": "slides/ggplot-presentation.html#add-axis-labels-and-title-with-labs",
    "href": "slides/ggplot-presentation.html#add-axis-labels-and-title-with-labs",
    "title": "Getting Started in ggplot",
    "section": "Add axis labels and title with labs",
    "text": "Add axis labels and title with labs\n\n\nggplot(gapminder,\n      aes(x = gdpPercap,\n          y = lifeExp,\n          color = continent)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth() +\n  scale_x_log10() +\n  labs(x = \"GDP per cap\",\n       y = \"Life Expectancy\",\n       title = \"The Effect of GDP per cap on Life Expectancy\")\n\n\n\n\n\n\n\n\n\n\n\nLabs can handle a lot of arguments. there are individual functuions for the x label and y label and title. But make your like easier by just using labs. You can also modify the legend in labs but I have various success with that. I tend to modify the legend using guides or theme"
  },
  {
    "objectID": "slides/ggplot-presentation.html#add-viridis-color-scale",
    "href": "slides/ggplot-presentation.html#add-viridis-color-scale",
    "title": "Getting Started in ggplot",
    "section": "Add viridis color scale",
    "text": "Add viridis color scale\n\n\nggplot(gapminder,\n      aes(x = gdpPercap,\n          y = lifeExp,\n          color = continent)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth() +\n  scale_x_log10() +\n  labs(x = \"GDP per cap\",\n       y = \"Life Expectanty\",\n       title = \"The Effect of GDP per cap on Life Expectancy\") +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "slides/ggplot-presentation.html#add-theme",
    "href": "slides/ggplot-presentation.html#add-theme",
    "title": "Getting Started in ggplot",
    "section": "Add theme",
    "text": "Add theme\n\n\nggplot(gapminder,\n      aes(x = gdpPercap,\n         y = lifeExp,\n         color = continent)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth() +\n  scale_x_log10() +\n  labs(x = \"GDP per cap\",\n       y = \"Life Expectanty\",\n       title = \"The Effect of GDP per cap on Life Expectancy\") +\n  scale_color_viridis_d() +\n  theme_bw()"
  },
  {
    "objectID": "slides/ggplot-presentation.html#facet-by-continent",
    "href": "slides/ggplot-presentation.html#facet-by-continent",
    "title": "Getting Started in ggplot",
    "section": "Facet by Continent",
    "text": "Facet by Continent\n\n\nggplot(gapminder,\n    aes(x = gdpPercap,\n        y = lifeExp,\n        color = continent)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth() +\n  scale_x_log10() +\n  labs(x = \"GDP per cap\",\n      y = \"Life Expectanty\",\n      title = \"The Effect of GDP per cap on Life Expectancy\") +\n  scale_color_viridis_d() +\n  theme_bw() +\n  facet_wrap(vars(continent))"
  },
  {
    "objectID": "slides/ggplot-presentation.html#change-theme-options",
    "href": "slides/ggplot-presentation.html#change-theme-options",
    "title": "Getting Started in ggplot",
    "section": "Change Theme Options",
    "text": "Change Theme Options\n\n\nggplot(gapminder,\n  aes(x = gdpPercap,\n      y = lifeExp,\n      color = continent)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth() +\n  scale_x_log10() +\n  labs(x = \"GDP per cap\",\n      y = \"Life Expectanty\",\n      title = \"The Effect of GDP per cap on Life Expectancy\") +\n  scale_color_viridis_d() +\n  theme_bw() +\n  facet_wrap(vars(continent)) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "slides/ggplot-presentation.html#a-grammar-we-can-use",
    "href": "slides/ggplot-presentation.html#a-grammar-we-can-use",
    "title": "Getting Started in ggplot",
    "section": "A Grammar We Can Use",
    "text": "A Grammar We Can Use\nMap wealth to the x-axis, health to the y-axis, add points, color by continent, size by population, scale the x-axis with a log. Please get rid of the default theme.\n\n\nggplot(gapminder,\n  aes(x = gdpPercap,\n      y = lifeExp,\n      size = pop,\n      color = continent)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Income\",\n       y = \"Life Expectancy\") +\n  scale_x_log10(labels = scales::dollar) +\n  theme_allen_bw() +\n  guides(size = guide_legend(reverse = TRUE,\n      title = \"Population\"),\n      color = guide_legend(title = \"Continent\"))\n\n\n\n\n\n\n\n\n\n\n\nThis is roughly the prompt that I was working with. The nice part about ggplot is you can start thinking about the plot in real words and then translate that over into code. Since we have a template of where to put things we can then focus on how best to visualize our data rather than fighting with thousands of dropdown menus"
  },
  {
    "objectID": "slides/ggplot-presentation.html#scales",
    "href": "slides/ggplot-presentation.html#scales",
    "title": "Getting Started in ggplot",
    "section": "Scales",
    "text": "Scales\n\n\n\n\nExample layer\n\n\nWhat it does\n\n\n\n\nscale_x_continuous()\n\n\nMake the x-axis continuous\n\n\n\n\nscale_x_continuous(breaks = 1:5) \n\n\nManually specify axis ticks\n\n\n\n\nscale_x_log10()\n\n\nLog the x-axis\n\n\n\n\nscale_color_gradient()\n\n\nUse a gradient\n\n\n\n\nscale_fill_viridis_d()\n\n\nFill with discrete viridis colors\n\n\n\n\n\nOften time we need to or want to adjust the scales that one or more of the axis are on or we pass off color arguments to"
  },
  {
    "objectID": "slides/ggplot-presentation.html#scales-in-action",
    "href": "slides/ggplot-presentation.html#scales-in-action",
    "title": "Getting Started in ggplot",
    "section": "Scales in Action",
    "text": "Scales in Action\n\n\nggplot(gapminder,\n  aes(x = gdpPercap,\n      y = lifeExp,\n      size = pop,\n      color = continent)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Income\",\n     y = \"Life Expectancy\") +\n  scale_x_log10(labels = scales::dollar) +\n  theme_bw() \n\n\n\n\n\n\n\n\n\n\n\nInstead of directly transforming your data ggplot allows you to do this through the scale_x function which is pretty neat. I will not go through each of them but that is pretty handy. If you want nicer looking labels you can use the scales package to make nicer looking labels. I.e adding commas to values on the x or y axis adding the dollar sign or percentage sign etc"
  },
  {
    "objectID": "slides/ggplot-presentation.html#scales-in-action-2",
    "href": "slides/ggplot-presentation.html#scales-in-action-2",
    "title": "Getting Started in ggplot",
    "section": "Scales in Action",
    "text": "Scales in Action\n\n\nggplot(gapminder,\n      aes(x = gdpPercap,\n          y = lifeExp,\n          size = pop,\n          color = continent)) +\n  geom_point(alpha = 0.5) +\n  scale_x_continuous(limits = c(0, 30000)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nIn some cases you may want to change the coordinates of the plot to expand or contract the coordinates or in some cases just flip them without hassle. ggplot defaults to using coord_cartesian on its own but coord_cartesian can be used to zoom in on part of the plot. The coor_flip here is a bit silly but it is insanely useful for barcharts and other charts where you may have added things into aes and don’t want to go back and redo it"
  },
  {
    "objectID": "slides/ggplot-presentation.html#coordinates",
    "href": "slides/ggplot-presentation.html#coordinates",
    "title": "Getting Started in ggplot",
    "section": "Coordinates",
    "text": "Coordinates\n\n\nggplot(gapminder,\n    aes(x = gdpPercap,\n     y = lifeExp)) +\n  geom_point(alpha = 0.5) +\n  scale_x_continuous(limits = c(0, 30000)) +\n  coord_flip() +\n  theme_bw()"
  },
  {
    "objectID": "slides/ggplot-presentation.html#your-turn-2",
    "href": "slides/ggplot-presentation.html#your-turn-2",
    "title": "Getting Started in ggplot",
    "section": "Your Turn",
    "text": "Your Turn\nMake this density plot of bill_length_mm filled by species. But change the default colors\n\n\n\n\n−+\n04:00\n\n\n\n\nlook at the help file for scale_fill_viridis and play around with the options"
  },
  {
    "objectID": "slides/ggplot-presentation.html#how-i-did-it",
    "href": "slides/ggplot-presentation.html#how-i-did-it",
    "title": "Getting Started in ggplot",
    "section": "How I Did It",
    "text": "How I Did It\n\n\nggplot(penguins,\n aes(x = bill_length_mm,\n    fill = species)) +\n  geom_density( alpha = 0.75) +\n  theme_bw() +\n  scale_fill_viridis_d(option = \"magma\")"
  },
  {
    "objectID": "slides/ggplot-presentation.html#facets",
    "href": "slides/ggplot-presentation.html#facets",
    "title": "Getting Started in ggplot",
    "section": "Facets",
    "text": "Facets\n\n\n\n\nExample layer\n\n\nWhat it does\n\n\n\n\nfacet_wrap(vars(continent))\n\n\nPlot for each continent\n\n\n\n\nfacet_wrap(vars(continent, year)) \n\n\nPlot for each continent/year\n\n\n\n\nfacet_wrap(…, ncol = 1)\n\n\nPut all facets in one column\n\n\n\n\nfacet_wrap(…, nrow = 1)\n\n\nPut all facets in one row"
  },
  {
    "objectID": "slides/ggplot-presentation.html#facet_wrap",
    "href": "slides/ggplot-presentation.html#facet_wrap",
    "title": "Getting Started in ggplot",
    "section": "facet_wrap",
    "text": "facet_wrap\n\n\nggplot(gapminder,\n  aes(x = gdpPercap,\n     y = lifeExp,\n     size = pop)) +\n  geom_point(alpha = 0.5) +\n  theme_bw() +\n  scale_x_log10() +\n  facet_wrap(vars(continent)) \n\n\n\n\n\n\n\n\n\n\n\nIf we want to get individual plots for each value of the variable we can feed this to facet wrap and is a really useful way of displaying information and saves you from creating subsets for each continent than plotting them individuall than combining them. You also have lots of"
  },
  {
    "objectID": "slides/ggplot-presentation.html#facet_grid",
    "href": "slides/ggplot-presentation.html#facet_grid",
    "title": "Getting Started in ggplot",
    "section": "facet_grid",
    "text": "facet_grid\n\n\nggplot(data = filter(gapminder,\n year %in% c(1987,1997,2002, 2007)),\n    aes(x = gdpPercap,\n     y = lifeExp,\n     size = pop)) +\n  geom_point(alpha = 0.5) +\n  theme_bw() +\n  scale_x_log10() +\n  facet_grid(vars(year))\n\n\n\n\n\n\n\n\n\n\n\nforms a matrix of panels defined by row and column faceting variables. It is most useful when you have two discrete variables, and all combinations of the variables exist in the data."
  },
  {
    "objectID": "slides/ggplot-presentation.html#facet_grid-1",
    "href": "slides/ggplot-presentation.html#facet_grid-1",
    "title": "Getting Started in ggplot",
    "section": "facet_grid",
    "text": "facet_grid\n\n\nggplot(data = filter(gapminder,\n year %in% c(1987,1997,2002, 2007)),\n    aes(x = gdpPercap,\n     y = lifeExp,\n     size = pop)) +\n  geom_point(alpha = 0.5) +\n  theme_bw() +\n  scale_x_log10() +\n  facet_grid(vars(year), vars(continent))"
  },
  {
    "objectID": "slides/ggplot-presentation.html#labels",
    "href": "slides/ggplot-presentation.html#labels",
    "title": "Getting Started in ggplot",
    "section": "Labels",
    "text": "Labels\n\n\n\n\nExample layer\n\n\nWhat it does\n\n\n\n\nlabs(title = “Neat title”)\n\n\nTitle\n\n\n\n\nlabs(caption = “Something”)\n\n\nCaption\n\n\n\n\nlabs(y = “Something”)\n\n\ny-axis\n\n\n\n\nlabs(size = “Population”)\n\n\nTitle of size legend"
  },
  {
    "objectID": "slides/ggplot-presentation.html#labels-with-labs",
    "href": "slides/ggplot-presentation.html#labels-with-labs",
    "title": "Getting Started in ggplot",
    "section": "Labels with labs",
    "text": "Labels with labs\n\n\nggplot(gapminder, \n       aes(x = gdpPercap,\n        y = lifeExp, \n        color = continent,\n        size = pop)) +\n geom_point(alpha = 0.5) +\n  scale_x_log10() +\n  labs(title = \"Health and wealth grow together\",\n       subtitle = \"Data from 2007\",\n       x = \"Wealth (GDP per capita)\",\n       y = \"Health (life expectancy)\",\n       color = \"Continent\",\n       size = \"Population\",\n       caption = \"Source: The Gapminder Project\")\n\n\n\n\n\n\n\n\n\n\n\nThe labs argument has a ton of flexibility. Basically if you feed something into aes you can change it in labs. I Tend to change legend labels in guides. That is just a personal preference but labs is superflexible. You will sometimes see other solutions for changing your legend."
  },
  {
    "objectID": "slides/ggplot-presentation.html#changing-the-default-theme",
    "href": "slides/ggplot-presentation.html#changing-the-default-theme",
    "title": "Getting Started in ggplot",
    "section": "Changing the Default Theme",
    "text": "Changing the Default Theme\n\n\ntheme_minimal\n\n\n\n\n\n\n\n\n\n\ntheme_dark"
  },
  {
    "objectID": "slides/ggplot-presentation.html#ggplot-in-the-wild",
    "href": "slides/ggplot-presentation.html#ggplot-in-the-wild",
    "title": "Getting Started in ggplot",
    "section": "ggplot in the wild",
    "text": "ggplot in the wild\n\nOften organizations will write their own theme\n\nCheck out the BBC’s\n\n\n\n\n##devtools::install_github('bbc/bbplot')\nlibrary(bbplot)\n ggplot(gapminder,\n  aes(x = gdpPercap,\n   y = lifeExp,\n    color = continent)) +\n  geom_point(alpha = 0.6) +\n  scale_color_manual(values = \n  c(\"#FAAB18\",\n    \"#1380A1\",\n    \"#990000\",\n    \"#588300\")) +\n  scale_x_log10() +\n  bbc_style()\n\n\n\n\n\n\n\n\n\n\n\nLots of organizations use ggplot. What this means for you is that you can find themes that lots of organizations use. I know for a fact that the economist makes all their code fully available for you. So if you see a cool graph from the economist you can just go to their github and figure out how they found it."
  },
  {
    "objectID": "slides/ggplot-presentation.html#the-theme-argument",
    "href": "slides/ggplot-presentation.html#the-theme-argument",
    "title": "Getting Started in ggplot",
    "section": "The theme argument",
    "text": "The theme argument\n\nHas lots and lots of options(94 to be exact)\nYou can change basically anything you could think of in a plot\n\nMy ggplot theme is basically just a some tweaks to theme arguments\n\n\n\ntheme_bw() + \ntheme(legend.position = \"bottom\",\n      plot.title = element_text(face = \"bold\"),\n      axis.title.y = element_text(face = \"italic\"))"
  },
  {
    "objectID": "slides/ggplot-presentation.html#saving-your-work",
    "href": "slides/ggplot-presentation.html#saving-your-work",
    "title": "Getting Started in ggplot",
    "section": "Saving your work",
    "text": "Saving your work\n\nyour_plot_here = ggplot(data, aes(x = blah, y = blah))\n\n\nggsave(\"name-of-your-file.pdf\",your_plot_here) \n\n\nggsave(\"name-of-your-file.pngs\",your_plot_here)"
  },
  {
    "objectID": "slides/ggplot-presentation.html#new-packages",
    "href": "slides/ggplot-presentation.html#new-packages",
    "title": "Getting Started in ggplot",
    "section": "New Packages",
    "text": "New Packages\n\ninstall.packages(c(\"sf\", \"tidygeocoder\"))\ndevtools::install_github(\"ropenscilabs/rnaturalearth\")\nlibrary(rnaturalearth)\nlibrary(sf)\n\n\nIf you are on a Mac please go to the r-spatial-website if you run into problems\nThe workhorse for this particular section will be sf\n\n\n\nThere are lots of mapping packages in R. I will only show you how to map in ggplot. You can get a really passable map in ggplot. I have included the code the Emre wrote that uses tmap. tmap is good and I have used it but once you get the hand of ggplot I think it is just easier to use ggplot than to learn how to use a new package\n\n\nThere are some other packages like geom_map and ggmap. They still work but sf(simple features) is really great and has a ton of support and resources. See this book"
  },
  {
    "objectID": "slides/ggplot-presentation.html#mapping-in-r",
    "href": "slides/ggplot-presentation.html#mapping-in-r",
    "title": "Getting Started in ggplot",
    "section": "Mapping in R",
    "text": "Mapping in R\n\nR and ggplot can get you pretty far\nThe stuff from these workshops broadly apply\n\nincluding your dplyr verbs\n\nLots of your needs to make static maps can be met\nDepending on what you are doing you may have to wait a bit"
  },
  {
    "objectID": "slides/ggplot-presentation.html#map-made-by-kieran-healy",
    "href": "slides/ggplot-presentation.html#map-made-by-kieran-healy",
    "title": "Getting Started in ggplot",
    "section": "Map Made By Kieran Healy",
    "text": "Map Made By Kieran Healy"
  },
  {
    "objectID": "slides/ggplot-presentation.html#shape-files",
    "href": "slides/ggplot-presentation.html#shape-files",
    "title": "Getting Started in ggplot",
    "section": "Shape Files",
    "text": "Shape Files\n\nTo read in shape files you use read_sf you should see something that looks like this!\n\n\n\nSimple feature collection with 177 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -90 xmax: 180 ymax: 83.64513\nGeodetic CRS:  WGS 84\n# A tibble: 177 × 2\n   SOVEREIGNT                                                           geometry\n   <chr>                                                      <MULTIPOLYGON [°]>\n 1 Fiji                        (((180 -16.06713, 180 -16.55522, 179.3641 -16.80…\n 2 United Republic of Tanzania (((33.90371 -0.95, 34.07262 -1.05982, 37.69869 -…\n 3 Western Sahara              (((-8.66559 27.65643, -8.665124 27.58948, -8.684…\n 4 Canada                      (((-122.84 49, -122.9742 49.00254, -124.9102 49.…\n 5 United States of America    (((-122.84 49, -120 49, -117.0312 49, -116.0482 …\n 6 Kazakhstan                  (((87.35997 49.21498, 86.59878 48.54918, 85.7682…\n 7 Uzbekistan                  (((55.96819 41.30864, 55.92892 44.99586, 58.5031…\n 8 Papua New Guinea            (((141.0002 -2.600151, 142.7352 -3.289153, 144.5…\n 9 Indonesia                   (((141.0002 -2.600151, 141.0171 -5.859022, 141.0…\n10 Argentina                   (((-68.63401 -52.63637, -68.25 -53.1, -67.75 -53…\n# … with 167 more rows\n\n\n\nWhen you download a shapefile you will get lots and lots of stuff. Lots of the shapefiles that you encounter in the wild are maintained by governments so keep those in the working directory. I will get to how to read in shape files in a minute. But when you read them into R the only thing that changes from your usual dataframe, tibble, data.table etc is that you will get a column in your dataset called geometry. This is just a list of things that delineate boundaries. for smaller countries like Fiji the number of boundaries are going to be smaller than say larger countries by area. It doesnt make sense to us but it makes sense to R"
  },
  {
    "objectID": "slides/ggplot-presentation.html#working-with-data",
    "href": "slides/ggplot-presentation.html#working-with-data",
    "title": "Getting Started in ggplot",
    "section": "Working with Data",
    "text": "Working with Data\n\nYou can use dplyr to manipulate data still.\n\n\n\nvdem_raw = vdemdata::vdem \n\nvdem_cleanish = vdem_raw %>% \n  select(v2x_regime, country_name, country_text_id, year) %>% \n  filter(year == \"2018\") %>% \n  mutate(ISO_A3 = countrycode(country_name, origin = \"country.name\",\n                              destination = \"iso3c\"),\n         regime_type = case_when(v2x_regime == 0 ~ \"Full Autocracy\",\n                                 v2x_regime == 1 ~ \"Electoral Autocracy\",\n                                 v2x_regime == 2 ~ \"Electoral Democracy\",\n                                 v2x_regime == 3 ~ \"Liberal Democracy\"),\n         regime_type = as.factor(regime_type)) %>% \n  arrange(ISO_A3)\n\n\nworld_map_fixed = world_map %>% \n  mutate(ISO_A3 = case_when(\n    ## If the country name is Norway or France, redo the ISO3 code\n    ADMIN == \"Norway\" ~ \"NOR\",\n    ADMIN == \"France\" ~ \"FRA\",\n    ## Otherwise use the existing ISO3 code\n    TRUE ~ ISO_A3)) %>% \n    filter(ISO_A3 != \"ATA\") \n\n\n\nplot_data_world = left_join(world_map_fixed, vdem_cleanish, by = \"ISO_A3\")  |> \n  filter(!is.na(regime_type))\n\n\n\nplot_data_world$regime_type = factor(plot_data_world$regime_type,\n                                     levels = c(\"Full Autocracy\",\n                                                \"Electoral Autocracy\",\n                                                \"Electoral Democracy\",\n                                                 \"Liberal Democracy\"))\n\n\n\n ggplot() + \n  geom_sf(data = plot_data_world, aes(fill = regime_type),\n          color = \"#CDCDCD\", size = 0.1) +\n   coord_sf(crs = st_crs(\"ESRI:54030\")) +\n    scale_fill_scico_d(palette = \"vik\", direction = -1, na.value = \"grey80\") +\n   labs(fill = \"Regime Types\",\n        caption = \"Data are derived from the Varities of Democracy Project\") +\n   theme_void(base_family = \"Roboto Condensed\") +\n   theme(legend.position = \"top\",\n         legend.text = element_text(size = 8),\n         plot.caption = element_text(hjust = 0.8))"
  },
  {
    "objectID": "slides/ggplot-presentation.html#making-maps-in-ggplot",
    "href": "slides/ggplot-presentation.html#making-maps-in-ggplot",
    "title": "Getting Started in ggplot",
    "section": "Making Maps in ggplot",
    "text": "Making Maps in ggplot\n\n\n## devtools::install_github(\"ropenscilabs/rnaturalearth\")\nlibrary(rnaturalearth)\n\nworld_map_ne = ne_states(returnclass = \"sf\")\n\nggplot() +\n  geom_sf(data = world_map_ne)\n\n\n\n\n\n\n\n\n\n\n\nas you can see nothing really fundamentally changes when you are making maps in ggplot. It is no different fundamentally than making a scatter plot or any other kind of plot here we are just feeding data to geom_sf"
  },
  {
    "objectID": "slides/ggplot-presentation.html#changing-the-projections",
    "href": "slides/ggplot-presentation.html#changing-the-projections",
    "title": "Getting Started in ggplot",
    "section": "Changing the Projections",
    "text": "Changing the Projections\n\n\nggplot() +\n  geom_sf(data = world_map_ne) + \n  coord_sf(crs = \"+proj=cea +lon_0=0 +lat_ts=45\") +\n  labs(title = \"Gall-Peters Projection\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nYou can really easily change the projections you want. Here we are just feeding it a different projections If you ever watched the west wing you may recognize this particular projection"
  },
  {
    "objectID": "slides/ggplot-presentation.html#mapping-middle-earth",
    "href": "slides/ggplot-presentation.html#mapping-middle-earth",
    "title": "Getting Started in ggplot",
    "section": "Mapping Middle Earth",
    "text": "Mapping Middle Earth\n\n\ncoastline = read_sf(\"data/ME-GIS/Coastline2.shp\")\ncontours = read_sf(\"data/ME-GIS/Contours_18.shp\")\nrivers = read_sf(\"data/ME-GIS/Rivers.shp\")\nlakes = read_sf(\"data/ME-GIS/Lakes.shp\")\nforests = read_sf(\"data/ME-GIS/Forests.shp\")\nmountains = read_sf(\"data/ME-GIS/Mountains_Anno.shp\")\nplacenames = read_sf(\"data/ME-GIS/Combined_Placenames.shp\")\ncities = read_sf(\"data/ME-GIS/Cities.shp\")\nroads = read_sf(\"data/ME-GIS/Roads.shp\")\ntowers_keeps =  read_sf(\"data/ME-GIS/Towers_and_Keeps.shp\")\nplaces = placenames |>\n  filter(NAME %in% c(\"Hobbiton\",\n                     \"Rivendell\",\n                     \"Edoras\",\n                     \"Minas Tirith\"))\n\nmordor = placenames[placenames$NAME == \"Mordor\",]\n\nmountains_to_label = mountains[mountains$name == \"Erebor The Lonely Mountain\",]\n\nggplot() +\n  geom_sf(data = contours,\n          size = 0.15,\n          color = \"grey90\") +\n  geom_sf(data = coastline,\n          size = 0.25,\n          color = \"grey50\") +\n  geom_sf(data = rivers,\n          size = 0.2,\n          color = \"#0776e0\",\n          alpha = 0.5) +\n  geom_sf(data = lakes,\n          size = 0.2,\n          color = \"#0776e0\",\n          fill = \"#0776e0\") +\n  geom_sf(data = forests,\n          size = 0,\n          fill = \"#035711\",\n          alpha = 0.5) +\n  geom_sf(data = mountains, size = 0.25) +\n  geom_sf(data = places) +\n  geom_sf_label(data = places,\n                aes(label = NAME),\n                nudge_y = 80000) +\n  geom_sf_label(data = mountains_to_label,\n                aes(label = name),\n                nudge_y = 80000) +\n  geom_sf_label(data = mordor,\n                aes(label = NAME),\n                nudge_y = 9000) +\n  theme_void() +\n  theme(plot.background = element_rect(fill = \"#fffce3\"))\n\n\n\n\n\n\n\n\n\n\n\nAs you can see this does not look fundamentally all that different from the rest of our ggplot code. We are just coloring and sizing ssutff depening on what we want. We can add annotations"
  },
  {
    "objectID": "slides/ggplot-presentation.html#working-without-shape-fileskind-of",
    "href": "slides/ggplot-presentation.html#working-without-shape-fileskind-of",
    "title": "Getting Started in ggplot",
    "section": "Working Without Shape Files(kind of)",
    "text": "Working Without Shape Files(kind of)\n\nAs is the case sometimes we do not have a shape file\nDon’t worry sf has you covered\nYou just need to feed it the right things"
  },
  {
    "objectID": "slides/ggplot-presentation.html#making-a-bespoke-shapefile",
    "href": "slides/ggplot-presentation.html#making-a-bespoke-shapefile",
    "title": "Getting Started in ggplot",
    "section": "Making a Bespoke Shapefile",
    "text": "Making a Bespoke Shapefile\n\nYou can either feed it latitude and longitudes\nOr you can feed it addresses\nMost free ones have rate limits\n\nSo be mindful of the size of your data\n\n\n\nIn this case if we do not have the geometry column but we have some addresses we can just geocode them to produce the column."
  },
  {
    "objectID": "slides/ggplot-presentation.html#making-a-bespoke-shapefilecont",
    "href": "slides/ggplot-presentation.html#making-a-bespoke-shapefilecont",
    "title": "Getting Started in ggplot",
    "section": "Making a Bespoke Shapefile(cont)",
    "text": "Making a Bespoke Shapefile(cont)\n\nga_cities = tribble( \n  ~city, ~lat, ~long,\n  \"Atlanta\", 33.748955, -84.388099,\n  \"Athens\", 33.950794, -83.358884,\n  \"Savannah\", 32.113192, -81.089350\n)\n\n\nga_cities_geometry = ga_cities |>  \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(\"EPSG:4326\"))\nga_cities_geometry\n\n\n\n\n\n \n  \n    city \n    geometry \n  \n \n\n  \n    Atlanta \n    POINT (-84.3881 33.74896) \n  \n  \n    Athens \n    POINT (-83.35888 33.95079) \n  \n  \n    Savannah \n    POINT (-81.08935 32.11319) \n  \n\n\n\n\n\n\n\nexamples derived from Andrew Heiss"
  },
  {
    "objectID": "slides/ggplot-presentation.html#making-a-bespoke-shapefilecont-1",
    "href": "slides/ggplot-presentation.html#making-a-bespoke-shapefilecont-1",
    "title": "Getting Started in ggplot",
    "section": "Making a Bespoke Shapefile(cont)",
    "text": "Making a Bespoke Shapefile(cont)\n\nlibrary(tidygeocoder)\n\nbreweries_I_visit = tribble(\n  ~name, ~address,\n  \"Russian River Brewing\", \"725 4th St, Santa Rosa, CA 95404\",\n  \"Orpheus Brewing\", \"1440 Dutch Valley Pl NE, Atlanta, GA 30324\",\n  \"Three Tavens\", \"121 New St, Decatur, GA 30030\",\n  \"HenHouse Brewing Company\", \"322 Bellevue Ave, Santa Rosa, CA 95407\"\n)\n\nbreweries_geocode = breweries_I_visit |> \n  geocode(address, method = \"osm\") ## backup if one service fails \n\nadd_geom = breweries_geocode |> \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(\"EPSG:4326\"))\n\n\n\n\n\n \n  \n    name \n    address \n    geometry \n  \n \n\n  \n    Russian River Brewing \n    725 4th St, Santa Rosa, CA 95404 \n    POINT (-122.7117 38.4418) \n  \n  \n    Orpheus Brewing \n    1440 Dutch Valley Pl NE, Atlanta, GA 30324 \n    POINT (-84.36874 33.79355) \n  \n  \n    Three Tavens \n    121 New St, Decatur, GA 30030 \n    POINT (-84.28488 33.77306) \n  \n  \n    HenHouse Brewing Company \n    322 Bellevue Ave, Santa Rosa, CA 95407 \n    POINT (-122.7255 38.40122) \n  \n\n\n\n\n\n\nin the geocode argument you can cascade the services just in case it fails. The neat thing is that you can feed it a dataframe of addresses. What is also pretty cool is that it plays really well with other R tidyverse packages so if you scrape a website to grab a ton of addresses than you can just feed it into tidygeocoder\nfrom there you can use ggplot or even create interactive leaflet maps"
  },
  {
    "objectID": "slides/ggplot-presentation.html#mapping-atlanta",
    "href": "slides/ggplot-presentation.html#mapping-atlanta",
    "title": "Getting Started in ggplot",
    "section": "Mapping Atlanta",
    "text": "Mapping Atlanta\n\n\nCombining what we have learned we can easily map Atlanta\nUsing the osmdata package by Open Street Maps we can do lots of things\nYou can also save lots of money on gifts trust me\nWe can grab a host of various things we would want for a good map\nAnd add points of interest\nI will just geocode some bars that pols grad students have drank at or have had to host events at."
  },
  {
    "objectID": "slides/ggplot-presentation.html#mapping-atlanta-1",
    "href": "slides/ggplot-presentation.html#mapping-atlanta-1",
    "title": "Getting Started in ggplot",
    "section": "Mapping Atlanta",
    "text": "Mapping Atlanta\n\n\nCode\nlibrary(osmdata)\n\nbig_streets = getbb(\"Atlanta United States\") %>% \n  opq() %>% \n  add_osm_feature(key = \"highway\", \n                  value = c(\"motorway\", \"primary\", \"motorway_link\", \"primary_link\")) %>%\n  osmdata_sf()\n\nmed_streets = getbb(\"Atlanta United States\") %>%\n  opq() %>%\n  add_osm_feature(key = \"highway\", \n                  value = c(\"secondary\", \"tertiary\", \"secondary_link\", \"tertiary_link\")) %>%\n  osmdata_sf()\n\n\nsmall_streets = getbb(\"Atlanta United States\") %>%\n  opq()%>%\n  add_osm_feature(key = \"highway\", \n                  value = c(\"residential\", \"living_street\",\n                            \"unclassified\",\n                            \"service\", \"footway\"\n                  )) %>%\n  osmdata_sf()\n\n\n  railway = getbb(\"Atlanta United States\")%>%\n  opq()%>%\n  add_osm_feature(key = \"railway\", value= \"rail\") %>%\n  osmdata_sf()\n\nbars = tribble(\n  ~name, ~address,\n  \"Hattie B's\", \"299 Moreland Ave NE, Atlanta, GA 30307\",\n  \"Side Bar\", \"79 Poplar St NW, Atlanta, GA 30303\",\n  \"Manny's\", \"602 North Highland Avenue Northeast, Atlanta, GA 30307\",\n  \"The Porter\", \"1156 Euclid Ave NE, Atlanta, GA 30307\",\n  \"Anatolia's\", \"Peachtree St, Atlanta, GA 30303\",\n  \"Agave\", \"242 Boulevard SE, Atlanta, GA 30312\",\n  \"Fetch\", \"520 Daniel St SE, Atlanta, GA 30312\",\n  \"Recess\", \"99-V, Krog St NE, Atlanta, GA 30307\",\n  \"Der Biergarten\", \"300 Marietta St NW, Atlanta, GA 30313\",\n  \"The Highlander\", \"931 Monroe Dr NE, Atlanta, GA 30308\" ,\n  \"Wrecking Bar\", \"292 Moreland Ave NE, Atlanta, GA 30307\"\n)\n\nbars_geocode = bars %>% \n  geocode(address, method = \"osm\")\n\nadd_geom = bars_geocode %>% \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(\"EPSG:4326\"))"
  },
  {
    "objectID": "slides/ggplot-presentation.html#plotting-the-results",
    "href": "slides/ggplot-presentation.html#plotting-the-results",
    "title": "Getting Started in ggplot",
    "section": "Plotting the results",
    "text": "Plotting the results\n\n\nggplot() +\n  geom_sf(data = railway$osm_lines,\n          inherit.aes = FALSE,\n          color = \"black\",\n          size = .2,\n          linetype=\"dotdash\",\n          alpha = .5) +\n  geom_sf(data = med_streets$osm_lines,\n          inherit.aes = FALSE,\n          color = \"black\",\n          size = .3,\n          alpha = .5) +\n  geom_sf(data = small_streets$osm_lines,\n          inherit.aes = FALSE,\n          color = \"#666666\",\n          size = .2,\n          alpha = .3) +\n  geom_sf(data = big_streets$osm_lines,\n          inherit.aes = FALSE,\n          color = \"black\",\n          size = .5,\n          alpha = .6) +\n  geom_sf(data = add_geom, aes(color = name),\n          color = \"#0039A6\") +\n  coord_sf(ylim = c(33.64, 33.89), \n           xlim = c (-84.56, -84.30), expand = FALSE) +\n  theme_void(base_family = \"Roboto Condensed\",\n   base_size = 20) +\n  theme(plot.subtitle = element_text(size = 10, hjust = 0.5, \n    margin = margin(2, 0, 5, 0)),\n    plot.title = element_text(face=\"bold\", hjust=.5),\n    plot.caption = element_text(size = 8, face = \"bold\")) + \n   labs(title = \"Atlanta\",\n   subtitle = \"33.7490°N/84.3880°W\")"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#packages-you-will-need",
    "href": "slides/data-analysis-in-R.html#packages-you-will-need",
    "title": "Data Analysis in R",
    "section": "Packages You Will Need",
    "text": "Packages You Will Need\n\ninstall.packages(\"pacman\")\n\npacman::p_load(\"sandwich\", \"lmtest\", \"marginaleffects\", \"emmeans\", \"modelsummary\", \"broom\",\n               \"caret\", \"tidyverse\", \"fixest\", \"kableExtra\", \"nullabor\", \"plm\", \"ggfortify\", install = TRUE)\nset.seed(1994)\n\npenguins = palmerpenguins::penguins \n\n\nNote you will see some version of theme_allen used\n\nThis is just set a set of modifications based on Kyle Butt’s personal theme, mainly the fonts\nIf you really like it I can include the underlying file in an email.\n\n\n\nNote this workshop is not a workshop that will teach you how these methods work. I presume that most of you will be familiar with linear regression and maximumlikelihood estimation. If any of you are here with more of a machine learning background this includes logit and what not.\nI also assume you know the basics of indexing I will go through a bit of it but you should probably know that before doing data analysis\nThis workshop is going to sort of through building the results section of a paper. This will generally be geared toward a social science audience. But the fundamentals are virtually the same."
  },
  {
    "objectID": "slides/data-analysis-in-R.html#describing-variables",
    "href": "slides/data-analysis-in-R.html#describing-variables",
    "title": "Data Analysis in R",
    "section": "Describing Variables",
    "text": "Describing Variables\n\nThis depends on what kind of variable it is i.e. continuous, categorical etc\nIt also depends on what story you need to tell\n\nIs this confounder a big deal?\nDo we see anticipation of treatment?\nAre there any outliers?\netc?\n\nRemember R is just a toolbox.\n\n\nExploratory data analysis is the first step of any data analysis pipeline. You need to understand your data in order to understand how to handle things. What does the distribution look like of your outcome variable. If you are like me and ended up taking a lot of causal inference classes and or read a lot of causal inference papers lots of the first parts of papers are just presenting EDA results to convince the reader that the assumptions are met.\nToday we will be asking what is the relationship between bill length and body mass\nhopefully that seems like a sensible research question. If not well the goal is just to show you how to generate descriptive statistics in R"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#first-cut",
    "href": "slides/data-analysis-in-R.html#first-cut",
    "title": "Data Analysis in R",
    "section": "First Cut",
    "text": "First Cut\n\nsummary(penguins) \n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\n\nIf we just want like the quickest and dirtiest look at all the variables in our data we can use summary. Functionally this works because palmerpenguins is a small toy dataset. Thankfully for me we can access the raw data that the penguins dataset is based on. This is just the penguins raw function in the palmer penguins packagte."
  },
  {
    "objectID": "slides/data-analysis-in-R.html#summary-with-a-bigger-data-frame",
    "href": "slides/data-analysis-in-R.html#summary-with-a-bigger-data-frame",
    "title": "Data Analysis in R",
    "section": "Summary with a bigger data frame",
    "text": "Summary with a bigger data frame\n\npenguins_bigger = palmerpenguins::penguins_raw\n\nsummary(penguins_bigger)\n\n  studyName         Sample Number      Species             Region         \n Length:344         Min.   :  1.00   Length:344         Length:344        \n Class :character   1st Qu.: 29.00   Class :character   Class :character  \n Mode  :character   Median : 58.00   Mode  :character   Mode  :character  \n                    Mean   : 63.15                                        \n                    3rd Qu.: 95.25                                        \n                    Max.   :152.00                                        \n                                                                          \n    Island             Stage           Individual ID      Clutch Completion \n Length:344         Length:344         Length:344         Length:344        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n    Date Egg          Culmen Length (mm) Culmen Depth (mm) Flipper Length (mm)\n Min.   :2007-11-09   Min.   :32.10      Min.   :13.10     Min.   :172.0      \n 1st Qu.:2007-11-28   1st Qu.:39.23      1st Qu.:15.60     1st Qu.:190.0      \n Median :2008-11-09   Median :44.45      Median :17.30     Median :197.0      \n Mean   :2008-11-27   Mean   :43.92      Mean   :17.15     Mean   :200.9      \n 3rd Qu.:2009-11-16   3rd Qu.:48.50      3rd Qu.:18.70     3rd Qu.:213.0      \n Max.   :2009-12-01   Max.   :59.60      Max.   :21.50     Max.   :231.0      \n                      NA's   :2          NA's   :2         NA's   :2          \n Body Mass (g)      Sex            Delta 15 N (o/oo) Delta 13 C (o/oo)\n Min.   :2700   Length:344         Min.   : 7.632    Min.   :-27.02   \n 1st Qu.:3550   Class :character   1st Qu.: 8.300    1st Qu.:-26.32   \n Median :4050   Mode  :character   Median : 8.652    Median :-25.83   \n Mean   :4202                      Mean   : 8.733    Mean   :-25.69   \n 3rd Qu.:4750                      3rd Qu.: 9.172    3rd Qu.:-25.06   \n Max.   :6300                      Max.   :10.025    Max.   :-23.79   \n NA's   :2                         NA's   :14        NA's   :13       \n   Comments        \n Length:344        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\n\n\nAs you can see you can that if you like a somewhat big data frame it can get a little unruly quickly because it is going to spit out a description for every column in your data set. If you want a single variable than you can just the dollar sign to index by name. We can also get each measure we want individually"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#getting-some-descriptive-statistics",
    "href": "slides/data-analysis-in-R.html#getting-some-descriptive-statistics",
    "title": "Data Analysis in R",
    "section": "Getting Some Descriptive Statistics",
    "text": "Getting Some Descriptive Statistics\nThe Mean\n\n\nBase\n\nmean(penguins$bill_depth_mm, na.rm = TRUE)\n\n[1] 17.15117\n\n\n\nTidyverse(overkill in this application)\n\nsummarise(penguins, mean(body_mass_g,\n                      na.rm = TRUE))\n\n# A tibble: 1 × 1\n  `mean(body_mass_g, na.rm = TRUE)`\n                              <dbl>\n1                             4202."
  },
  {
    "objectID": "slides/data-analysis-in-R.html#quartiles",
    "href": "slides/data-analysis-in-R.html#quartiles",
    "title": "Data Analysis in R",
    "section": "Quartiles",
    "text": "Quartiles\n\n\nBase\n\nquantile(penguins$bill_depth_mm,\n         na.rm = TRUE,\n         probs = c(.25, .50, .75))\n\n 25%  50%  75% \n15.6 17.3 18.7 \n\n\n\nTidyverse(overkill in this instance)\n\npenguins |> \n  summarise(quartile_length = quantile(bill_length_mm,\n                                    na.rm = TRUE,\n                                    probs = c(.25,\n                                              .50,\n                                              .75)))\n\n# A tibble: 3 × 1\n  quartile_length\n            <dbl>\n1            39.2\n2            44.4\n3            48.5"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#t-tests",
    "href": "slides/data-analysis-in-R.html#t-tests",
    "title": "Data Analysis in R",
    "section": "T-tests",
    "text": "T-tests\n\n\nBase R\n\npenguins_t_test = na.omit(penguins) \n\npenguins_t_test$gentoo = ifelse(penguins_t_test$species == \"Gentoo\", TRUE, FALSE)\n\nt.test(flipper_length_mm ~ gentoo, data = penguins_t_test) \n\n\n\n\n    Welch Two Sample t-test\n\ndata:  flipper_length_mm by gentoo\nt = -32.47, df = 263.2, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group FALSE and group TRUE is not equal to 0\n95 percent confidence interval:\n -26.84983 -23.77964\nsample estimates:\nmean in group FALSE  mean in group TRUE \n           191.9206            217.2353 \n\n\n\nTidyverse\n\npenguins_t_test = penguins |> \n  drop_na() |> \n  mutate(gentoo = ifelse(species == \"Gentoo\",\n                         TRUE,\n                         FALSE))\n\nt.test(flipper_length_mm ~ gentoo, data = penguins_t_test) \n\n\n\n\n    Welch Two Sample t-test\n\ndata:  flipper_length_mm by gentoo\nt = -32.47, df = 263.2, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group FALSE and group TRUE is not equal to 0\n95 percent confidence interval:\n -26.84983 -23.77964\nsample estimates:\nmean in group FALSE  mean in group TRUE \n           191.9206            217.2353 \n\n\n\n\n\nIn this case we have our first example of using of the formula. To do statsy stuff in R we generally have to use the tilde. This is just a function of how R does statistical analysis. Would and equal sign make sense here sure but like we have to use equal for lots of other stuff."
  },
  {
    "objectID": "slides/data-analysis-in-R.html#correlations",
    "href": "slides/data-analysis-in-R.html#correlations",
    "title": "Data Analysis in R",
    "section": "Correlations",
    "text": "Correlations\n\n\nBase\n\npenguins_df = penguins[,c(3:6)]  |> ## lets you do it by column position\n  na.omit() \n\ncor(penguins_df)\n\n## this lets you do it by class\n penguins[, sapply(penguins, is.numeric)]  |> \n  na.omit() |>\n   cor()\n\n\n\n\n\n \n  \n      \n    bill_length_mm \n    bill_depth_mm \n  \n \n\n  \n    bill_length_mm \n    1.0000000 \n    -0.2350529 \n  \n  \n    bill_depth_mm \n    -0.2350529 \n    1.0000000 \n  \n\n\n\n\n\n\nTidyverse\n\npenguins_df = penguins |> \n  drop_na() |> \n  select(where(is.numeric), -year, - body_mass_g) #### Just cutting out year for presentation \n\ncor(penguins_df)\n\n\n\n\n\n \n  \n      \n    bill_length_mm \n    bill_depth_mm \n  \n \n\n  \n    bill_length_mm \n    1.0000000 \n    -0.2286256 \n  \n  \n    bill_depth_mm \n    -0.2286256 \n    1.0000000"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#it-is-also-important-to-plot-your-data",
    "href": "slides/data-analysis-in-R.html#it-is-also-important-to-plot-your-data",
    "title": "Data Analysis in R",
    "section": "It is Also Important to plot your data!",
    "text": "It is Also Important to plot your data!\n\n\n\nThe Datasaurus Dozen\n\n\n\nRemember numbers hide lots of things from our ggplot workshop I showed this figure. These all have roughly the same mean and standard deviation"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#some-basic-graphs",
    "href": "slides/data-analysis-in-R.html#some-basic-graphs",
    "title": "Data Analysis in R",
    "section": "Some Basic Graphs",
    "text": "Some Basic Graphs\n\n\nggplot(penguins,\n       aes(x = bill_length_mm ,\n           y = body_mass_g,\n           color = species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Bill Length(milimeters)\",\n       y = \"Body mass(grams)\") +\n  guides(color = guide_legend(title = \"Species\",\n                              override.aes = list(fill = NA))) +\n  theme_allen_bw()"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#some-basic-graphs-cont",
    "href": "slides/data-analysis-in-R.html#some-basic-graphs-cont",
    "title": "Data Analysis in R",
    "section": "Some Basic Graphs Cont",
    "text": "Some Basic Graphs Cont\n\n\nggplot(penguins,\n   aes(x = bill_depth_mm,\n       y = body_mass_g)) +\n  geom_point(aes(color = species)) +\n  geom_smooth(aes(color = species),\n              method = \"lm\") +\n  geom_smooth(method = \"lm\") + \n  labs(x = \"Bill Length(milimeters)\",\n        y = \"Body mass(grams)\") +\n  guides(color = guide_legend(title = \"Species\",\n        override.aes = list(fill = NA))) +\n  theme_allen_bw()\n\n\n\n\n\n\n\n\n\n\n\nNotice that when we plot by subgroup we see evidence of simpson’s pardox"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#distributions",
    "href": "slides/data-analysis-in-R.html#distributions",
    "title": "Data Analysis in R",
    "section": "Distributions",
    "text": "Distributions\n\n\nggplot(penguins,\n  aes(x = flipper_length_mm,\n      fill = species)) +\n  geom_histogram(alpha = 0.6) +\n  labs(x = \"Flipper Length(milimeters)\") +\n  guides(fill = guide_legend(title = \"Species\")) +\n  theme_allen_minimal()"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#distributionscont",
    "href": "slides/data-analysis-in-R.html#distributionscont",
    "title": "Data Analysis in R",
    "section": "Distributions(cont)",
    "text": "Distributions(cont)\n\n\n## you can turn off scientific notion using option(scipen = 999)\n\nggplot(penguins, aes(x = body_mass_g, fill = species)) +\n  geom_density(alpha = 0.7) +\n  labs(x = \"Body Mass(grams)\") +\n  guides(fill = guide_legend(title = \"Species\")) +\n  theme_allen_minimal()"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#user-written-extensions",
    "href": "slides/data-analysis-in-R.html#user-written-extensions",
    "title": "Data Analysis in R",
    "section": "User Written Extensions",
    "text": "User Written Extensions\n\n\nlibrary(gghalves)\nlibrary(MetBrewer)\nlibrary(ggdist)\n\nggplot(penguins,\n  aes(x = species,\n      y = bill_depth_mm,\n     fill = species)) +\n  geom_boxplot(\n    width = .2, fill = \"white\",\n    size = 1.5, outlier.shape = NA\n  ) +\n  stat_halfeye(\n    adjust = .33,\n    width = .67, \n    color = NA,\n    position = position_nudge(x = .15)\n  ) +\n  geom_half_point(\n    side = \"l\", \n    range_scale = .3, \n    alpha = .25, size = 1\n  ) +\n  scale_fill_met_d(name = \"Veronese\") +\n  labs(x = NULL,\n      fill = \"Species\",\n      y = \"Bill Depth(millimeters)\") +\n  coord_flip() +\n  theme_allen_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nCode and example are from Cédric Scherer’s Beyond the Bar plot talk"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#generating-table-1",
    "href": "slides/data-analysis-in-R.html#generating-table-1",
    "title": "Data Analysis in R",
    "section": "Generating Table 1",
    "text": "Generating Table 1\n\npenguins = as.data.frame(penguins)\n\ndatasummary(All(penguins) ~ Mean + SD + Max + Min + Median + Histogram,\n            data = penguins, output = \"html\",\n            title = \"Descriptive Statistics\") \n\n\n\nDescriptive Statistics\n \n  \n      \n    Mean \n    SD \n    Max \n    Min \n    Median \n    Histogram \n  \n \n\n  \n    bill_length_mm \n    43.92 \n    5.46 \n    59.60 \n    32.10 \n    44.45 \n    ▁▅▆▆▆▇▇▂▁ \n  \n  \n    bill_depth_mm \n    17.15 \n    1.97 \n    21.50 \n    13.10 \n    17.30 \n    ▃▄▄▄▇▆▇▅▂▁ \n  \n  \n    flipper_length_mm \n    200.92 \n    14.06 \n    231 \n    172 \n    197.00 \n    ▂▅▇▄▁▄▄▂▁ \n  \n  \n    body_mass_g \n    4201.75 \n    801.95 \n    6300 \n    2700 \n    4050.00 \n    ▁▄▇▅▄▄▃▃▂▁ \n  \n  \n    year \n    2008.03 \n    0.82 \n    2009 \n    2007 \n    2008.00 \n    ▆▇▇"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#generating-a-balance-table",
    "href": "slides/data-analysis-in-R.html#generating-a-balance-table",
    "title": "Data Analysis in R",
    "section": "Generating a Balance Table",
    "text": "Generating a Balance Table\n\ndatasummary_balance(~gentoo,\n                    data = penguins_t_test)\n\n\n\n \n\n\nFALSE (N=214)\nTRUE (N=119)\n\n\n  \n      \n       \n    Mean \n    Std. Dev. \n    Mean \n    Std. Dev. \n    Diff. in Means \n    Std. Error \n  \n \n\n  \n    bill_length_mm \n     \n    42.0 \n    5.5 \n    47.6 \n    3.1 \n    5.6 \n    0.5 \n  \n  \n    bill_depth_mm \n     \n    18.4 \n    1.2 \n    15.0 \n    1.0 \n    -3.4 \n    0.1 \n  \n  \n    flipper_length_mm \n     \n    191.9 \n    7.2 \n    217.2 \n    6.6 \n    25.3 \n    0.8 \n  \n  \n    body_mass_g \n     \n    3714.7 \n    435.7 \n    5092.4 \n    501.5 \n    1377.7 \n    54.8 \n  \n  \n    year \n     \n    2008.0 \n    0.8 \n    2008.1 \n    0.8 \n    0.0 \n    0.1 \n  \n  \n     \n     \n    N \n    Pct. \n    N \n    Pct. \n     \n     \n  \n  \n    species \n    Adelie \n    146 \n    68.2 \n    0 \n    0.0 \n     \n     \n  \n  \n     \n    Chinstrap \n    68 \n    31.8 \n    0 \n    0.0 \n     \n     \n  \n  \n     \n    Gentoo \n    0 \n    0.0 \n    119 \n    100.0 \n     \n     \n  \n  \n    island \n    Biscoe \n    44 \n    20.6 \n    119 \n    100.0 \n     \n     \n  \n  \n     \n    Dream \n    123 \n    57.5 \n    0 \n    0.0 \n     \n     \n  \n  \n     \n    Torgersen \n    47 \n    22.0 \n    0 \n    0.0 \n     \n     \n  \n  \n    sex \n    female \n    107 \n    50.0 \n    58 \n    48.7 \n     \n     \n  \n  \n     \n    male \n    107 \n    50.0 \n    61 \n    51.3"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#some-basics",
    "href": "slides/data-analysis-in-R.html#some-basics",
    "title": "Data Analysis in R",
    "section": "Some Basics",
    "text": "Some Basics\n\nFor lm and other models we use this general form\n\n\nestimator(Outcome ~ IV1 + IV2, data = your_data)\n\n\nIf you want to include all the variables in your data set than you can do that with .\nRemember R can hold lots of datasets so we have to be explicit with where the data is coming from\n\nnote that we have several different datasets named penguins_blah\nmaking sure you have kept track of the using dataset is important"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#univariate-regression",
    "href": "slides/data-analysis-in-R.html#univariate-regression",
    "title": "Data Analysis in R",
    "section": "Univariate Regression",
    "text": "Univariate Regression\n\npeng_naive = lm(body_mass_g ~ bill_length_mm, data = penguins)\n\npeng_naive\n\n\nCall:\nlm(formula = body_mass_g ~ bill_length_mm, data = penguins)\n\nCoefficients:\n   (Intercept)  bill_length_mm  \n        362.31           87.42  \n\n\n\nnotice that the output is fairly sparse but in the environment you have a list object in the environment. R creates these as lists because there are tons of things of various classes. So to grab them you can use broom or you can index them."
  },
  {
    "objectID": "slides/data-analysis-in-R.html#section",
    "href": "slides/data-analysis-in-R.html#section",
    "title": "Data Analysis in R",
    "section": "",
    "text": "peng_naive = lm(body_mass_g ~ bill_length_mm, data = penguins)\n\nsummary(peng_naive)\n\n\nCall:\nlm(formula = body_mass_g ~ bill_length_mm, data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1762.08  -446.98    32.59   462.31  1636.86 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     362.307    283.345   1.279    0.202    \nbill_length_mm   87.415      6.402  13.654   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 645.4 on 340 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.3542,    Adjusted R-squared:  0.3523 \nF-statistic: 186.4 on 1 and 340 DF,  p-value: < 2.2e-16\n\n\n\nThis is probably what you are used to seeing if you are coming over from another language"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#multiple-regression",
    "href": "slides/data-analysis-in-R.html#multiple-regression",
    "title": "Data Analysis in R",
    "section": "Multiple Regression",
    "text": "Multiple Regression\n\npeng_adjust = lm(body_mass_g ~  bill_length_mm + flipper_length_mm + species, data = penguins)\n\nsummary(peng_adjust)\n\n\nCall:\nlm(formula = body_mass_g ~ bill_length_mm + flipper_length_mm + \n    species, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-808.83 -230.35  -26.16  223.18 1050.37 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -3904.387    529.257  -7.377 1.27e-12 ***\nbill_length_mm       61.736      7.126   8.664  < 2e-16 ***\nflipper_length_mm    27.429      3.176   8.638 2.34e-16 ***\nspeciesChinstrap   -748.562     81.534  -9.181  < 2e-16 ***\nspeciesGentoo        90.435     88.647   1.020    0.308    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 340.1 on 337 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.8222,    Adjusted R-squared:  0.8201 \nF-statistic: 389.7 on 4 and 337 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#getting-diagnostic-statistics",
    "href": "slides/data-analysis-in-R.html#getting-diagnostic-statistics",
    "title": "Data Analysis in R",
    "section": "Getting Diagnostic Statistics",
    "text": "Getting Diagnostic Statistics\n\n\n\n\n\nBase R\n\npeng_adjust = lm(body_mass_g ~  bill_length_mm + flipper_length_mm + species,\n                 data = peng_sans_miss_base)\n## remember you need to open the train car door or it will return a listy thing\npeng_sans_miss_base$.fitted_vals_brack = peng_adjust[[5]] \n\npeng_sans_miss_base$.fitted_vals_dollar = peng_adjust$fitted.values\n\npeng_sans_miss_base$.predicted_vals = predict(peng_adjust, interval = \"prediction\")\n\npeng_sans_miss_base$.residuals_vals = peng_adjust$residuals\n\npeng_sans_miss_base$.studentized_resids = rstudent(peng_adjust)\n\npeng_sans_miss_base$.cooks_distance = cooks.distance(peng_adjust)\n\n\nTidyverse\n\npeng_adjust = lm(body_mass_g ~  bill_length_mm + flipper_length_mm + species,\n   data = peng_sans_miss_tidy) \npeng_diag_tidy = augment(peng_adjust,\n                         data = peng_sans_miss_tidy)\n\n\n\n\n\nAs good data analysts we should always check that our assumptions are met. One really good test is to just plot our fitted values against our residuals. So lets go ahead and grab them. In base you just index them via the dollar sign or brackets. In the tidyverse can get them via broom augment. They will be returned with the name of the thing you want starting with a dot.\nThe reason is that the team behind broom did not want to override existing stuff\n\n\nI just dropped the missing values to make my life easier"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#plotting",
    "href": "slides/data-analysis-in-R.html#plotting",
    "title": "Data Analysis in R",
    "section": "Plotting",
    "text": "Plotting\n\n\nBase\n\nplot(peng_sans_miss_base$.fitted_vals_dollar,\n     peng_sans_miss_base$.residuals_vals,\n     col = peng_sans_miss_base$species)\nabline(lm(peng_sans_miss_base$.residuals_vals ~ peng_sans_miss_base$.fitted_vals_dollar))\n\n\n\n\n\n\n\n\n\nTidyverse\n\nggplot(peng_diag_tidy, aes(x = .fitted,\n                           y = .resid)) +\n  geom_point(aes(color = species)) +\n  geom_smooth(method = \"lm\") +\n  theme_allen_bw()"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#checking-for-normality",
    "href": "slides/data-analysis-in-R.html#checking-for-normality",
    "title": "Data Analysis in R",
    "section": "Checking for Normality",
    "text": "Checking for Normality\n\n\nBase\n\nhist(peng_sans_miss_base$.residuals_vals)\n\n\n\n\n\n\n\n\n\nTidyverse\n\nggplot(peng_diag_tidy, aes(x = .resid)) +\n  geom_histogram(binwidth = 100) +\n  theme_allen_bw()"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#base-r-diagnostic-plots",
    "href": "slides/data-analysis-in-R.html#base-r-diagnostic-plots",
    "title": "Data Analysis in R",
    "section": "Base R diagnostic plots",
    "text": "Base R diagnostic plots\n\npar(mfrow = c(2, 2))\nplot(peng_adjust)"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#ggplot2-extension-for-same-thing",
    "href": "slides/data-analysis-in-R.html#ggplot2-extension-for-same-thing",
    "title": "Data Analysis in R",
    "section": "ggplot2 extension for same thing",
    "text": "ggplot2 extension for same thing\n\nautoplot(peng_adjust)"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#user-extensions",
    "href": "slides/data-analysis-in-R.html#user-extensions",
    "title": "Data Analysis in R",
    "section": "User Extensions",
    "text": "User Extensions\n\n\n\nset.seed(1994)  ## Shuffle these the same way every time\n\nshuffled_residuals = lineup(null_lm(body_mass_g ~ bill_length_mm + flipper_length_mm + species,\n                                     method = \"rotate\"),\n                             true = peng_diag_tidy,\n                             n = 9)\n#### decrypt(\"CLg7 X161 sO bJws6sJO vv\")\n\nggplot(shuffled_residuals, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  facet_wrap(vars(.sample))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is kind of a cool way to see if you can find your residuals in the plot. If one sticks out that is a bad sign that you havent dealt with some issues"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#tests",
    "href": "slides/data-analysis-in-R.html#tests",
    "title": "Data Analysis in R",
    "section": "Tests",
    "text": "Tests\n\ndecrypt(\"CLg7 X161 sO bJws6sJO vv\")\n\n[1] \"True data in position  1\"\n\n\n\nbptest(peng_adjust)\n\n\n\n\n\n \n  \n    statistic \n    p.value \n    parameter \n    method \n  \n \n\n  \n    3.678191 \n    0.4513059 \n    4 \n    studentized Breusch-Pagan test \n  \n\n\n\n\n\n\nbgtest(peng_adjust)\n\n\n\n\n\n \n  \n    statistic \n    p.value \n    parameter \n    method \n  \n \n\n  \n    8.776406 \n    0.0030515 \n    1 \n    Breusch-Godfrey test for serial correlation of order up to 1 \n  \n\n\n\n\n\n\nnull for Breusch Pagan there is no hetrosckedasticity\nfail to reject\nNull for Breusch Godfrey is there is no serial correlation of order up to 1\nOkay it looks like something we have violated some of our assumptions"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#fixing-our-standard-errors",
    "href": "slides/data-analysis-in-R.html#fixing-our-standard-errors",
    "title": "Data Analysis in R",
    "section": "Fixing Our Standard Errors",
    "text": "Fixing Our Standard Errors\n\nWe can do this “on the fly” in R\nThere are ways to do this in the model formula with various packages\n\nBut it is better to do this “on the fly”"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#adjusting-our-standard-errors-for-real-this-time",
    "href": "slides/data-analysis-in-R.html#adjusting-our-standard-errors-for-real-this-time",
    "title": "Data Analysis in R",
    "section": "Adjusting Our Standard Errors for Real This time",
    "text": "Adjusting Our Standard Errors for Real This time\n\ncoeftest(peng_adjust, vcov= vcovHC)\n\n\nt test of coefficients:\n\n                    Estimate Std. Error t value  Pr(>|t|)    \n(Intercept)       -3864.0732   507.3100 -7.6168 2.807e-13 ***\nbill_length_mm       60.1173     6.5195  9.2212 < 2.2e-16 ***\nflipper_length_mm    27.5443     3.0951  8.8993 < 2.2e-16 ***\nspeciesChinstrap   -732.4167    76.3112 -9.5978 < 2.2e-16 ***\nspeciesGentoo       113.2542    89.2584  1.2688    0.2054    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nUsing sandwich we can adjust our standard errors like this"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#a-whole-host-of-standard-errors",
    "href": "slides/data-analysis-in-R.html#a-whole-host-of-standard-errors",
    "title": "Data Analysis in R",
    "section": "A Whole Host of Standard Errors",
    "text": "A Whole Host of Standard Errors\n\nvc <- list(\n  \"Standard\"              = vcov(peng_adjust),\n  \"Sandwich (basic)\"      = sandwich(peng_adjust),\n  \"Clustered\"             = vcovCL(peng_adjust, cluster = ~ species),\n  \"Clustered (two-way)\"   = vcovCL(peng_adjust, cluster = ~ species + year),\n  \"HC3\"                   = vcovHC(peng_adjust),\n  \"Andrews' kernel HAC\"   = kernHAC(peng_adjust),\n  \"Newey-West\"            = NeweyWest(peng_adjust),\n  \"Bootstrap\"             = vcovBS(peng_adjust),\n  \"Bootstrap (clustered)\" = vcovBS(peng_adjust, cluster = ~ species)\n)\n\nadjusted_models = lapply(vc, function(x) coeftest(peng_adjust, vcov = x))\n\n\nWe can feed a huge list of standard errors fairly quickly it this took virtually no time. Instead of going back and adjusting a huge set of models as would be the case in other softwares where you specify standard error fixes in the model here you can simply specify it once and forget it."
  },
  {
    "objectID": "slides/data-analysis-in-R.html#what-does-it-look-like",
    "href": "slides/data-analysis-in-R.html#what-does-it-look-like",
    "title": "Data Analysis in R",
    "section": "What does it look like",
    "text": "What does it look like"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#making-table-2",
    "href": "slides/data-analysis-in-R.html#making-table-2",
    "title": "Data Analysis in R",
    "section": "Making Table 2",
    "text": "Making Table 2\n\nModel Summary is my favorite table making package\n\nif you run modelsummary::supported_models() you will see it fits most needs\nalternatively if you don’t want to read the list and fit your model than do modelsummary::get_estimates(model_I_fitted) it will tell if the output is supported\n\nSupports a variety of formats including word\n\n\nse_info = tibble(term = \"Standard errors\",\"iid\", \"robust\", \"bootstrap\", \"stata\", \"clustered by sex\")\nmodelsummary(peng_adjust,\n             stars = TRUE,\n             coef_omit = \"(Intercept)|flipper_length_mm|.*species\",\n             add_rows = se_info,\n             vcov =  list(\"iid\", \"robust\", \"bootstrap\", \"stata\", cluster = ~ sex ),\n             gof_map = c(\"nobs\", \"r.squared\"))\n\n\nAs you can see it also lets you adjust your standard errors\n\nYou can supply it one kind of standard error\nOr a List like I did"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#the-results",
    "href": "slides/data-analysis-in-R.html#the-results",
    "title": "Data Analysis in R",
    "section": "The Results",
    "text": "The Results\n\n\n\n\n \n  \n      \n    Model 1 \n    Model 2 \n    Model 3 \n    Model 4 \n    Model 5 \n  \n \n\n  \n    bill_length_mm \n    60.117*** \n    60.117*** \n    60.117*** \n    60.117*** \n    60.117*** \n  \n  \n     \n    (7.207) \n    (6.519) \n    (6.158) \n    (6.429) \n    (1.591) \n  \n  \n    Num.Obs. \n    333 \n    333 \n    333 \n    333 \n    333 \n  \n  \n    R2 \n    0.824 \n    0.824 \n    0.824 \n    0.824 \n    0.824 \n  \n  \n    Standard errors \n    iid \n    robust \n    bootstrap \n    stata \n    clustered by sex \n  \n\n\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#interactions",
    "href": "slides/data-analysis-in-R.html#interactions",
    "title": "Data Analysis in R",
    "section": "Interactions",
    "text": "Interactions\n\nTo include a multiplicative term we can use *, : or /\nThe constitutive terms will appear automatically with * and /\nIn the case of / it returns the marginal effect\n\nAs well as the correct standard errors and pvalues\n\n\n\nThe nice part about R is that you can include interactions in a few different ways. One thing is that R will include the constuitive terms for you. The slash will sort of trick R into including the full marginal effects for you.\n\n\npengs_interact_stand = lm(body_mass_g ~ bill_length_mm * sex + species + flipper_length_mm,\n                           data = peng_sans_miss_tidy)\n\nsummary(pengs_interact_stand)\n\n\nCall:\nlm(formula = body_mass_g ~ bill_length_mm * sex + species + flipper_length_mm, \n    data = peng_sans_miss_tidy)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-722.25 -189.89   -5.58  188.97  897.79 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            -1279.184    601.073  -2.128 0.034073 *  \nbill_length_mm            28.695      7.980   3.596 0.000374 ***\nsexmale                 1004.946    279.166   3.600 0.000368 ***\nspeciesChinstrap        -302.268     81.331  -3.716 0.000238 ***\nspeciesGentoo            670.469     95.795   6.999 1.47e-11 ***\nflipper_length_mm         19.052      2.954   6.449 4.06e-10 ***\nbill_length_mm:sexmale   -12.525      6.403  -1.956 0.051324 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 290.7 on 326 degrees of freedom\nMultiple R-squared:  0.872, Adjusted R-squared:  0.8697 \nF-statistic: 370.2 on 6 and 326 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#full-marginal-effects-way",
    "href": "slides/data-analysis-in-R.html#full-marginal-effects-way",
    "title": "Data Analysis in R",
    "section": "Full Marginal Effects way",
    "text": "Full Marginal Effects way\n\npengs_interact_marg = lm(body_mass_g ~ bill_length_mm / sex + flipper_length_mm + species, data = peng_sans_miss_tidy)\n\nsummary(pengs_interact_marg)\n\n\nCall:\nlm(formula = body_mass_g ~ bill_length_mm/sex + flipper_length_mm + \n    species, data = peng_sans_miss_tidy)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-733.40 -188.22  -11.83  187.92  868.75 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            -491.606    569.987  -0.862 0.389052    \nbill_length_mm           17.813      7.520   2.369 0.018421 *  \nflipper_length_mm        17.354      2.969   5.845 1.23e-08 ***\nspeciesChinstrap       -305.537     82.800  -3.690 0.000263 ***\nspeciesGentoo           706.682     96.992   7.286 2.41e-12 ***\nbill_length_mm:sexmale   10.252      1.002  10.235  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 296 on 327 degrees of freedom\nMultiple R-squared:  0.8669,    Adjusted R-squared:  0.8649 \nF-statistic: 426.1 on 5 and 327 DF,  p-value: < 2.2e-16\n\n\n\nSo this will give you the correct marginal effects but if you want the full range of values we need our marginal effects package"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#getting-marginal-effects-for-interactions",
    "href": "slides/data-analysis-in-R.html#getting-marginal-effects-for-interactions",
    "title": "Data Analysis in R",
    "section": "Getting Marginal Effects for Interactions",
    "text": "Getting Marginal Effects for Interactions\n\n\npengs_interact_effect = lm(body_mass_g ~ bill_length_mm  + flipper_length_mm * sex + species, data = peng_sans_miss_tidy)\n\nplot_cco(\n  pengs_interact_effect,\n  effect =  \"sex\", \n  condition = \"flipper_length_mm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\ninterflex is also good a package and implements diagnostics for violations of the assumptions underlying interaction terms and a way to interpret the effects. inters also has an interesting implementation where they use LASSO\n\n\n\n\nWe may be interested in what happens when we shift the values of our continous variable. One awesome way to do this is through our marginal effects package. In this case we can look at the effect as bill_length_mm and the cco. CCO is This function plots contrasts (y-axis) against values of predictor(s) variable(s) (x-axis and colors).\nSo in this case this is just showing the effect when penguins take on the value of male"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#tables-with-multiple-models",
    "href": "slides/data-analysis-in-R.html#tables-with-multiple-models",
    "title": "Data Analysis in R",
    "section": "Tables with multiple models",
    "text": "Tables with multiple models\n\nmodelsummary(list(peng_naive, peng_adjust, pengs_interact_marg),\n             stars = TRUE,\n             gof_omit = \".*\", ## omitting all goodness of fit for space\n             coef_map = c(\"bill_length_mm\" = \"Bill Lenth(mm)\",\n                          \"flipper_length_mm\" = \"Flipper Lenth(mm)\",\n                          \"bill_length_mm x sexmale\" = \"Bill Length(mm) x Male Penguin\"),\n             vcov = \"robust\",\n             note = \"Robust Standard errors in Parenthsis\")\n\n\n\n \n  \n      \n    Model 1 \n    Model 2 \n    Model 3 \n  \n \n\n  \n    Bill Lenth(mm) \n    87.415*** \n    60.117*** \n    17.813* \n  \n  \n     \n    (6.898) \n    (6.519) \n    (6.970) \n  \n  \n    Flipper Lenth(mm) \n     \n    27.544*** \n    17.354*** \n  \n  \n     \n     \n    (3.095) \n    (2.902) \n  \n\n\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n\n Robust Standard errors in Parenthsis"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#fixed-effects",
    "href": "slides/data-analysis-in-R.html#fixed-effects",
    "title": "Data Analysis in R",
    "section": "Fixed Effects",
    "text": "Fixed Effects\n\nlibrary(plm)\n\n\npenguins_feols = feols(body_mass_g ~  bill_length_mm + flipper_length_mm | species + island,\n  data =  peng_sans_miss_tidy)\n\npeng_stand = lm(body_mass_g ~  bill_length_mm + flipper_length_mm + factor(species) + factor(island),\n  data = peng_sans_miss_tidy)\n\npeng_plm = plm(body_mass_g ~  bill_length_mm + flipper_length_mm  + island,\n  data = peng_sans_miss_tidy,\n  index = c(\"species\"),\n  model = \"within\")\n\n\nThere are more than a few ways to get a fixed effects regression in R my prefered way is using fixest. Because a species and island fixed effect would be a bit ridicoulous we can use star wars to demonstrate our point a bit\nTo sort of get on with the rest of the workshop I will not spend to much time on this. But it is stupid fast and lets you do a whole lot. It does not have random effects tho. PLM is really useful for lots of other common panel estimators. But fixest is still the winner when it comes to] fixed effects models."
  },
  {
    "objectID": "slides/data-analysis-in-R.html#generating-predictions",
    "href": "slides/data-analysis-in-R.html#generating-predictions",
    "title": "Data Analysis in R",
    "section": "Generating Predictions",
    "text": "Generating Predictions\n\nset.seed(1994)\n\npenguins_prediction = mutate(penguins, id = row_number()) |> \ndrop_na()\n\npeng_train = penguins_prediction |> \n  sample_frac(0.7)\n\npeng_test = anti_join(penguins_prediction, peng_train, by = \"id\")\n\n\npeng_test$data_set = \"Test\"\n\n\npenguins_training_model = lm(body_mass_g ~ bill_length_mm, data = peng_train)\n\npenguins_train_plot = augment(penguins_training_model, \n                      interval = \"prediction\") |>\n                      select(.fitted, .lower, .upper, body_mass_g, bill_length_mm) |>\n                      mutate(data_set = \"train\")\n\npeng_predict_train = augment(penguins_training_model,\n                           interval = \"prediction\",\n                           newdata = peng_test) |> \n  select(.fitted, .lower, .upper,  body_mass_g, bill_length_mm, data_set)\n\nall_together = rbind(penguins_train_plot,\n                    peng_predict_train)"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#visual-inspection",
    "href": "slides/data-analysis-in-R.html#visual-inspection",
    "title": "Data Analysis in R",
    "section": "Visual Inspection",
    "text": "Visual Inspection\n\n\nggplot(all_together,\n  aes(x = bill_length_mm,\n    y = body_mass_g,\n   color = data_set,\n    fill = data_set)) +\n  geom_point(alpha = 0.7) +\n  geom_line(aes(y = .fitted)) +\n  geom_ribbon(aes(ymin = .lower,\n             ymax = .upper),\n               alpha = 0.3,\n               col = NA) +\n  scale_color_discrete(name = \"Training sample?\",\n                       aesthetics = c(\"color\",\n                                      \"fill\")) +\n  theme_allen_bw()"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#maximum-likelihood-estimation-machine-learning",
    "href": "slides/data-analysis-in-R.html#maximum-likelihood-estimation-machine-learning",
    "title": "Data Analysis in R",
    "section": "Maximum Likelihood Estimation  Machine Learning ",
    "text": "Maximum Likelihood Estimation  Machine Learning \n\nR comes with a whole host of maximum likelihood estimators(MLE)\n\nAs well as user written packages\n\nYou will also probably want to load marginaleffects to get marginal effects of your model\nemmeans is also a solid package for getting marginal effects\nNote: They do differ on what they can do and how they generate marginal effects\n\nAndrew Heiss has a nice summary of this at the bottom of this blog post"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#the-data-we-are-using",
    "href": "slides/data-analysis-in-R.html#the-data-we-are-using",
    "title": "Data Analysis in R",
    "section": "The data we are using",
    "text": "The data we are using\n\npenguins_glm = penguins |> \n  drop_na() |> \n  mutate(big_penguin = ifelse(body_mass_g > median(body_mass_g), 1,0),\n         big_penguin_logic = ifelse(body_mass_g > median(body_mass_g), TRUE, FALSE),\n         female = ifelse(sex == \"female\", TRUE, FALSE))"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#lets-see-what-this-looks-like",
    "href": "slides/data-analysis-in-R.html#lets-see-what-this-looks-like",
    "title": "Data Analysis in R",
    "section": "Lets See What this looks like",
    "text": "Lets See What this looks like\n\n\n\nggplot(penguins_glm, aes(x = bill_length_mm,\n                         y = big_penguin)) +\n  geom_point() +\n  geom_smooth(method = \"glm\",\n              method.args = list(family = binomial(link = \"logit\"))) +\n  theme_allen_bw()"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#our-model",
    "href": "slides/data-analysis-in-R.html#our-model",
    "title": "Data Analysis in R",
    "section": "Our Model",
    "text": "Our Model\n\nbase_model = glm(big_penguin ~ bill_length_mm + flipper_length_mm + species + female,\n                 data = penguins_glm,\n                 family = binomial(link = \"logit\"))\n\nsummary(base_model)\n\n\nCall:\nglm(formula = big_penguin ~ bill_length_mm + flipper_length_mm + \n    species + female, family = binomial(link = \"logit\"), data = penguins_glm)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.5554  -0.1754  -0.0458   0.1525   3.3236  \n\nCoefficients:\n                   Estimate Std. Error z value Pr(>|z|)    \n(Intercept)       -28.21658    7.65713  -3.685 0.000229 ***\nbill_length_mm      0.16149    0.10149   1.591 0.111593    \nflipper_length_mm   0.11095    0.03734   2.971 0.002966 ** \nspeciesChinstrap   -3.06047    1.16962  -2.617 0.008880 ** \nspeciesGentoo       4.77533    1.71546   2.784 0.005374 ** \nfemaleTRUE         -3.30482    1.07008  -3.088 0.002013 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 461.27  on 332  degrees of freedom\nResidual deviance: 147.40  on 327  degrees of freedom\nAIC: 159.4\n\nNumber of Fisher Scoring iterations: 8"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#average-marginal-effects",
    "href": "slides/data-analysis-in-R.html#average-marginal-effects",
    "title": "Data Analysis in R",
    "section": "Average Marginal Effects",
    "text": "Average Marginal Effects\n\n\n\navg_marginal_effect = base_model |> \n  marginaleffects()  |> \n  tidy()\n\nggplot(data = filter(avg_marginal_effect, !term == \"species\"),\n       aes(x = estimate,\n           y = term)) +\n  geom_pointrange(aes(xmin = conf.low,\n                      xmax = conf.high)) +\n  geom_vline(xintercept = 0) +\n  theme_allen_minimal()"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#user-specified-values",
    "href": "slides/data-analysis-in-R.html#user-specified-values",
    "title": "Data Analysis in R",
    "section": "User Specified Values",
    "text": "User Specified Values\n\n\nemmeans\n\npredictions_emmeans = base_model |> \n emmeans(~ bill_length_mm + species, var = \"bill_length_mm\",\n                   at = list(bill_length_mm = seq(0, 60,1)),\n                   regrid = \"response\") |> \n  as_tibble()\ncolors_plot = c(\"##f0be3d\", \"##931e18\", \"##247d3f\")\n\nggplot(predictions_emmeans, aes(x = bill_length_mm, y = prob, color = species)) +\n  geom_line() + \n  labs(x = \"Bill Length(millimeters)\", y = \"Predicted Probablity of Being a Big Penguin\",\n       color = NULL) +\n  theme(legend.position = \"bottom\") +\n  theme_allen_minimal() +\n  scale_color_manual(values = colors_plot)\n\n\nMarginal Effects\n\np  = plot_cap(base_model, condition = c(\"bill_length_mm\", \"species\"), conf_level = 0.01) ## can be saved and changed with ggplot options \n\np + \n  labs(x = \"Bill Length(millimeters)\", y = \"Predicted Probablity of Being a Big Penguin\",\n       color = NULL, fill = NULL,\n       caption = \"By Default plot_cap plots confidence intervals \\nso you have you make them really small\") +\n  theme(legend.position = \"bottom\") +\n  theme_allen_minimal() +\n  scale_color_manual(values = colors_plot)"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#results",
    "href": "slides/data-analysis-in-R.html#results",
    "title": "Data Analysis in R",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#packages-you-will-need-1",
    "href": "slides/data-analysis-in-R.html#packages-you-will-need-1",
    "title": "Data Analysis in R",
    "section": "Packages You Will Need",
    "text": "Packages You Will Need\n\nset.seed(1994) # make things reproducible\nlibrary(ISLR2)\nlibrary(caret)\nlibrary(leaps)\nlibrary(mlbench)\nlibrary(tidymodels)\n\n\nNote caret and tidymodels have some namespace conflicts.\n\nI personally would prefer to load one or the other\n\n\n\n\n\n\n\n\nImportant\n\n\nThe Elements of Statistical Learning has all the R and Python code located at this website.\n\n\n\n\nI generally am not a machine learning person. So I am just going to show you the basics of a machine learning workflow in R."
  },
  {
    "objectID": "slides/data-analysis-in-R.html#creating-training-and-test-sets",
    "href": "slides/data-analysis-in-R.html#creating-training-and-test-sets",
    "title": "Data Analysis in R",
    "section": "Creating Training and Test Sets",
    "text": "Creating Training and Test Sets\n\n\nCaret\n\n# Preprocess data  \npeng =  penguins |> drop_na()\n\npeng_caret = as.data.frame(peng)  \n\ntrain_index_caret = caret::createDataPartition(peng_caret$body_mass_g, p = 0.7, list = FALSE, times = 1)\n\ntrain_caret = peng_caret[train_index_caret,]\n\ntest_caret = peng_caret[-train_index_caret,]\n\n\nTidymodels\n\npeng_models =  rsample::initial_split(peng, prop = 0.7)\n\npeng_tidy_train = rsample::training(peng_models)\n\npeng_tidy_test = rsample::testing(peng_models)"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#create-cross-validations",
    "href": "slides/data-analysis-in-R.html#create-cross-validations",
    "title": "Data Analysis in R",
    "section": "Create Cross Validations",
    "text": "Create Cross Validations\n\n\nCaret\n\npeng_caret_cv = trainControl(method = \"cv\",\n      number = 10)\n\n\nTidymodels\n\npeng_cv_tidy = vfold_cv(peng_tidy_train, v = 10, strata = body_mass_g)"
  },
  {
    "objectID": "slides/data-analysis-in-R.html#fit-a-lasso-regression",
    "href": "slides/data-analysis-in-R.html#fit-a-lasso-regression",
    "title": "Data Analysis in R",
    "section": "Fit a lasso regression",
    "text": "Fit a lasso regression\n\nlasso_caret = train(body_mass_g ~ .,\n                 data = train_caret,\n                 method = \"lasso\",\n                 trControl = peng_caret_cv)\n\n\n\ntuning_grid_caret = data.frame(.fraction = 10^seq(-2, -1, length.out = 10))\n\n\n\nlasso_caret_tune = train(body_mass_g ~ .,\n                 data = train_caret,\n                 method = \"lasso\",\n                 trControl = peng_caret_cv,\n                 tuneGrid = tuning_grid_caret)\n\npredictions_caret = predict(lasso_caret_tune, newdata = test_caret)\n\n\nAll in all machine learning in R works. I just have not done it all that much. I have mostly done strucutural topic mo"
  },
  {
    "objectID": "slides/data-cleaning-data-table.html#packages-you-will-need",
    "href": "slides/data-cleaning-data-table.html#packages-you-will-need",
    "title": "Data Cleaning in R",
    "section": "Packages You Will Need",
    "text": "Packages You Will Need\n\ninstall.packages(\"pacman\")\n\npacman::p_load(\"palmerpenguins\",\"data.table\", install = TRUE)\n\n\ndata(\"penguins\")\n\ndata(\"starwars\", package = \"dplyr\" )\n## Or \nstarwars = read_csv(\"data/starwars.csv\")\npenguins = read_csv(\"data/penguins.csv\")\n\n\n## Or \nstarwars = fread(\"data/starwars.csv\")\npenguins = fread(\"data/penguins.csv\")"
  },
  {
    "objectID": "slides/data-cleaning-data-table.html#data.table-pros",
    "href": "slides/data-cleaning-data-table.html#data.table-pros",
    "title": "Data Cleaning in R",
    "section": "data.table pros",
    "text": "data.table pros\n\nmore concise(as we will see)\nless dependencies\nfaster(as our data gets bigger)"
  },
  {
    "objectID": "slides/data-cleaning-data-table.html#a-comparison-of-tasks",
    "href": "slides/data-cleaning-data-table.html#a-comparison-of-tasks",
    "title": "Data Cleaning in R",
    "section": "A comparison of tasks",
    "text": "A comparison of tasks\n\n## tidyverse \npenguins |>\nfilter(species == \"Chinstrap\") |>\ngroup_by(island) |>\nsummarise(mean_weight = mean(body_mass_g, na.rm = TRUE))\n\n## data.table\npenguins_dt = as.data.table(penguins)\n\npenguins_dt[species == \"Chinstrap\",\n mean(body_mass_g, na.rm = TRUE),\n  by = island]\n\n\n\n\n# A tibble: 1 × 2\n  island mean_weight\n  <fct>        <dbl>\n1 Dream        3733.\n\n\n   island       V1\n1:  Dream 3733.088"
  },
  {
    "objectID": "slides/data-cleaning-data-table.html#a-statement-from-josh",
    "href": "slides/data-cleaning-data-table.html#a-statement-from-josh",
    "title": "Data Cleaning in R",
    "section": "A Statement from Josh",
    "text": "A Statement from Josh\n\nI really wanted to learn data.table which is the motivation for this slide show\nThis is my attempt to do it\nBoth packages are awesome\nI have zero stakes in either direction\nBoth suit different data cleaning tasks well\nTheir are compelling reasons to favor one over the other\nIf one is more intuitive to you that is great use the one you feel more comfortable with\nKnowing both is awesome\n\nI don’t promote evangelism of one over the other"
  },
  {
    "objectID": "slides/data-cleaning-data-table.html#what-is-the-difference",
    "href": "slides/data-cleaning-data-table.html#what-is-the-difference",
    "title": "Data Cleaning in R",
    "section": "What is the difference?",
    "text": "What is the difference?\n\n\nBoth work with an “enhanced” version of a data.frame\n\nFor dplyr this is called a tibble\nFor data.table this is called a data.table\n\nThe main difference(from what I understand) is that data.table tries to modify by reference as much as possible\nWhat does that mean?\nThink about about what the pipe.\nnew_object <- object |> dplyr_verb(thing we want to do) |> dplyr_verb(next thing we want to do)\nThis relies on a Copy-on-modify approach\n\n\nCopy-on-modify: Creates a copy of your data. Implies extra computational overhead(slower)\nModify-in-place: Avoids creating copies and simply cahnges the data where it sits in memory(faster)"
  },
  {
    "objectID": "slides/data-cleaning-data-table.html#the-common-syntax-of-data.table",
    "href": "slides/data-cleaning-data-table.html#the-common-syntax-of-data.table",
    "title": "Data Cleaning in R",
    "section": "The Common Syntax of data.table",
    "text": "The Common Syntax of data.table\n\n\n\nDT[i, j, by]\n\n\ni = The rows you want to work with\nj = The columns you want to work with\nby = a grouping variable\n\n\n\n\n\nThe Dplyr equivalents\n\n\nfilter;slice;arrange\nselect, mutate, summarise\ngroup_by"
  },
  {
    "objectID": "slides/data-cleaning-data-table.html#using-tests",
    "href": "slides/data-cleaning-data-table.html#using-tests",
    "title": "Data Cleaning in R",
    "section": "Using tests",
    "text": "Using tests\n\npenguins_dt[species %in% c(\"Chinstrap\",\"Gentoo\") | body_mass_g <= 4050]\n\n\n\n   species body_mass_g\n1:  Adelie        3750\n2:  Adelie        3800\n3:  Adelie        3250\n4:  Adelie        3450\n5:  Adelie        3650"
  },
  {
    "objectID": "slides/data-cleaning-data-table.html#using-regular-expressions",
    "href": "slides/data-cleaning-data-table.html#using-regular-expressions",
    "title": "Data Cleaning in R",
    "section": "Using regular expressions",
    "text": "Using regular expressions\n\nstarwars_dt = as.data.table(starwars)\n\nstarwars_dt[grepl(\"Skywalker\", name)]\n\n\n\n               name\n1:   Luke Skywalker\n2: Anakin Skywalker\n3:   Shmi Skywalker"
  },
  {
    "objectID": "slides/data-cleaning-data-table.html#finding-missing-data",
    "href": "slides/data-cleaning-data-table.html#finding-missing-data",
    "title": "Data Cleaning in R",
    "section": "Finding Missing Data",
    "text": "Finding Missing Data\n\npenguins_dt[is.na(sex)][]\n\n\n\n    species  sex\n 1:  Adelie <NA>\n 2:  Adelie <NA>\n 3:  Adelie <NA>\n 4:  Adelie <NA>\n 5:  Adelie <NA>\n 6:  Adelie <NA>\n 7:  Gentoo <NA>\n 8:  Gentoo <NA>\n 9:  Gentoo <NA>\n10:  Gentoo <NA>\n11:  Gentoo <NA>"
  },
  {
    "objectID": "slides/data-cleaning-data-table.html#finiding-non-missing-data",
    "href": "slides/data-cleaning-data-table.html#finiding-non-missing-data",
    "title": "Data Cleaning in R",
    "section": "Finiding Non-missing Data",
    "text": "Finiding Non-missing Data\n\npenguins_dt[!is.na(sex)][]\n\n\n\n       species    sex\n  1:    Adelie   male\n  2:    Adelie female\n  3:    Adelie female\n  4:    Adelie female\n  5:    Adelie   male\n ---                 \n329: Chinstrap   male\n330: Chinstrap female\n331: Chinstrap   male\n332: Chinstrap   male\n333: Chinstrap female"
  },
  {
    "objectID": "slides/data-cleaning-data-table.html#the-basics",
    "href": "slides/data-cleaning-data-table.html#the-basics",
    "title": "Data Cleaning in R",
    "section": "The basics",
    "text": "The basics\n\nLike Dplyr we can use column postion or name\n\n\n\npenguins_dt[, 1:3]\n\n\n\n   species    island bill_length_mm\n1:  Adelie Torgersen           39.1\n2:  Adelie Torgersen           39.5\n\n\n\n\n\npenguins_dt[, c(\"body_mass_g\", \"species\")]\n\n\n\n   body_mass_g species\n1:        3750  Adelie\n2:        3800  Adelie"
  },
  {
    "objectID": "slides/data-cleaning-data-table.html#excluding-using--",
    "href": "slides/data-cleaning-data-table.html#excluding-using--",
    "title": "Data Cleaning in R",
    "section": "Excluding Using -",
    "text": "Excluding Using -\n\npenguins_dt[,-c(1:3)]\n\n\n\n   bill_depth_mm flipper_length_mm body_mass_g    sex year\n1:          18.7               181        3750   male 2007\n2:          17.4               186        3800 female 2007\n3:          18.0               195        3250 female 2007\n\n\n\npenguins_dt[,-c(\"body_mass_g\",\"species\")]\n\n\n\n      island bill_length_mm bill_depth_mm flipper_length_mm    sex year\n1: Torgersen           39.1          18.7               181   male 2007\n2: Torgersen           39.5          17.4               186 female 2007"
  },
  {
    "objectID": "slides/data-cleaning-data-table.html#in-data.table",
    "href": "slides/data-cleaning-data-table.html#in-data.table",
    "title": "Data Cleaning in R",
    "section": ". in data.table",
    "text": ". in data.table\n\nThe . serves as a shorthand for c and list\nSo we can put . in the j postition letting us use less characters\n\n\npenguins_dt[, .(body_mass_g, island)]\n\n\n\n   body_mass_g    island\n1:        3750 Torgersen\n2:        3800 Torgersen\n\n\n\npenguins_dt[, .(body_mass_g, island)]"
  },
  {
    "objectID": "slides/data-cleaning-data-table.html#regular-expressions",
    "href": "slides/data-cleaning-data-table.html#regular-expressions",
    "title": "Data Cleaning in R",
    "section": "Regular Expressions",
    "text": "Regular Expressions\nWe can still use regular expressions - They just look a little different\n\nstarwars_dt[, .SD , .SDcols = patterns(\"color\")][] \n\n\n\n\n   hair_color  skin_color eye_color\n1:      blond        fair      blue\n2:       <NA>        gold    yellow\n3:       <NA> white, blue       red\n4:       none       white    yellow\n5:      brown       light     brown"
  },
  {
    "objectID": "slides/data-cleaning-data-table.html#regular-expressionscont",
    "href": "slides/data-cleaning-data-table.html#regular-expressionscont",
    "title": "Data Cleaning in R",
    "section": "Regular Expressions(cont)",
    "text": "Regular Expressions(cont)\n\nWhat is going on with .SD is that data.table needs a place holder\n.SD stands for Subset of Data.table\n\nIf we tried to do this:\n\nstarwars_dt[, .SDcols = patterns(\"color\")]\n\nIt would return\n\n\n\nWarning in `[.data.table`(starwars_dt, , .SDcols = patterns(\"color\")): i and j\nare both missing so ignoring the other arguments. This warning will be upgraded\nto error in future.\n\n\n                     name height   mass    hair_color          skin_color\n 1:        Luke Skywalker    172   77.0         blond                fair\n 2:                 C-3PO    167   75.0          <NA>                gold\n 3:                 R2-D2     96   32.0          <NA>         white, blue\n 4:           Darth Vader    202  136.0          none               white\n 5:           Leia Organa    150   49.0         brown               light\n 6:             Owen Lars    178  120.0   brown, grey               light\n 7:    Beru Whitesun lars    165   75.0         brown               light\n 8:                 R5-D4     97   32.0          <NA>          white, red\n 9:     Biggs Darklighter    183   84.0         black               light\n10:        Obi-Wan Kenobi    182   77.0 auburn, white                fair\n11:      Anakin Skywalker    188   84.0         blond                fair\n12:        Wilhuff Tarkin    180     NA  auburn, grey                fair\n13:             Chewbacca    228  112.0         brown             unknown\n14:              Han Solo    180   80.0         brown                fair\n15:                Greedo    173   74.0          <NA>               green\n16: Jabba Desilijic Tiure    175 1358.0          <NA>    green-tan, brown\n17:        Wedge Antilles    170   77.0         brown                fair\n18:      Jek Tono Porkins    180  110.0         brown                fair\n19:                  Yoda     66   17.0         white               green\n20:             Palpatine    170   75.0          grey                pale\n21:             Boba Fett    183   78.2         black                fair\n22:                 IG-88    200  140.0          none               metal\n23:                 Bossk    190  113.0          none               green\n24:      Lando Calrissian    177   79.0         black                dark\n25:                 Lobot    175   79.0          none               light\n26:                Ackbar    180   83.0          none        brown mottle\n27:            Mon Mothma    150     NA        auburn                fair\n28:          Arvel Crynyd     NA     NA         brown                fair\n29: Wicket Systri Warrick     88   20.0         brown               brown\n30:             Nien Nunb    160   68.0          none                grey\n31:          Qui-Gon Jinn    193   89.0         brown                fair\n32:           Nute Gunray    191   90.0          none       mottled green\n33:         Finis Valorum    170     NA         blond                fair\n34:         Jar Jar Binks    196   66.0          none              orange\n35:          Roos Tarpals    224   82.0          none                grey\n36:            Rugor Nass    206     NA          none               green\n37:              Ric Olié    183     NA         brown                fair\n38:                 Watto    137     NA         black          blue, grey\n39:               Sebulba    112   40.0          none           grey, red\n40:         Quarsh Panaka    183     NA         black                dark\n41:        Shmi Skywalker    163     NA         black                fair\n42:            Darth Maul    175   80.0          none                 red\n43:           Bib Fortuna    180     NA          none                pale\n44:           Ayla Secura    178   55.0          none                blue\n45:              Dud Bolt     94   45.0          none          blue, grey\n46:               Gasgano    122     NA          none         white, blue\n47:        Ben Quadinaros    163   65.0          none grey, green, yellow\n48:            Mace Windu    188   84.0          none                dark\n49:          Ki-Adi-Mundi    198   82.0         white                pale\n50:             Kit Fisto    196   87.0          none               green\n51:             Eeth Koth    171     NA         black               brown\n52:            Adi Gallia    184   50.0          none                dark\n53:           Saesee Tiin    188     NA          none                pale\n54:           Yarael Poof    264     NA          none               white\n55:              Plo Koon    188   80.0          none              orange\n56:            Mas Amedda    196     NA          none                blue\n57:          Gregar Typho    185   85.0         black                dark\n58:                 Cordé    157     NA         brown               light\n59:           Cliegg Lars    183     NA         brown                fair\n60:     Poggle the Lesser    183   80.0          none               green\n61:       Luminara Unduli    170   56.2         black              yellow\n62:         Barriss Offee    166   50.0         black              yellow\n63:                 Dormé    165     NA         brown               light\n64:                 Dooku    193   80.0         white                fair\n65:   Bail Prestor Organa    191     NA         black                 tan\n66:            Jango Fett    183   79.0         black                 tan\n67:            Zam Wesell    168   55.0        blonde fair, green, yellow\n68:       Dexter Jettster    198  102.0          none               brown\n69:               Lama Su    229   88.0          none                grey\n70:               Taun We    213     NA          none                grey\n71:            Jocasta Nu    167     NA         white                fair\n72:         Ratts Tyerell     79   15.0          none          grey, blue\n73:                R4-P17     96     NA          none         silver, red\n74:            Wat Tambor    193   48.0          none         green, grey\n75:              San Hill    191     NA          none                grey\n76:              Shaak Ti    178   57.0          none    red, blue, white\n77:              Grievous    216  159.0          none        brown, white\n78:               Tarfful    234  136.0         brown               brown\n79:       Raymus Antilles    188   79.0         brown               light\n80:             Sly Moore    178   48.0          none                pale\n81:            Tion Medon    206   80.0          none                grey\n82:                  Finn     NA     NA         black                dark\n83:                   Rey     NA     NA         brown               light\n84:           Poe Dameron     NA     NA         brown               light\n85:                   BB8     NA     NA          none                none\n86:        Captain Phasma     NA     NA       unknown             unknown\n87:         Padmé Amidala    165   45.0         brown               light\n                     name height   mass    hair_color          skin_color\n        eye_color birth_year            sex    gender      homeworld\n 1:          blue       19.0           male masculine       Tatooine\n 2:        yellow      112.0           none masculine       Tatooine\n 3:           red       33.0           none masculine          Naboo\n 4:        yellow       41.9           male masculine       Tatooine\n 5:         brown       19.0         female  feminine       Alderaan\n 6:          blue       52.0           male masculine       Tatooine\n 7:          blue       47.0         female  feminine       Tatooine\n 8:           red         NA           none masculine       Tatooine\n 9:         brown       24.0           male masculine       Tatooine\n10:     blue-gray       57.0           male masculine        Stewjon\n11:          blue       41.9           male masculine       Tatooine\n12:          blue       64.0           male masculine         Eriadu\n13:          blue      200.0           male masculine       Kashyyyk\n14:         brown       29.0           male masculine       Corellia\n15:         black       44.0           male masculine          Rodia\n16:        orange      600.0 hermaphroditic masculine      Nal Hutta\n17:         hazel       21.0           male masculine       Corellia\n18:          blue         NA           male masculine     Bestine IV\n19:         brown      896.0           male masculine           <NA>\n20:        yellow       82.0           male masculine          Naboo\n21:         brown       31.5           male masculine         Kamino\n22:           red       15.0           none masculine           <NA>\n23:           red       53.0           male masculine      Trandosha\n24:         brown       31.0           male masculine        Socorro\n25:          blue       37.0           male masculine         Bespin\n26:        orange       41.0           male masculine       Mon Cala\n27:          blue       48.0         female  feminine      Chandrila\n28:         brown         NA           male masculine           <NA>\n29:         brown        8.0           male masculine          Endor\n30:         black         NA           male masculine        Sullust\n31:          blue       92.0           male masculine           <NA>\n32:           red         NA           male masculine Cato Neimoidia\n33:          blue       91.0           male masculine      Coruscant\n34:        orange       52.0           male masculine          Naboo\n35:        orange         NA           male masculine          Naboo\n36:        orange         NA           male masculine          Naboo\n37:          blue         NA           <NA>      <NA>          Naboo\n38:        yellow         NA           male masculine       Toydaria\n39:        orange         NA           male masculine      Malastare\n40:         brown       62.0           <NA>      <NA>          Naboo\n41:         brown       72.0         female  feminine       Tatooine\n42:        yellow       54.0           male masculine       Dathomir\n43:          pink         NA           male masculine         Ryloth\n44:         hazel       48.0         female  feminine         Ryloth\n45:        yellow         NA           male masculine        Vulpter\n46:         black         NA           male masculine        Troiken\n47:        orange         NA           male masculine           Tund\n48:         brown       72.0           male masculine     Haruun Kal\n49:        yellow       92.0           male masculine          Cerea\n50:         black         NA           male masculine    Glee Anselm\n51:         brown         NA           male masculine       Iridonia\n52:          blue         NA         female  feminine      Coruscant\n53:        orange         NA           male masculine        Iktotch\n54:        yellow         NA           male masculine        Quermia\n55:         black       22.0           male masculine          Dorin\n56:          blue         NA           male masculine       Champala\n57:         brown         NA           male masculine          Naboo\n58:         brown         NA         female  feminine          Naboo\n59:          blue       82.0           male masculine       Tatooine\n60:        yellow         NA           male masculine       Geonosis\n61:          blue       58.0         female  feminine         Mirial\n62:          blue       40.0         female  feminine         Mirial\n63:         brown         NA         female  feminine          Naboo\n64:         brown      102.0           male masculine        Serenno\n65:         brown       67.0           male masculine       Alderaan\n66:         brown       66.0           male masculine   Concord Dawn\n67:        yellow         NA         female  feminine          Zolan\n68:        yellow         NA           male masculine           Ojom\n69:         black         NA           male masculine         Kamino\n70:         black         NA         female  feminine         Kamino\n71:          blue         NA         female  feminine      Coruscant\n72:       unknown         NA           male masculine    Aleen Minor\n73:     red, blue         NA           none  feminine           <NA>\n74:       unknown         NA           male masculine          Skako\n75:          gold         NA           male masculine     Muunilinst\n76:         black         NA         female  feminine          Shili\n77: green, yellow         NA           male masculine          Kalee\n78:          blue         NA           male masculine       Kashyyyk\n79:         brown         NA           male masculine       Alderaan\n80:         white         NA           <NA>      <NA>         Umbara\n81:         black         NA           male masculine         Utapau\n82:          dark         NA           male masculine           <NA>\n83:         hazel         NA         female  feminine           <NA>\n84:         brown         NA           male masculine           <NA>\n85:         black         NA           none masculine           <NA>\n86:       unknown         NA           <NA>      <NA>           <NA>\n87:         brown       46.0         female  feminine          Naboo\n        eye_color birth_year            sex    gender      homeworld\n           species\n 1:          Human\n 2:          Droid\n 3:          Droid\n 4:          Human\n 5:          Human\n 6:          Human\n 7:          Human\n 8:          Droid\n 9:          Human\n10:          Human\n11:          Human\n12:          Human\n13:        Wookiee\n14:          Human\n15:         Rodian\n16:           Hutt\n17:          Human\n18:          Human\n19: Yoda's species\n20:          Human\n21:          Human\n22:          Droid\n23:     Trandoshan\n24:          Human\n25:          Human\n26:   Mon Calamari\n27:          Human\n28:          Human\n29:           Ewok\n30:      Sullustan\n31:          Human\n32:      Neimodian\n33:          Human\n34:         Gungan\n35:         Gungan\n36:         Gungan\n37:           <NA>\n38:      Toydarian\n39:            Dug\n40:           <NA>\n41:          Human\n42:         Zabrak\n43:        Twi'lek\n44:        Twi'lek\n45:     Vulptereen\n46:          Xexto\n47:          Toong\n48:          Human\n49:         Cerean\n50:       Nautolan\n51:         Zabrak\n52:     Tholothian\n53:       Iktotchi\n54:       Quermian\n55:        Kel Dor\n56:       Chagrian\n57:          Human\n58:          Human\n59:          Human\n60:      Geonosian\n61:       Mirialan\n62:       Mirialan\n63:          Human\n64:          Human\n65:          Human\n66:          Human\n67:       Clawdite\n68:       Besalisk\n69:       Kaminoan\n70:       Kaminoan\n71:          Human\n72:         Aleena\n73:          Droid\n74:        Skakoan\n75:           Muun\n76:        Togruta\n77:        Kaleesh\n78:        Wookiee\n79:          Human\n80:           <NA>\n81:         Pau'an\n82:          Human\n83:          Human\n84:          Human\n85:          Droid\n86:           <NA>\n87:          Human\n           species\n                                                                                                                    films\n 1:                           The Empire Strikes Back,Revenge of the Sith,Return of the Jedi,A New Hope,The Force Awakens\n 2:     The Empire Strikes Back,Attack of the Clones,The Phantom Menace,Revenge of the Sith,Return of the Jedi,A New Hope\n 3: The Empire Strikes Back,Attack of the Clones,The Phantom Menace,Revenge of the Sith,Return of the Jedi,A New Hope,...\n 4:                                             The Empire Strikes Back,Revenge of the Sith,Return of the Jedi,A New Hope\n 5:                           The Empire Strikes Back,Revenge of the Sith,Return of the Jedi,A New Hope,The Force Awakens\n 6:                                                                   Attack of the Clones,Revenge of the Sith,A New Hope\n 7:                                                                   Attack of the Clones,Revenge of the Sith,A New Hope\n 8:                                                                                                            A New Hope\n 9:                                                                                                            A New Hope\n10:     The Empire Strikes Back,Attack of the Clones,The Phantom Menace,Revenge of the Sith,Return of the Jedi,A New Hope\n11:                                                           Attack of the Clones,The Phantom Menace,Revenge of the Sith\n12:                                                                                        Revenge of the Sith,A New Hope\n13:                           The Empire Strikes Back,Revenge of the Sith,Return of the Jedi,A New Hope,The Force Awakens\n14:                                               The Empire Strikes Back,Return of the Jedi,A New Hope,The Force Awakens\n15:                                                                                                            A New Hope\n16:                                                                      The Phantom Menace,Return of the Jedi,A New Hope\n17:                                                                 The Empire Strikes Back,Return of the Jedi,A New Hope\n18:                                                                                                            A New Hope\n19:                The Empire Strikes Back,Attack of the Clones,The Phantom Menace,Revenge of the Sith,Return of the Jedi\n20:                The Empire Strikes Back,Attack of the Clones,The Phantom Menace,Revenge of the Sith,Return of the Jedi\n21:                                                       The Empire Strikes Back,Attack of the Clones,Return of the Jedi\n22:                                                                                               The Empire Strikes Back\n23:                                                                                               The Empire Strikes Back\n24:                                                                            The Empire Strikes Back,Return of the Jedi\n25:                                                                                               The Empire Strikes Back\n26:                                                                                  Return of the Jedi,The Force Awakens\n27:                                                                                                    Return of the Jedi\n28:                                                                                                    Return of the Jedi\n29:                                                                                                    Return of the Jedi\n30:                                                                                                    Return of the Jedi\n31:                                                                                                    The Phantom Menace\n32:                                                           Attack of the Clones,The Phantom Menace,Revenge of the Sith\n33:                                                                                                    The Phantom Menace\n34:                                                                               Attack of the Clones,The Phantom Menace\n35:                                                                                                    The Phantom Menace\n36:                                                                                                    The Phantom Menace\n37:                                                                                                    The Phantom Menace\n38:                                                                               Attack of the Clones,The Phantom Menace\n39:                                                                                                    The Phantom Menace\n40:                                                                                                    The Phantom Menace\n41:                                                                               Attack of the Clones,The Phantom Menace\n42:                                                                                                    The Phantom Menace\n43:                                                                                                    Return of the Jedi\n44:                                                           Attack of the Clones,The Phantom Menace,Revenge of the Sith\n45:                                                                                                    The Phantom Menace\n46:                                                                                                    The Phantom Menace\n47:                                                                                                    The Phantom Menace\n48:                                                           Attack of the Clones,The Phantom Menace,Revenge of the Sith\n49:                                                           Attack of the Clones,The Phantom Menace,Revenge of the Sith\n50:                                                           Attack of the Clones,The Phantom Menace,Revenge of the Sith\n51:                                                                                The Phantom Menace,Revenge of the Sith\n52:                                                                                The Phantom Menace,Revenge of the Sith\n53:                                                                                The Phantom Menace,Revenge of the Sith\n54:                                                                                                    The Phantom Menace\n55:                                                           Attack of the Clones,The Phantom Menace,Revenge of the Sith\n56:                                                                               Attack of the Clones,The Phantom Menace\n57:                                                                                                  Attack of the Clones\n58:                                                                                                  Attack of the Clones\n59:                                                                                                  Attack of the Clones\n60:                                                                              Attack of the Clones,Revenge of the Sith\n61:                                                                              Attack of the Clones,Revenge of the Sith\n62:                                                                                                  Attack of the Clones\n63:                                                                                                  Attack of the Clones\n64:                                                                              Attack of the Clones,Revenge of the Sith\n65:                                                                              Attack of the Clones,Revenge of the Sith\n66:                                                                                                  Attack of the Clones\n67:                                                                                                  Attack of the Clones\n68:                                                                                                  Attack of the Clones\n69:                                                                                                  Attack of the Clones\n70:                                                                                                  Attack of the Clones\n71:                                                                                                  Attack of the Clones\n72:                                                                                                    The Phantom Menace\n73:                                                                              Attack of the Clones,Revenge of the Sith\n74:                                                                                                  Attack of the Clones\n75:                                                                                                  Attack of the Clones\n76:                                                                              Attack of the Clones,Revenge of the Sith\n77:                                                                                                   Revenge of the Sith\n78:                                                                                                   Revenge of the Sith\n79:                                                                                        Revenge of the Sith,A New Hope\n80:                                                                              Attack of the Clones,Revenge of the Sith\n81:                                                                                                   Revenge of the Sith\n82:                                                                                                     The Force Awakens\n83:                                                                                                     The Force Awakens\n84:                                                                                                     The Force Awakens\n85:                                                                                                     The Force Awakens\n86:                                                                                                     The Force Awakens\n87:                                                           Attack of the Clones,The Phantom Menace,Revenge of the Sith\n                                                                                                                    films\n                               vehicles\n 1:   Snowspeeder,Imperial Speeder Bike\n 2:                                    \n 3:                                    \n 4:                                    \n 5:               Imperial Speeder Bike\n 6:                                    \n 7:                                    \n 8:                                    \n 9:                                    \n10:                     Tribubble bongo\n11: Zephyr-G swoop bike,XJ-6 airspeeder\n12:                                    \n13:                               AT-ST\n14:                                    \n15:                                    \n16:                                    \n17:                         Snowspeeder\n18:                                    \n19:                                    \n20:                                    \n21:                                    \n22:                                    \n23:                                    \n24:                                    \n25:                                    \n26:                                    \n27:                                    \n28:                                    \n29:                                    \n30:                                    \n31:                     Tribubble bongo\n32:                                    \n33:                                    \n34:                                    \n35:                                    \n36:                                    \n37:                                    \n38:                                    \n39:                                    \n40:                                    \n41:                                    \n42:                        Sith speeder\n43:                                    \n44:                                    \n45:                                    \n46:                                    \n47:                                    \n48:                                    \n49:                                    \n50:                                    \n51:                                    \n52:                                    \n53:                                    \n54:                                    \n55:                                    \n56:                                    \n57:                                    \n58:                                    \n59:                                    \n60:                                    \n61:                                    \n62:                                    \n63:                                    \n64:                    Flitknot speeder\n65:                                    \n66:                                    \n67:          Koro-2 Exodrive airspeeder\n68:                                    \n69:                                    \n70:                                    \n71:                                    \n72:                                    \n73:                                    \n74:                                    \n75:                                    \n76:                                    \n77:         Tsmeu-6 personal wheel bike\n78:                                    \n79:                                    \n80:                                    \n81:                                    \n82:                                    \n83:                                    \n84:                                    \n85:                                    \n86:                                    \n87:                                    \n                               vehicles\n                                                                                               starships\n 1:                                                                              X-wing,Imperial shuttle\n 2:                                                                                                     \n 3:                                                                                                     \n 4:                                                                                      TIE Advanced x1\n 5:                                                                                                     \n 6:                                                                                                     \n 7:                                                                                                     \n 8:                                                                                                     \n 9:                                                                                               X-wing\n10: Jedi starfighter,Trade Federation cruiser,Naboo star skiff,Jedi Interceptor,Belbullab-22 starfighter\n11:                                              Trade Federation cruiser,Jedi Interceptor,Naboo fighter\n12:                                                                                                     \n13:                                                                   Millennium Falcon,Imperial shuttle\n14:                                                                   Millennium Falcon,Imperial shuttle\n15:                                                                                                     \n16:                                                                                                     \n17:                                                                                               X-wing\n18:                                                                                               X-wing\n19:                                                                                                     \n20:                                                                                                     \n21:                                                                                              Slave 1\n22:                                                                                                     \n23:                                                                                                     \n24:                                                                                    Millennium Falcon\n25:                                                                                                     \n26:                                                                                                     \n27:                                                                                                     \n28:                                                                                               A-wing\n29:                                                                                                     \n30:                                                                                    Millennium Falcon\n31:                                                                                                     \n32:                                                                                                     \n33:                                                                                                     \n34:                                                                                                     \n35:                                                                                                     \n36:                                                                                                     \n37:                                                                                 Naboo Royal Starship\n38:                                                                                                     \n39:                                                                                                     \n40:                                                                                                     \n41:                                                                                                     \n42:                                                                                             Scimitar\n43:                                                                                                     \n44:                                                                                                     \n45:                                                                                                     \n46:                                                                                                     \n47:                                                                                                     \n48:                                                                                                     \n49:                                                                                                     \n50:                                                                                                     \n51:                                                                                                     \n52:                                                                                                     \n53:                                                                                                     \n54:                                                                                                     \n55:                                                                                     Jedi starfighter\n56:                                                                                                     \n57:                                                                                        Naboo fighter\n58:                                                                                                     \n59:                                                                                                     \n60:                                                                                                     \n61:                                                                                                     \n62:                                                                                                     \n63:                                                                                                     \n64:                                                                                                     \n65:                                                                                                     \n66:                                                                                                     \n67:                                                                                                     \n68:                                                                                                     \n69:                                                                                                     \n70:                                                                                                     \n71:                                                                                                     \n72:                                                                                                     \n73:                                                                                                     \n74:                                                                                                     \n75:                                                                                                     \n76:                                                                                                     \n77:                                                                             Belbullab-22 starfighter\n78:                                                                                                     \n79:                                                                                                     \n80:                                                                                                     \n81:                                                                                                     \n82:                                                                                                     \n83:                                                                                                     \n84:                                                                                  T-70 X-wing fighter\n85:                                                                                                     \n86:                                                                                                     \n87:                                                   H-type Nubian yacht,Naboo star skiff,Naboo fighter\n                                                                                               starships"
  },
  {
    "objectID": "slides/data-cleaning-data-table.html#creating-new-variables",
    "href": "slides/data-cleaning-data-table.html#creating-new-variables",
    "title": "Data Cleaning in R",
    "section": "Creating New Variables",
    "text": "Creating New Variables\nOne Cool thing about data.table is that you do not need to reassign the object once you modify it\nThis creates a new variable\n\npenguins_dt[, body_mass_g_sq := body_mass_g^2]\n\nThis will just square body_mass_g in place\n\n\n\nYou will not see these changes unless you ask data.table to show you with an additional set of []\n\nstarwars_dt[,mass := mass^2][]\n\n\n\n             name height mass hair_color  skin_color eye_color birth_year  sex\n1: Luke Skywalker    172 5929      blond        fair      blue         19 male\n2:          C-3PO    167 5625       <NA>        gold    yellow        112 none\n3:          R2-D2     96 1024       <NA> white, blue       red         33 none\n      gender homeworld species\n1: masculine  Tatooine   Human\n2: masculine  Tatooine   Droid\n3: masculine     Naboo   Droid\n                                                                                                                   films\n1:                           The Empire Strikes Back,Revenge of the Sith,Return of the Jedi,A New Hope,The Force Awakens\n2:     The Empire Strikes Back,Attack of the Clones,The Phantom Menace,Revenge of the Sith,Return of the Jedi,A New Hope\n3: The Empire Strikes Back,Attack of the Clones,The Phantom Menace,Revenge of the Sith,Return of the Jedi,A New Hope,...\n                            vehicles               starships\n1: Snowspeeder,Imperial Speeder Bike X-wing,Imperial shuttle\n2:                                                          \n3:"
  },
  {
    "objectID": "slides/data-cleaning-data-table.html#multiple-operations",
    "href": "slides/data-cleaning-data-table.html#multiple-operations",
    "title": "Data Cleaning in R",
    "section": "Multiple Operations",
    "text": "Multiple Operations\n\nAs we saw with dplyr we can do this\n\n\npenguins |>\nmutate(square_mass = body_mass_g^2,\n       big_penguin = ifelse(square_mass >= median(square_mass, na.rm = TRUE) , TRUE, FALSE)) |>\n       select(species, square_mass, big_penguin) |>\n       head(5)\n\n# A tibble: 5 × 3\n  species square_mass big_penguin\n  <fct>         <dbl> <lgl>      \n1 Adelie     14062500 FALSE      \n2 Adelie     14440000 FALSE      \n3 Adelie     10562500 FALSE      \n4 Adelie           NA NA         \n5 Adelie     11902500 FALSE      \n\n\n\nIt would be nice if we can do this with data.table!"
  },
  {
    "objectID": "slides/data-cleaning-data-table.html#multiple-operations-1",
    "href": "slides/data-cleaning-data-table.html#multiple-operations-1",
    "title": "Data Cleaning in R",
    "section": "Multiple operations",
    "text": "Multiple operations\n\n\nGood news. We can! But there are some caveats\nWe can do this by chaining [] together\nOr using the magrittr pipe\nWe cannot do this by doing lots of stuff in our J column"
  },
  {
    "objectID": "slides/data-cleaning-data-table.html#multiple-operations-with",
    "href": "slides/data-cleaning-data-table.html#multiple-operations-with",
    "title": "Data Cleaning in R",
    "section": "Multiple Operations with []",
    "text": "Multiple Operations with []\n\npenguins_dt[, square_mass := body_mass_g^2][, big_peng := ifelse(square_mass >= median(square_mass, na.rm = TRUE) , TRUE, FALSE)]\n\n\n\n   species    island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1:  Adelie Torgersen           39.1          18.7               181        3750\n2:  Adelie Torgersen           39.5          17.4               186        3800\n3:  Adelie Torgersen           40.3          18.0               195        3250\n4:  Adelie Torgersen             NA            NA                NA          NA\n5:  Adelie Torgersen           36.7          19.3               193        3450\n      sex year body_mass_g_sq square_mass big_peng\n1:   male 2007       14062500    14062500    FALSE\n2: female 2007       14440000    14440000    FALSE\n3: female 2007       10562500    10562500    FALSE\n4:   <NA> 2007             NA          NA       NA\n5: female 2007       11902500    11902500    FALSE"
  },
  {
    "objectID": "slides/data-cleaning-data-table.html#multiple-operations-with-the-pipe",
    "href": "slides/data-cleaning-data-table.html#multiple-operations-with-the-pipe",
    "title": "Data Cleaning in R",
    "section": "Multiple Operations with the pipe",
    "text": "Multiple Operations with the pipe\n\npenguins_dt[, square_mass := body_mass_g^2] %>%\n.[, big_peng := ifelse(square_mass >= median(square_mass, na.rm = TRUE) , TRUE, FALSE)]\n\n\n\n   species    island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1:  Adelie Torgersen           39.1          18.7               181        3750\n2:  Adelie Torgersen           39.5          17.4               186        3800\n3:  Adelie Torgersen           40.3          18.0               195        3250\n4:  Adelie Torgersen             NA            NA                NA          NA\n5:  Adelie Torgersen           36.7          19.3               193        3450\n6:  Adelie Torgersen           39.3          20.6               190        3650\n      sex year body_mass_g_sq square_mass big_peng\n1:   male 2007       14062500    14062500    FALSE\n2: female 2007       14440000    14440000    FALSE\n3: female 2007       10562500    10562500    FALSE\n4:   <NA> 2007             NA          NA       NA\n5: female 2007       11902500    11902500    FALSE\n6:   male 2007       13322500    13322500    FALSE"
  },
  {
    "objectID": "slides/data-cleaning-data-table.html#our-last-slot-is-by",
    "href": "slides/data-cleaning-data-table.html#our-last-slot-is-by",
    "title": "Data Cleaning in R",
    "section": "Our last slot is by",
    "text": "Our last slot is by\n\nThis works broadly the same as our group_by argument\n\n\npenguins_dt[, mean(bill_length_mm, na.rm = TRUE), by = species]\n\n\n\n\n\n \n  \n    species \n    V1 \n  \n \n\n  \n    Adelie \n    38.79139 \n  \n  \n    Gentoo \n    47.50488 \n  \n  \n    Chinstrap \n    48.83382"
  },
  {
    "objectID": "slides/data-cleaning-data-table.html#summarising-across-multiple-columns",
    "href": "slides/data-cleaning-data-table.html#summarising-across-multiple-columns",
    "title": "Data Cleaning in R",
    "section": "Summarising across multiple columns",
    "text": "Summarising across multiple columns\n\nIn dplyr this is a bit more intuitive\n\n\npenguins |>\ngroup_by(species) |>\nsummarise(across(where(is.numeric),\n c(Mean = mean, Min = min, Max = max ),\n  na.rm = TRUE,\n .names = \"{.col}_{.fn}\"))\n\n\n\n\n\n \n  \n    species \n    bill_length_mm_Mean \n    bill_length_mm_Min \n    bill_length_mm_Max \n    bill_depth_mm_Mean \n    bill_depth_mm_Min \n    bill_depth_mm_Max \n    flipper_length_mm_Mean \n    flipper_length_mm_Min \n    flipper_length_mm_Max \n    body_mass_g_Mean \n    body_mass_g_Min \n    body_mass_g_Max \n    year_Mean \n    year_Min \n    year_Max \n  \n \n\n  \n    Adelie \n    38.79139 \n    32.1 \n    46.0 \n    18.34636 \n    15.5 \n    21.5 \n    189.9536 \n    172 \n    210 \n    3700.662 \n    2850 \n    4775 \n    2008.013 \n    2007 \n    2009 \n  \n  \n    Chinstrap \n    48.83382 \n    40.9 \n    58.0 \n    18.42059 \n    16.4 \n    20.8 \n    195.8235 \n    178 \n    212 \n    3733.088 \n    2700 \n    4800 \n    2007.971 \n    2007 \n    2009 \n  \n  \n    Gentoo \n    47.50488 \n    40.9 \n    59.6 \n    14.98211 \n    13.1 \n    17.3 \n    217.1870 \n    203 \n    231 \n    5076.016 \n    3950 \n    6300 \n    2008.081 \n    2007 \n    2009"
  },
  {
    "objectID": "slides/data-cleaning-data-table.html#summarising-multiple-columns",
    "href": "slides/data-cleaning-data-table.html#summarising-multiple-columns",
    "title": "Data Cleaning in R",
    "section": "Summarising Multiple Columns",
    "text": "Summarising Multiple Columns\n\npenguins_dt = as.data.table(penguins)\n\nsummary_fun = function(x) list(Mean = mean(x, na.rm = TRUE),\n             Min = min(x, na.rm = TRUE),\n              Max = max(x, na.rm = TRUE))\n\npenguins_dt[, lapply(.SD, summary_fun), .SDcols = is.numeric, by = species]\n\n\n\n\n\n \n  \n    species \n    bill_length_mm \n    bill_depth_mm \n    flipper_length_mm \n    body_mass_g \n    year \n  \n \n\n  \n    Adelie \n    38.79139 \n    18.34636 \n    189.9536 \n    3700.662 \n    2008.013 \n  \n  \n    Adelie \n    32.1 \n    15.5 \n    172 \n    2850 \n    2007 \n  \n  \n    Adelie \n    46 \n    21.5 \n    210 \n    4775 \n    2009 \n  \n  \n    Gentoo \n    47.50488 \n    14.98211 \n    217.187 \n    5076.016 \n    2008.081 \n  \n  \n    Gentoo \n    40.9 \n    13.1 \n    203 \n    3950 \n    2007 \n  \n  \n    Gentoo \n    59.6 \n    17.3 \n    231 \n    6300 \n    2009 \n  \n  \n    Chinstrap \n    48.83382 \n    18.42059 \n    195.8235 \n    3733.088 \n    2007.971 \n  \n  \n    Chinstrap \n    40.9 \n    16.4 \n    178 \n    2700 \n    2007 \n  \n  \n    Chinstrap \n    58 \n    20.8 \n    212 \n    4800 \n    2009"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#our-team",
    "href": "slides/Getting-Started-in-R.html#our-team",
    "title": "Getting Started in R",
    "section": "Our Team",
    "text": "Our Team\n\n\nAlso, we do not have the capacity to provide pre-scheduled frequent software tutoring that is divorced from a specific assignment or software troubleshooting task at hand – in other words, we are also not generalized software tutors. Our one-on-one software assistance is available to help you troubleshoot specific tasks or assignments. If you are seeking generalized software help, we direct you to our live workshops and our recorded workshops to gain the foundational skills for using various analytical software. Then, when or if the time comes that you have targeted software questions or specific issues to tackle related to a course assignment or research project, please feel free to contact us for one-on-one support."
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#get-ready-badges",
    "href": "slides/Getting-Started-in-R.html#get-ready-badges",
    "title": "Getting Started in R",
    "section": "Get Ready Badges",
    "text": "Get Ready Badges\n\n\nhttps://research.library.gsu.edu/dataservices/data-ready\nThese are awesome to share on social media i.e. linkedin which is a good signal"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#how-to-get-the-badges",
    "href": "slides/Getting-Started-in-R.html#how-to-get-the-badges",
    "title": "Getting Started in R",
    "section": "How To Get the Badges",
    "text": "How To Get the Badges"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#why-use-r",
    "href": "slides/Getting-Started-in-R.html#why-use-r",
    "title": "Getting Started in R",
    "section": "Why Use R?",
    "text": "Why Use R?\n\n\nPart of this first portion is just a sales pitch.\nR is a vastly popular language in the datascience industry. While it is less popular than sequel or Python it is still one of the most heavily demanded language from private industry.\nThis sort of makes sense once we dig into what they are used for and who uses them. Sequel stands for structured query language. You have probably heard of big data before. Think of the amount of data you generate from what ever apps you use. Think about your use every minute. Now think about the N of people in this workshop. Now think about all the people at Georgia State. The amount of data being generated in a few minutes. The max storage of excel file 1,048,576 rows by 16,384 columns. We could all pretty quickly overwhelm a single excel file. Enter SQL you can store large data bases in SQL and just as importanly that is how you get that data.\nPython is a general purpose programming language it is used for data analysis as well but it has applications in everything from web development to game development. Dropbox is basically just a ton of python code. Lots of people who grow up to be data scientists come from a CS background where you are introduced to python pretty early.\nHowever, if you simply add up the propietary softwares there are fewer industry available jobs for you or you students compartively."
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#why-r-and-rstudiocont",
    "href": "slides/Getting-Started-in-R.html#why-r-and-rstudiocont",
    "title": "Getting Started in R",
    "section": "Why R and RStudio?(cont)",
    "text": "Why R and RStudio?(cont)\nData science reasons\n\nAlongside Python, R has become the de facto language for data science.\n\nSee: The Impressive Growth of R, The Popularity of Data Science Software\n\nOpen-source (free!) with a global user-base spanning academia and industry.\n\nPersonal Benefits\n\nThe community is insanely nice\n\nEspecially compared to Python and Stata\n\nA great “first” language to learn\n\nSource: Google Data Analytics Professional Certificate\n\nSupports all types of statistical methods and data collection\n\n\nIn the data science industry R has enjoyed similar growth rates to python in popularity. R is kind of quirky compartively to python for a whole host of computer sciency reasons and just path dependency. Whatever the case there is no denying that both are hugely in demand skills not just in Silicon Valley but at places that produce data analysis or data visualization\nR is also becoming wildly popular in econ and political science because not only is it free but it is a great skill to have given the difficulties of the academic job market. I am political scientist so I will mostly making these references. The Rstudio team in particular has worked really hard on adding support for Python, Julia, and java.\nMuch of this workshops materials are based on materials people publicly share as long as they get the proper acknowledment. Again thank you Grant.\nLots of data is readily available in R. Census API, Twitter API. If you are into sports you can grab a ton of data to compute those fancy metrics you are into. I know there is also data on Rupaul’s drag race you can download."
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#r-rstudio-whats-the-difference",
    "href": "slides/Getting-Started-in-R.html#r-rstudio-whats-the-difference",
    "title": "Getting Started in R",
    "section": "R? Rstudio? Whats the Difference?",
    "text": "R? Rstudio? Whats the Difference?\n\nR is a statistical programming language\nRStudio is a convenient interface for R (an Integrated Developer Environment, IDE)\nAt its simplest:\n\nR is like a car’s engine\nRStudio is like a car’s dashboard\n\n\n\n\nThe most common way that we interact with R is through Rstudio you can technically run R by just opening R and typing in code. But most people do not do this. It is not especially friendly to work in there is no syntax highlighting no code completetion. There isnt even really an option to add keyboard shortcuts. It is kind of like a nascar can go real fast but it is not a comfortable drive.\nRstudio has lots of handy features that help you. Much like a car. If we didnt have the dashboard but still had the engine and some wheels and a steering we could drive the car if needed. However a car with a dashboard lets us figure out what the car is doing more easily"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#i-have-downloaded-rstudio-now-what",
    "href": "slides/Getting-Started-in-R.html#i-have-downloaded-rstudio-now-what",
    "title": "Getting Started in R",
    "section": "I Have Downloaded Rstudio Now What?",
    "text": "I Have Downloaded Rstudio Now What?\n\n\nHopefully You Have downloaded the Approriate Version of R and Rstudio for your Operating system. Now we need to know where we can start putting stuff. And it should look something like you can just start typing code into the console window if you want. But for the most part that is a bad idea. Because you will not be able to remember what you have done and more importantly you are going to have to redo it all each time.\nBest Practices are for you to work in an R script file. Later on in the semster we will hopefully have a workshop on getting started in Rmarkdown but for now just click on r script file if you are following along.\nIf you download the script for you will see lots of lots of # these let you comment your code. That way you can let yourself or a reader know what you did."
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#setting-your-working-directory",
    "href": "slides/Getting-Started-in-R.html#setting-your-working-directory",
    "title": "Getting Started in R",
    "section": "Setting Your Working Directory",
    "text": "Setting Your Working Directory\n\n\nYour working directory is where all your files live\nYou may know where your files are…\nBut R does not\nIf you want to use any data that does not come with a package you are going to need to tell R where it lives"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#cats-and-boxes",
    "href": "slides/Getting-Started-in-R.html#cats-and-boxes",
    "title": "Getting Started in R",
    "section": "Cats and Boxes",
    "text": "Cats and Boxes\n\n\nYou can put a box inside a box\nYou can put a cat inside a box\nYou can put a cat inside a box inside of a box\nYou cannot put a box inside a cat\nYou cannot put cat in a cat\n\n\nHow working directories work is that they are comprised of files and folders. You need to let R know what file is in what folder. You can also put a cat in a box, but you must never try to put a box in a cat. Boxes are like folders/directories, cats are like files. This sort of represents the hierarchy of this all. Folders come first than the last thing is the file itself\nWe are basically just telling R where things live. Kind of like how we put a strange address into the gps. We are telling it exactly where things live and what house number they are.\nWhen we organize our files into HW 1 or manuscript whatever name what we are doing is creating a new neighborhood on our computer. R will default to places it knows. Most commonly where it lives. In order to do something as simple as loading our dataset in R needs directions to this neighborhood"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#setting-your-working-directorycont",
    "href": "slides/Getting-Started-in-R.html#setting-your-working-directorycont",
    "title": "Getting Started in R",
    "section": "Setting Your Working Directory(cont)",
    "text": "Setting Your Working Directory(cont)\n\n\nSeeing What Working Directory You are Using\n\ngetwd()## The working directory where all the materials for the workshops live\n\n[1] \"/Users/josh/Dropbox/Research-Data-Services-Workshops/research-data-services-r-workshops/slides\"\n\n\n\nSetting Your Working Directory\n\nsetwd(\"your/working/directory/here/\") ## sets the working directory on mac\nsetwd(\"your\\working\\directory\\here\") ## sets the working directory on windows\n\n\n\n\nSo this is where the files for the workshop are living. The address is this file path right here. Once we set our working directory R will start trying to figure out where stuff is relative to this location. So if your data is in a folder named data in this working directoy you tell it that there is one additional turn to take and that is to the data folder. If the data is living in your downloads folder and your working directory is the one on the left. R is not going to know where the data is living and then return an error. This is kind of like when you give your friends directions and then tell them to call you if you get lost. That error message is R telling you that it is lost."
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#how-to-make-your-life-easier",
    "href": "slides/Getting-Started-in-R.html#how-to-make-your-life-easier",
    "title": "Getting Started in R",
    "section": "How To Make Your Life Easier",
    "text": "How To Make Your Life Easier\n\nsource: Jenny Bryan\nWhile setting your working directory manually is fine it is a lot more fragile. If you collaborate with people now or in the future they are going to have to manually set the working directory on their computer. It is not neccessarily reproducible or practical for a variety of reasons."
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#how-to-make-your-life-easier-1",
    "href": "slides/Getting-Started-in-R.html#how-to-make-your-life-easier-1",
    "title": "Getting Started in R",
    "section": "How To Make Your Life Easier",
    "text": "How To Make Your Life Easier\n\n\nWorking Directory for My Laptop\n\"/Users/josh/Dropbox/Research-Data-Services-Workshops/research-data-services-r-workshops/slides\" \n\n\nWorking Directory of My Office Computer\n\"/Volumes/6TB Raid 10/Dropbox/Research-Data-Services-Workshops/research-data-services-r-workshops/slides\"\n\n\n\n\nOften times collaborating with our selves is the first step. So these are two different working dirrectories I have. I could likely hack my way around it. However many of your colleagues that you work with do not have a similar set up. It is best to work in R projects.\nR projects are great because they will just set the working directory to wherever the project is living. The additional benefit is that it will restart R. Which is good because R will just keep lots of packages loaded in the background. This can cause what are called namespace conflicts. This used to be really bad with plyr and dplyr but plyr has since been retired. Truly problematic namespace conflicts are not something that I have experienced but they do happen. Fresh R sessions are the easiest way to avoid them"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#r-projects",
    "href": "slides/Getting-Started-in-R.html#r-projects",
    "title": "Getting Started in R",
    "section": "R Projects",
    "text": "R Projects"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#objects",
    "href": "slides/Getting-Started-in-R.html#objects",
    "title": "Getting Started in R",
    "section": "Objects",
    "text": "Objects\n\nEverything is an object\nEverything has a name\nYou do stuff with functions\nPackages(i.e. libraries) are homes to pre-written functions.\n\nYou can also write your own functions and in some cases should.\n\n\n\nEverything that exists in R is an object in the sense that it is a kind of data structure that can be manipulated. I think this is better understood with functions and expression\nBefore we start R is an object oriented programming(sort of) What this means is just how we are defining what things we have and how they relate to each other. A dog has various things associated with it. They are four legged have a good sense of smell, a member of the canine family, they eat a certain set of food.\nOnce we define what those things are and how they relate to each other R will figure out what class it is.\nWhat is this object to it. Once it figures this out this sets out strict limitations on what R can do with those objects but just as importantly it tells R what it can’t do with those objects. Think of like a set of tricks or in CS speak methods to do things. There are things we can do with dogs or to dogs that are acceptable. This differs from cats. Cats and dogs have similar attributes but they are different classes.\nWhile sometimes it is frustrating because sometimes you just want to do a thing it helps you protect you from yourself.\nReturning back to our pet metaphor. Each pet has a name and the thing we want it to do has names. Sit, stay, come here, hey you what are you doing in there. These are sort of like functions. We are manipulating the object."
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#install-and-loading-packages",
    "href": "slides/Getting-Started-in-R.html#install-and-loading-packages",
    "title": "Getting Started in R",
    "section": "Install and loading packages",
    "text": "Install and loading packages\n\nConsole or Script install.packages(\"package-i-need-to-install\")\n\nIn the case of multiple packages you can do install.packages(c(\"Packages\", \"I\", \"don't\",\"have\"))\n\nRStudio Click the “Packages” tab in the bottom-right window pane. Then click “Install” and search for these two packages.\n\n\n\nCredit for the Gif goes to Grant Mcdermott. As we will learn there are lots of ways to do one thing in R and the packages that are loaded when you open up R have lots of important things that you will use to just do some simple analysis or if you want to build your own functions."
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#install-and-loadcont.",
    "href": "slides/Getting-Started-in-R.html#install-and-loadcont.",
    "title": "Getting Started in R",
    "section": "Install and load(cont.)",
    "text": "Install and load(cont.)\nOnce the packages are installed we need load them into our R session with the library() function\n\n# We talk to ourselves using #\nlibrary(Package) \nlibrary(I)\nlibrary(JustInstalled)\n\nNotice too that you don’t need quotes around the package names any more.\n\nReason: R now recognises these packages as defined objects with given names. (“Everything in R is an object and everything has a name.”)"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#r-some-basics",
    "href": "slides/Getting-Started-in-R.html#r-some-basics",
    "title": "Getting Started in R",
    "section": "R Some Basics",
    "text": "R Some Basics"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#basic-maths",
    "href": "slides/Getting-Started-in-R.html#basic-maths",
    "title": "Getting Started in R",
    "section": "Basic Maths",
    "text": "Basic Maths\n\nR is equipped with lots of mathematical operations\n\n\n2+2 ## addition\n\n[1] 4\n\n4-2 ## subtaction\n\n[1] 2\n\n600*100 ##multiplication\n\n[1] 60000\n\n100/10 ##division\n\n[1] 10\n\n10*10/(3^4*2)-2 ## Pemdas \n\n[1] -1.382716\n\nlog(100)\n\n[1] 4.60517\n\nsqrt(100)\n\n[1] 10\n\n\n\nThe most common hazing ritual when learning R and object oriented programming is using R as calculator. Since we most often use it for advanced statistical analysis it should be able to handle lots things we can throw at it"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#basic-maths-1",
    "href": "slides/Getting-Started-in-R.html#basic-maths-1",
    "title": "Getting Started in R",
    "section": "Basic Maths",
    "text": "Basic Maths\nR is also equipped with modulo operations (integer division and remainders), matrix algebra, etc\n\n100 %/% 60 # How many whole hours in 100 minutes?\n\n[1] 1\n\n100 %% 60 # How many minutes are left over?\n\n[1] 40\n\n\n\nm <- matrix(1:8, nrow=2) # Don't worry about the <- for now \nn <- matrix(8:15, nrow=4) # this is just me creating matrices \n\nmat <- matrix(1:15, ncol = 5)\n\nm %*% n # Matrix multiplication\n\n     [,1] [,2]\n[1,]  162  226\n[2,]  200  280\n\nmat \n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    4    7   10   13\n[2,]    2    5    8   11   14\n[3,]    3    6    9   12   15\n\nt(mat) # transpose a matrix\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n[4,]   10   11   12\n[5,]   13   14   15\n\n\n\nThere are lots of other useful mathematical operations we can use in R to make our lives a lot easier or if you are in the postion of having to hand calculate them either because your professor is making you or you want to make sure your new cool estimator is available for other people to use you can do that. In this case we are doing this by “hand”."
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#logical-statements-booleans",
    "href": "slides/Getting-Started-in-R.html#logical-statements-booleans",
    "title": "Getting Started in R",
    "section": "Logical Statements & Booleans",
    "text": "Logical Statements & Booleans\n\n\n\n\nTest\n\n\nMeaning\n\n\nTest\n\n\nMeaning\n\n\n\n\nx < y\n\n\nLess than\n\n\nx %in% y\n\n\nIn set\n\n\n\n\nx > y\n\n\nGreater than\n\n\nis.na(x)\n\n\nIs missing\n\n\n\n\n==\n\n\nEqual to\n\n\n!is.na(x)\n\n\nIs not missing\n\n\n\n\nx <= y\n\n\nLess than or equal to\n\n\n\n\nx >= y\n\n\nGreater than or equal to\n\n\n\n\nx != y\n\n\nNot equal to\n\n\n\n\nx | y\n\n\nOr\n\n\n\n\nx & y\n\n\nAnd\n\n\n\n\n\nR comes with standard set of boolean operators these are the common ones that you will use or run into in the wild. Note that to say something equals something because in R = is used in 2 different ways either as passing arguments off to a function or for assignment.\nLogical expressions, like comparison expressions, return a true (1) or false (0) value when processed\nBooleans are basically just paired comparisions."
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#booleans-and-logicals-in-action",
    "href": "slides/Getting-Started-in-R.html#booleans-and-logicals-in-action",
    "title": "Getting Started in R",
    "section": "Booleans and Logicals in Action",
    "text": "Booleans and Logicals in Action\n\n1>2 \n\n[1] FALSE\n\n1<2\n\n[1] TRUE\n\n1 == 2\n\n[1] FALSE\n\n1 < 2 | 3 > 4 ## only one test needs to true to return true\n\n[1] TRUE\n\n1 < 2 & 3>4 ## both tests must be true to return true\n\n[1] FALSE"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#logicals-booleans-and-precedence",
    "href": "slides/Getting-Started-in-R.html#logicals-booleans-and-precedence",
    "title": "Getting Started in R",
    "section": "Logicals, Booleans, and Precedence",
    "text": "Logicals, Booleans, and Precedence\n\n\nR like most other programming languages will evaluate our logical operators(==, >, etc) before our booleans(|, &, etc).\n\n\n\n\n1 > 0.5 & 2\n\n[1] TRUE\n\n\n\n\n\n\nWhat’s happening here is that R is evaluating two separate “logical” statements:\n1 > 0.5, which is is obviously TRUE.\n2, which is TRUE(!) because R is “helpfully” converting it to as.logical(2).\nIt is way safer to make explicit what you are doing.\nIf your code is doing something weird it might just be because of precedence issues\n\nSee R Cookbook 2.11\n\n\n\n\n\n\n1 > 0.5 & 1 > 2\n\n[1] FALSE\n\n\n\nComputer operations have a pemdas of their own so when you are milling about in R it is important to remember how you set up your tests"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#other-useful-tricks",
    "href": "slides/Getting-Started-in-R.html#other-useful-tricks",
    "title": "Getting Started in R",
    "section": "Other Useful Tricks",
    "text": "Other Useful Tricks\nValue matching using %in%\nTo see whether an object is contained within (i.e. matches one of) a list of items, use %in%.\n\n4 %in% 1:10\n\n[1] TRUE\n\n4 %in% 5:10\n\n[1] FALSE"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#cool-now-what",
    "href": "slides/Getting-Started-in-R.html#cool-now-what",
    "title": "Getting Started in R",
    "section": "Cool Now What?",
    "text": "Cool Now What?\n\n\nWhile this is boring it opens up lots\nWe may need to set up a group of tests to do something to data.\nWe may need all this math stuff to create new variables\nHowever we need to Assign them to reuse them later in functions.\n\nIncluding datasets\n\n\n\n\nR as a calculator and evaluating whether 1 is greater than 2 gets very tiring like as soon as you start doing it and is hardly ever that practical."
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#assignment",
    "href": "slides/Getting-Started-in-R.html#assignment",
    "title": "Getting Started in R",
    "section": "Assignment",
    "text": "Assignment\n\nThe most popular assigment operator in R is <- which is just < followed by -\n\nread aloud as “gets”\n\n\n\na <- 2 + 2\n\na * 2\n\n[1] 8\n\nh <- \"harry potter\" # note that text needs to be wrapped in quotes \n\n\nYou can also use -> but this is far less common and makes me uncomfortable\n\n\n a^2 -> b\n\n\nAssignment might be a foregin concept to you if you have no coding experience or making the transition from excel or stata. Basically if you go through and copy and paste all this stuff into an R script it will run but we will not be able to use this.\nGets is just less than followed by a dash\nSo using our friend gets we can assign 2 + 2 to a or in the parlance of R a gets 2 + 2 this lets us reuse it later whether we want to perform additional maths or use it for later"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#assignmentcont",
    "href": "slides/Getting-Started-in-R.html#assignmentcont",
    "title": "Getting Started in R",
    "section": "Assignment(cont)",
    "text": "Assignment(cont)\n\n\nUsing = as an assignment operator also works and is the one I tend to use\n\nNote: = is also used to evaluate arguments within functions\n\n\n\n\n\nb = b * 2\n\nd = b/3\n\n\n\nTbh this is a matter of taste really.\n\nR added = in the 2000’s to make it easier for people coming from other object oriented programming languages\n\nJust keep it consistent..\nJust keep it consistent..or become ungovernable and use all three in one script.\n\nI am not responsible for you getting yelled at on Stackoverflow if you do this.\n\n\n\n\n\nTo be honest R purists are one of the last hold outs of using the gets operator essentially back in the main frame computer days there was a physical key that would do this for you. R is actually really really old so there are some quirks to it that bug lots of people. We are not going to get into it any further in this workshop but if you have prior experience in an object oriented language R will be a bit peculiar at times. Like in somne cases position of the argument does not matter indentation does not matter which will really throw you off if you are coming from python or vice versa\n\n\n\nMore discussion here and here"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#global-environmentcont",
    "href": "slides/Getting-Started-in-R.html#global-environmentcont",
    "title": "Getting Started in R",
    "section": "Global Environment(cont)",
    "text": "Global Environment(cont)\n\n\n\nError in mean(y): object 'y' not found\n\n\n\nGives us a hint out about what went wrong\n\n\n\n\n\n\nIf we look at the global environment Y does not exist as an object but lots of other stuff does that is not.\nBecause x and y live within e we need to tell that they belong to e so to fix it we would need to tell lm where y and x"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#fixing-our-issue",
    "href": "slides/Getting-Started-in-R.html#fixing-our-issue",
    "title": "Getting Started in R",
    "section": "Fixing Our Issue",
    "text": "Fixing Our Issue\n\n\nTo do this we need to index e to get to y\n\n\n\n\nmean(e$y)\n\n[1] 30.5\n\n\n\nIn R one of the biggest things is that being explicit about what name to use is how we do things. If you are not you are gonna get error messages at best.There are lots of ways"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#what-are-objects",
    "href": "slides/Getting-Started-in-R.html#what-are-objects",
    "title": "Getting Started in R",
    "section": "What are Objects?",
    "text": "What are Objects?\n\nObjects are what we work with in R\n\n\n\n [1] \"is.array\"                \"is.atomic\"              \n [3] \"is.call\"                 \"is.character\"           \n [5] \"is.complex\"              \"is.data.frame\"          \n [7] \"is.double\"               \"is.element\"             \n [9] \"is.environment\"          \"is.expression\"          \n[11] \"is.factor\"               \"is.finite\"              \n[13] \"is.function\"             \"is.infinite\"            \n[15] \"is.integer\"              \"is.language\"            \n[17] \"is.list\"                 \"is.loaded\"              \n[19] \"is.logical\"              \"is.matrix\"              \n[21] \"is.na\"                   \"is.na.data.frame\"       \n[23] \"is.na.numeric_version\"   \"is.na.POSIXlt\"          \n[25] \"is.na<-\"                 \"is.na<-.default\"        \n[27] \"is.na<-.factor\"          \"is.na<-.numeric_version\"\n[29] \"is.name\"                 \"is.nan\"                 \n[31] \"is.null\"                 \"is.numeric\"             \n[33] \"is.numeric_version\"      \"is.numeric.Date\"        \n[35] \"is.numeric.difftime\"     \"is.numeric.POSIXt\"      \n[37] \"is.object\"               \"is.ordered\"             \n[39] \"is.package_version\"      \"is.pairlist\"            \n[41] \"is.primitive\"            \"is.qr\"                  \n[43] \"is.R\"                    \"is.raw\"                 \n[45] \"is.recursive\"            \"is.single\"              \n[47] \"is.symbol\"               \"is.table\"               \n[49] \"is.unsorted\"             \"is.vector\"              \n[51] \"isa\"                     \"isatty\"                 \n[53] \"isBaseNamespace\"         \"isdebugged\"             \n[55] \"isFALSE\"                 \"isIncomplete\"           \n[57] \"isNamespace\"             \"isNamespaceLoaded\"      \n[59] \"isOpen\"                  \"isRestart\"              \n[61] \"isS4\"                    \"isSeekable\"             \n[63] \"isSymmetric\"             \"isSymmetric.matrix\"     \n[65] \"isTRUE\"                 \n\n\n\nUsing a bit of R we can see all the different logical tests that tests the class of objects in R. Here you can see that there are lots"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#the-usual-suspects",
    "href": "slides/Getting-Started-in-R.html#the-usual-suspects",
    "title": "Getting Started in R",
    "section": "The Usual Suspects",
    "text": "The Usual Suspects\n\nvectors\nmatrices\ndata frames\ndates\nlists\nfunctions\n\n\nf = cbind(a,b) ## this will just create a matrix \nclass(f)## lets us check the class of something \n\n[1] \"matrix\" \"array\" \n\ng = as.data.frame(f) ## converts to a different class\nstr(g) ## shows some info about the structure of the object\n\n'data.frame':   1 obs. of  2 variables:\n $ a: num 4\n $ b: num 32\n\ntypeof(g) ## shows how r is storing the object object\n\n[1] \"list\"\n\n\n\nThese are the ones that tend to come up most regularly used. You can also coerce once class to another. Using as will just force something to be a different class.\nWhy type of returns list: data.frame and data.table are both collections (lists) of items (vectors, if you will), each item of the same length (ie each column is an item in the list, internally to R anyway). This is why unlike in a matrix, columns can have different classes."
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#a-quick-aside-on-naming-stuff",
    "href": "slides/Getting-Started-in-R.html#a-quick-aside-on-naming-stuff",
    "title": "Getting Started in R",
    "section": "A Quick Aside on Naming Stuff",
    "text": "A Quick Aside on Naming Stuff\nThings we can never name stuff\nThe reason we can’t use any of these are because they are reserved for R\nif \nelse \nwhile \nfunction \nfor\nTRUE \nFALSE \nNULL \nInf \nNaN \nNA \n\n\nIf you have noticed every object in our little session has a name. But we should be careful about what we name our stuff. We can technically name our objects just about anything except these because they are reserved for R\n\n\nThere are more see this website for a more complete list"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#a-quick-aside-on-naming-stuffcont",
    "href": "slides/Getting-Started-in-R.html#a-quick-aside-on-naming-stuffcont",
    "title": "Getting Started in R",
    "section": "A Quick Aside on Naming Stuff(cont)",
    "text": "A Quick Aside on Naming Stuff(cont)\nSemi-reserved words\nFor simple things like assigning c = 4 and then doing d = c(1,2,3,4) R will be able to distinguish between assign c the value of 4 and the c that calls concatenate which is way more important in R.\nHowever it is generally a good idea, unless you know what you are doing, to avoid naming things that are functions in R because R will get confused.\n\nYou will get an error usually that says that Object of type ‘closure’ is not subsettable. This is just R saying wow I am like super confused. Like I have these two things named the same thing R will in the best case give up. It got super stressed and just quit. Other times it will just use the one that was set last. So just avoid doing this for now."
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#how-and-what-to-name-objects",
    "href": "slides/Getting-Started-in-R.html#how-and-what-to-name-objects",
    "title": "Getting Started in R",
    "section": "How and What to Name Objects",
    "text": "How and What to Name Objects\nThe best practice is to use concise descriptive names\nWhen loading in data typically I do raw_my_dataset_name and after data all of my cleaning I do clean_my_dataset_name\n\nObjects must start with a letter. But can contain letters, numbers, _, or .\n\nsnake_case_like_this_is_what_I_use\nsomePeopleUseCamelCase\nsome_People.are_Do_not.like_Convention\n\n\n\n\nExample and Discussion provided in R for Data Sciency by Hadley Wickham"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#the-data-we-are-working-with",
    "href": "slides/Getting-Started-in-R.html#the-data-we-are-working-with",
    "title": "Getting Started in R",
    "section": "The Data We are Working With",
    "text": "The Data We are Working With\n\nartwork by @allison_horst\nToday we are going to be using data from the Palmer Penguins R package. This is data that documents various measurement of penguin species from Palmer station. There are a few arguments we can use but I will show you first how to import the penguin data"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#importing-data",
    "href": "slides/Getting-Started-in-R.html#importing-data",
    "title": "Getting Started in R",
    "section": "Importing Data",
    "text": "Importing Data\n\nYou have the option of pointing and clicking via import dataset\nI would recommend importing data via code\n\nYou don’t have to remember what you named the object originally\nSaves future you time\n\nThis is a common error you will get\n\n\npenguins = read.csv(\"peguins.csv\")\n\nError in file(file, \"rt\"): cannot open the connection\n\npenguins = read.csv(\"penguins.csv\")\n\nError in file(file, \"rt\"): cannot open the connection\n\n\n\nThis happens most often when\n\nthe file name is spelled wrong\nthe file is in a subdirectory or your working directory is not set correctly\n\n\nFixing the error\n\npenguins = read.csv(\"data/penguins.csv\")\n\n\nRemember to work and reuse stuff in R we need to assign it to an object. So first we need to make sure our working directory is set to the correct one. In my case I am keeping the penguins in a data folder"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#your-turn",
    "href": "slides/Getting-Started-in-R.html#your-turn",
    "title": "Getting Started in R",
    "section": "Your Turn",
    "text": "Your Turn\n\nCreate an object with what ever you want in it\nread in the data included to the website using read.csv\n\nWhat happens when you do not assign the dataset?\n\nassign the penguins dataset to an object named penguins\nuse View, head, and tail to inspect the dataset\nusing install.packages() install ggplot2\n\n\n\n\n−+\n04:00"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#our-data",
    "href": "slides/Getting-Started-in-R.html#our-data",
    "title": "Getting Started in R",
    "section": "Our Data",
    "text": "Our Data\n\n\n\n\n \n  \n    species \n    island \n    bill_length_mm \n    bill_depth_mm \n    flipper_length_mm \n    body_mass_g \n    sex \n    year \n  \n \n\n  \n    Adelie \n    Torgersen \n    39.1 \n    18.7 \n    181 \n    3750 \n    male \n    2007 \n  \n  \n    Adelie \n    Torgersen \n    39.5 \n    17.4 \n    186 \n    3800 \n    female \n    2007 \n  \n  \n    Adelie \n    Torgersen \n    40.3 \n    18.0 \n    195 \n    3250 \n    female \n    2007 \n  \n  \n    Adelie \n    Torgersen \n    NA \n    NA \n    NA \n    NA \n    NA \n    2007 \n  \n  \n    Adelie \n    Torgersen \n    36.7 \n    19.3 \n    193 \n    3450 \n    female \n    2007 \n  \n  \n    Adelie \n    Torgersen \n    39.3 \n    20.6 \n    190 \n    3650 \n    male \n    2007 \n  \n\n\n\n\n\n\nUsing the magical penguin data we will leave you here with some sort of actionable stuff"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#indexing",
    "href": "slides/Getting-Started-in-R.html#indexing",
    "title": "Getting Started in R",
    "section": "Indexing []",
    "text": "Indexing []\n\nWe can use [] to index objects.\nThere are two slots we can use rows and columns in the brackets if we are using a dataframe like this.\nobject_name[rows, columns]\nWe can also subset our data by column position using : or c(column 1, column 2)\n\n\n\n\npenguins[1,1]\n\n\n\n\n\n \n  \n    x \n  \n \n\n  \n    Adelie \n  \n\n\n\n\n\n\n\npenguins[1,1:2]\n\npenguins[1,c(1,4)]\n\n\n\n\n\n \n  \n    species \n    island \n  \n \n\n  \n    Adelie \n    Torgersen \n  \n\n\n\n\n\n\n\n \n  \n    species \n    bill_depth_mm \n  \n \n\n  \n    Adelie \n    18.7 \n  \n\n\n\n\n\n\n\n\nknowing how to index stuff is important because often times we neeed to tell R what to get. Which is pretty critical especially if we want to use all the flexibility of R. You will need to be able to work with values of your dataset and you need to be able to navigate the software. The drop down menus for R kind of stop here so now you are in more coding territory.\nOne thing to keep in mind is that in R indexing starts at 1. So if you want to get the first element of a vector you use 1. Whereas in other languages indexing starts at zero. What does that mean substantivly. Well to get the first element of a vector in another language you would use zero.\nI use lists a lot to report coefficients from a regression or to automatically update my syllabus when things change."
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#indexing-cont",
    "href": "slides/Getting-Started-in-R.html#indexing-cont",
    "title": "Getting Started in R",
    "section": "Indexing [] (cont)",
    "text": "Indexing [] (cont)\n\nLists are super flexible because they can hold lots of different kinds of stuff\n\nWhen you estimate an OLS model it returns a list.\n\nWe can tell R what element of a list using a combo of [] and [[]]\n\n\nmy_list = list(a = 1:4, b = \"Hello World\", c = data.frame(x = 1:3, y = 4:6))\n\n\nmy_list[[1]][2] ## get the first item in the list and the second element of that item\n\n[1] 2\n\nmy_list[2]\n\n$b\n[1] \"Hello World\"\n\nmy_list[[3]][[1]]\n\n[1] 1 2 3"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#vs",
    "href": "slides/Getting-Started-in-R.html#vs",
    "title": "Getting Started in R",
    "section": "[] vs [[]]",
    "text": "[] vs [[]]\n\n\nthe difference still trips me up so I think of this picture. The single set of the brackets simply selects one train car while double brackets opens the train car"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#negative-indexing",
    "href": "slides/Getting-Started-in-R.html#negative-indexing",
    "title": "Getting Started in R",
    "section": "Negative Indexing",
    "text": "Negative Indexing\n\nWe can also exclude various elements using - and/or tests that I showed you earlier\n\n\npenguins[,-1]\n\n\n\n\n\n \n  \n    island \n    bill_length_mm \n    bill_depth_mm \n    flipper_length_mm \n    body_mass_g \n    sex \n    year \n  \n \n\n  \n    Torgersen \n    39.1 \n    18.7 \n    181 \n    3750 \n    male \n    2007 \n  \n  \n    Torgersen \n    39.5 \n    17.4 \n    186 \n    3800 \n    female \n    2007 \n  \n  \n    Torgersen \n    40.3 \n    18.0 \n    195 \n    3250 \n    female \n    2007 \n  \n  \n    Torgersen \n    NA \n    NA \n    NA \n    NA \n    NA \n    2007 \n  \n  \n    Torgersen \n    36.7 \n    19.3 \n    193 \n    3450 \n    female \n    2007 \n  \n  \n    Torgersen \n    39.3 \n    20.6 \n    190 \n    3650 \n    male \n    2007"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#negative-indexingcont",
    "href": "slides/Getting-Started-in-R.html#negative-indexingcont",
    "title": "Getting Started in R",
    "section": "Negative Indexing(cont)",
    "text": "Negative Indexing(cont)\n\nWe can use - or : as well to subset stuff\n\n\n\n\npenguins[,-(1:4)]\n\n\n\n\n\n \n  \n    flipper_length_mm \n    body_mass_g \n    sex \n    year \n  \n \n\n  \n    181 \n    3750 \n    male \n    2007 \n  \n  \n    186 \n    3800 \n    female \n    2007 \n  \n  \n    195 \n    3250 \n    female \n    2007 \n  \n  \n    NA \n    NA \n    NA \n    2007 \n  \n  \n    193 \n    3450 \n    female \n    2007 \n  \n  \n    190 \n    3650 \n    male \n    2007 \n  \n\n\n\n\n\n\n\npenguins[,-c(2,3,5,8)]\n\n\n\n\n\n \n  \n    species \n    bill_depth_mm \n    body_mass_g \n    sex \n  \n \n\n  \n    Adelie \n    18.7 \n    3750 \n    male \n  \n  \n    Adelie \n    17.4 \n    3800 \n    female \n  \n  \n    Adelie \n    18.0 \n    3250 \n    female \n  \n  \n    Adelie \n    NA \n    NA \n    NA \n  \n  \n    Adelie \n    19.3 \n    3450 \n    female \n  \n  \n    Adelie \n    20.6 \n    3650 \n    male"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#subsetting-by-tests",
    "href": "slides/Getting-Started-in-R.html#subsetting-by-tests",
    "title": "Getting Started in R",
    "section": "Subsetting By Tests",
    "text": "Subsetting By Tests\n\npenguins[penguins[\"sex\"] == \"female\", c(\"species\", \"sex\")]\n\n\n\n\n\n \n  \n      \n    species \n    sex \n  \n \n\n  \n    2 \n    Adelie \n    female \n  \n  \n    3 \n    Adelie \n    female \n  \n  \n    NA \n    NA \n    NA \n  \n  \n    5 \n    Adelie \n    female \n  \n  \n    7 \n    Adelie \n    female \n  \n  \n    NA.1 \n    NA \n    NA \n  \n  \n    NA.2 \n    NA \n    NA \n  \n  \n    NA.3 \n    NA \n    NA \n  \n  \n    NA.4 \n    NA \n    NA \n  \n  \n    13 \n    Adelie \n    female"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#indexing-1",
    "href": "slides/Getting-Started-in-R.html#indexing-1",
    "title": "Getting Started in R",
    "section": "$ Indexing",
    "text": "$ Indexing\nA really useful way of indexing in R is referencing stuff by name rather than position. - The way we do this is throught the $\n\nmy_list$a\n\n[1] 1 2 3 4\n\nmy_list$b\n\n[1] \"Hello World\"\n\nmy_list$c\n\n\n\n\n\nx\ny\n\n\n\n\n1\n4\n\n\n2\n5\n\n\n3\n6\n\n\n\n\n\n\n\nThe dollar sign is really helpful if you just want to pick out one thing. Brackets are flexible because you are bascially just returning a smaller list. Double brackets or the dollar sign return a list of length one so it is not as flexible"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#indexingcont",
    "href": "slides/Getting-Started-in-R.html#indexingcont",
    "title": "Getting Started in R",
    "section": "Indexing(cont)",
    "text": "Indexing(cont)\n\nmy_list[[3]][[2]] ## these are just returning the same thing \n\n[1] 4 5 6\n\nmy_list$c$y\n\n[1] 4 5 6\n\n\n\nYou can also use brackets to pick which element of the thing you want to figure out is. These are all important to know or be aware of all the ways you can find something you need. And like a lot of things in R there are tons of ways to get the same task done. The right way is what returns the right answer for you is the solution. Worry about speeding things up later. If you just remove one set of brackets from two it will return the element in a slightly different format"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#in-action",
    "href": "slides/Getting-Started-in-R.html#in-action",
    "title": "Getting Started in R",
    "section": "$ in action",
    "text": "$ in action\nThis will just subset things\n\npenguins[penguins$species == \"Gentoo\", c(\"species\", \"island\", \"bill_length_mm\")] \n\n\n\n\n\n \n  \n      \n    species \n    island \n    bill_length_mm \n  \n \n\n  \n    153 \n    Gentoo \n    Biscoe \n    46.1 \n  \n  \n    154 \n    Gentoo \n    Biscoe \n    50.0 \n  \n  \n    155 \n    Gentoo \n    Biscoe \n    48.7 \n  \n  \n    156 \n    Gentoo \n    Biscoe \n    50.0 \n  \n  \n    157 \n    Gentoo \n    Biscoe \n    47.6 \n  \n  \n    158 \n    Gentoo \n    Biscoe \n    46.5 \n  \n  \n    159 \n    Gentoo \n    Biscoe \n    45.4 \n  \n  \n    160 \n    Gentoo \n    Biscoe \n    46.7 \n  \n  \n    161 \n    Gentoo \n    Biscoe \n    43.3 \n  \n  \n    162 \n    Gentoo \n    Biscoe \n    46.8"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#in-actioncont",
    "href": "slides/Getting-Started-in-R.html#in-actioncont",
    "title": "Getting Started in R",
    "section": "$ in action(cont)",
    "text": "$ in action(cont)\n\nsummary(penguins)\n\n   species             island          bill_length_mm  bill_depth_mm  \n Length:344         Length:344         Min.   :32.10   Min.   :13.10  \n Class :character   Class :character   1st Qu.:39.23   1st Qu.:15.60  \n Mode  :character   Mode  :character   Median :44.45   Median :17.30  \n                                       Mean   :43.92   Mean   :17.15  \n                                       3rd Qu.:48.50   3rd Qu.:18.70  \n                                       Max.   :59.60   Max.   :21.50  \n                                       NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex                 year     \n Min.   :172.0     Min.   :2700   Length:344         Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   Class :character   1st Qu.:2007  \n Median :197.0     Median :4050   Mode  :character   Median :2008  \n Mean   :200.9     Mean   :4202                      Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                      3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                      Max.   :2009  \n NA's   :2         NA's   :2                                       \n\n\n\nmean(penguins$bill_depth_mm)\n\n[1] NA\n\n\nuh oh what happened?\n\nOkay so now that we have found different ways to index things lets get some summary statistics for our penguins dataframe. We can use column indexing but in our case it will make our lives easier if we use the dollar sign"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#finding-help",
    "href": "slides/Getting-Started-in-R.html#finding-help",
    "title": "Getting Started in R",
    "section": "Finding Help",
    "text": "Finding Help\n\nAsking for help in R is easy the most common ways are help(thingineedhelpwith) and ?thingineedhelpwith\n\n\n?mean\n\n\n?thingineedhelpwith is probably the most common because it requires less typing.\n\n\nNotice how R returns NA as the mean. R is generally pretty conservative and will be cautious when you try to get the mean for bill depth it returns NA because there are a few NA values in the column. We will talk about various solutions to getting rid of NA’s in data cleaning in a future workshop but for now we may just want to find the mean of our column. Mean and many functions have a way to do this inside them"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#fixing-our-issue-1",
    "href": "slides/Getting-Started-in-R.html#fixing-our-issue-1",
    "title": "Getting Started in R",
    "section": "Fixing our issue",
    "text": "Fixing our issue\n\n\nmean(penguins$bill_depth_mm, na.rm =TRUE)\n\n[1] 17.15117\n\n\n\n\nGood documentation fluctuates wildly because it is an open source language\nIf in doubt\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#your-turn-1",
    "href": "slides/Getting-Started-in-R.html#your-turn-1",
    "title": "Getting Started in R",
    "section": "Your Turn",
    "text": "Your Turn\n\nFind the minimum value of bill_length_mm\nFind the maximum value of body_mass_g\nSubset the penguins data any way you want using [] or $\nAssign each of them to an object\nCreate a vector from 1:10 index that vector using [] to return 2 and 4\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#some-additional-useful-stuff",
    "href": "slides/Getting-Started-in-R.html#some-additional-useful-stuff",
    "title": "Getting Started in R",
    "section": "Some additional useful stuff",
    "text": "Some additional useful stuff\n\nSometimes we want summary statistics per group\n\nWhat kind of penguins live where\nAre their any interesting patterns by group etc\n\nFortunately R comes with some handy functions to use\ntable counts each factor level\ntapply will let you group stuff by a factor and get some useful balance statistics"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#table",
    "href": "slides/Getting-Started-in-R.html#table",
    "title": "Getting Started in R",
    "section": "Table",
    "text": "Table\n\ntable(penguins$sex)\n\n\nfemale   male \n   165    168 \n\n\n\ntable(penguins$sex, useNA = \"ifany\")\n\n\nfemale   male   <NA> \n   165    168     11 \n\n\n\ntable is handy but in some cases it does not give us all the information we need in some cases we might have missing values and table will just ignore them. You just need to supply it with useNA much like how we supplied na.rm"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#tapply-and-calculating-descriptive-statistics-by-groups",
    "href": "slides/Getting-Started-in-R.html#tapply-and-calculating-descriptive-statistics-by-groups",
    "title": "Getting Started in R",
    "section": "tapply and calculating descriptive statistics by groups",
    "text": "tapply and calculating descriptive statistics by groups\n\ntapply(penguins$species,penguins$island, table)\n\n$Biscoe\n\nAdelie Gentoo \n    44    124 \n\n$Dream\n\n   Adelie Chinstrap \n       56        68 \n\n$Torgersen\n\nAdelie \n    52 \n\n\n\ntapply(penguins$bill_depth_mm, penguins$species, mean, na.rm = TRUE)\n\n   Adelie Chinstrap    Gentoo \n 18.34636  18.42059  14.98211 \n\n\n\nWe may want to see where our penguin friends live so we can use tapply which is a handy trick. There will be some convincing you need to do with to make drop the missing values in tapply. I fought with it on the plane so trust me."
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#plotting",
    "href": "slides/Getting-Started-in-R.html#plotting",
    "title": "Getting Started in R",
    "section": "Plotting",
    "text": "Plotting\n\n\n\nplot(penguins$bill_length_mm,\n     penguins$body_mass_g,\n     xlab = \"Bill Length(mm)\",\n     ylab = \"Body Mass(g)\")"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#plottingcont",
    "href": "slides/Getting-Started-in-R.html#plottingcont",
    "title": "Getting Started in R",
    "section": "Plotting(cont)",
    "text": "Plotting(cont)\n\n\n\nhist(penguins$bill_length_mm,\n xlim = c(30, 60))"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#making-new-things",
    "href": "slides/Getting-Started-in-R.html#making-new-things",
    "title": "Getting Started in R",
    "section": "Making New Things",
    "text": "Making New Things\n\nTo foreshadow our next workshop often we need to do things with our data\n\nLike deal with all those pesky missing values\nCreate new variables\nsubset our data(kind of like we have been doing)\nrecode our variables\n\nTo add new variables we can use what we know\n\n\npenguins$range_body_mass = max(penguins$body_mass_g, na.rm = TRUE) - min(penguins$body_mass_g, na.rm = TRUE)\n\npenguins$chinstrap[penguins$species == \"Adelie\" | penguins$species == \"Gentoo\"] <- \"Not Chinstrap\"\n\npenguins[is.na(penguins)] <- 0\n\npenguins$chinstrap[penguins$species == \"Chinstrap\"] <- \"Chinstrap\"\n\npenguins[,c(1,9:10)]\n\n\n\n\n\nspecies\nrange_body_mass\nchinstrap\n\n\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nAdelie\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nGentoo\n3600\nNot Chinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap\n\n\nChinstrap\n3600\nChinstrap"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#cleaning-up-after-yourslef",
    "href": "slides/Getting-Started-in-R.html#cleaning-up-after-yourslef",
    "title": "Getting Started in R",
    "section": "Cleaning up after yourslef",
    "text": "Cleaning up after yourslef\n\nrm(objectname) will remove the objects you created\nrm(list=ls()) will remove all the objects your created\nYou can remove packages, sometimes, with detach(package:packageyouwanttoremove)\n\nThis can be iffy for a variety of reasons\nSome packages automatically load another package or depend on another.\n\nHowever, restarting your R session is generally best practice because it will do both"
  },
  {
    "objectID": "slides/Getting-Started-in-R.html#getting-good-at-r",
    "href": "slides/Getting-Started-in-R.html#getting-good-at-r",
    "title": "Getting Started in R",
    "section": "Getting Good at R",
    "text": "Getting Good at R\n\n\n\nThe only way to write good code is to write tons of shitty code first. Feeling shame about bad code stops you from getting to good code\n\n— Hadley Wickham (@hadleywickham) April 17, 2015\n\n\n\n\nyou are going to run into lots of error messages and it is going to be really frustrating. Warning messages and errors are something we all run into. Trust me I have been close to needing a new computer more than few times. Part of getting good at R is just figuring out what the hell"
  },
  {
    "objectID": "slides/data-cleaning.html#our-team",
    "href": "slides/data-cleaning.html#our-team",
    "title": "Data Cleaning in R",
    "section": "Our Team",
    "text": "Our Team\n\n\nAlso, we do not have the capacity to provide pre-scheduled frequent software tutoring that is divorced from a specific assignment or software troubleshooting task at hand – in other words, we are also not generalized software tutors.\nOur one-on-one software assistance is available to help you troubleshoot specific tasks or assignments. If you are seeking generalized software help, we direct you to our live workshops and our recorded workshops to gain the foundational skills for using various analytical software. Then, when or if the time comes that you have targeted software questions or specific issues to tackle related to a course assignment or research project, please feel free to contact us for one-on-one support."
  },
  {
    "objectID": "slides/data-cleaning.html#get-ready-badges",
    "href": "slides/data-cleaning.html#get-ready-badges",
    "title": "Data Cleaning in R",
    "section": "Get Ready Badges",
    "text": "Get Ready Badges\n\n\nhttps://research.library.gsu.edu/dataservices/data-ready\nThese are awesome to share on social media i.e. linkedin which is a good signal to potential employers that you know this stuff"
  },
  {
    "objectID": "slides/data-cleaning.html#how-to-get-the-badges",
    "href": "slides/data-cleaning.html#how-to-get-the-badges",
    "title": "Data Cleaning in R",
    "section": "How To Get the Badges",
    "text": "How To Get the Badges"
  },
  {
    "objectID": "slides/data-cleaning.html#in-our-last-workshop-we-covered",
    "href": "slides/data-cleaning.html#in-our-last-workshop-we-covered",
    "title": "Data Cleaning in R",
    "section": "In our last workshop we covered",
    "text": "In our last workshop we covered\n\nAssignment\nIndexing\nGenerating Descriptive Statistics\nSome data cleaning(subsetting, generating new variables)\nA Bit of Graphing\n\nToday we will cover:\n\nThe Tidyverse (minus ggplot2)"
  },
  {
    "objectID": "slides/data-cleaning.html#packages-you-will-need",
    "href": "slides/data-cleaning.html#packages-you-will-need",
    "title": "Data Cleaning in R",
    "section": "Packages You Will Need",
    "text": "Packages You Will Need\n\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\nstarwars = read_csv(\"starwars.csv\")\npenguins = read_csv(\"penguins.csv\")"
  },
  {
    "objectID": "slides/data-cleaning.html#what-is-in-the-tidyverse",
    "href": "slides/data-cleaning.html#what-is-in-the-tidyverse",
    "title": "Data Cleaning in R",
    "section": "What is in the Tidyverse?",
    "text": "What is in the Tidyverse?\n\ntidyverse::tidyverse_packages()\n\n [1] \"broom\"         \"cli\"           \"crayon\"        \"dbplyr\"       \n [5] \"dplyr\"         \"dtplyr\"        \"forcats\"       \"ggplot2\"      \n [9] \"googledrive\"   \"googlesheets4\" \"haven\"         \"hms\"          \n[13] \"httr\"          \"jsonlite\"      \"lubridate\"     \"magrittr\"     \n[17] \"modelr\"        \"pillar\"        \"purrr\"         \"readr\"        \n[21] \"readxl\"        \"reprex\"        \"rlang\"         \"rstudioapi\"   \n[25] \"rvest\"         \"stringr\"       \"tibble\"        \"tidyr\"        \n[29] \"xml2\"          \"tidyverse\"    \n\n\n\nThese are the packages that are considered a part of the tidyerse. These offer a whole host of awesome capabilities. These all work off of the same philosophy so once you get the hang of one the rest will make more sense.\nSome of these are core tidyverse things others you have to load in seperetly like haven lubridate and broom."
  },
  {
    "objectID": "slides/data-cleaning.html#what-is-loaded",
    "href": "slides/data-cleaning.html#what-is-loaded",
    "title": "Data Cleaning in R",
    "section": "What is loaded?",
    "text": "What is loaded?\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\nThe tidyverse is pretty verbose because it loads a lot of things and in the case of lag and filter actually makes its lag and filter function the one that R uses.\ndplyr: is a grammar of data manipulation\ntidyr: this helps make tidy data\nforcats: helps us work with factors\nstringr: helps us work with strings\npurr: is basically just there to help you with iterating operations\ntibble: adds an additional class to R called a tibble that creates a special tibble thing\nreadr: reads in data however if you get data in some non-standard format than you need to use haven which will let you read in stata dta files, sas data files and SPSS .sav files."
  },
  {
    "objectID": "slides/data-cleaning.html#namespace-conflicts",
    "href": "slides/data-cleaning.html#namespace-conflicts",
    "title": "Data Cleaning in R",
    "section": "Namespace Conflicts",
    "text": "Namespace Conflicts\n\nSince R is open source you can name your functions just about anything\n\nthis results in lots of packages having similarly named functions or the same name\n\nDplyr is just warning us that if we use filter or lag it will use dplyr’s version of the function\nWhenever R runs into a namespace conflict it will default to the last package that was loaded\nThat is why it is generally best practice to load the most important package last\nYou can also use packagename::function you want to use to get around it.\n\nThis is called a namespace call\nexplicitly tells R which function you are using\n\n\n\nThe actual dependency count for the tidyverse it self is pretty large so this is something you might run into and is kind of controversial. This is something to be aware of because dependencies can really be annoying to deal with."
  },
  {
    "objectID": "slides/data-cleaning.html#section",
    "href": "slides/data-cleaning.html#section",
    "title": "Data Cleaning in R",
    "section": "",
    "text": "Extract rows with filter()\n\n\n\n\n\n\n\nExtract columns with select()\n\n\n\n\n\n\n\nArrange/sort rows with arrange()\n\n\n\n\n\n\n\nMake new columns with mutate()\n\n\n\n\n\n\n\nMake group summaries withgroup_by() %>% summarize()\n\n\n\n\n\n\n\n\nDplyr tries to be really human readable so we refer to each function as verbs. walk through table\nnote we will talk about the %>% for now do not worry about it\nfilter() allows you to keep rows based on the values of the columns1. The first argument is the data frame. The second and subsequent arguments are the conditions that must be true to keep the row.\nselect() allows you to rapidly zoom in on a useful subset using operations based on the names of the variables. select(). Our datasets today are pretty small\narrange() changes the order of the rows based on the value of the columns. It takes a data frame and a set of column names (or more complicated expressions) to order by\nThe job of mutate() is to add new columns that are calculated from the existing columns\ngroup_by() doesn’t change the data but, if you look closely at the output, you’ll notice that it’s now “grouped by” month. This means subsequent operations will now work “by month”"
  },
  {
    "objectID": "slides/data-cleaning.html#how-a-dplyr-verb-works",
    "href": "slides/data-cleaning.html#how-a-dplyr-verb-works",
    "title": "Data Cleaning in R",
    "section": "How A Dplyr Verb Works",
    "text": "How A Dplyr Verb Works\n\n\n\nverb(.data, ...)\n\n\n\nAll dplyr stuff works along these lines\nverb is one of the Dplyr verbs i.e select\n.data is the dataset you want to manipulate\n\nthis is true for the rest of the tidyverse\n\n... is just a set of things that the verb does."
  },
  {
    "objectID": "slides/data-cleaning.html#select",
    "href": "slides/data-cleaning.html#select",
    "title": "Data Cleaning in R",
    "section": "Select()",
    "text": "Select()\n\n\n\n select(.data = penguins, species)\n\n\n\n\n\n\n \n  \n    species \n  \n \n\n  \n    Adelie \n  \n  \n    Adelie \n  \n  \n    Adelie \n  \n  \n    Adelie \n  \n  \n    Adelie \n  \n  \n    Adelie \n  \n  \n    ... \n  \n\n\n\n\n\n\n\n\n\n\nYou can give select a range of columns with firstcolumn:thirdcolumn or omit stuff using -columnwedontwant\n\n\n\n\nI will only partially cover select because it is a pretty intuitive verb. Just give it the name of our columns. But see that the first argument we feed our verb is data and in this case the thing that verb does is just select the column name we want.\nThroughout the presentation you can copy and paste the code and run it yourself. You are probably going to get some different stuff than me. Thats because behind the scene you will see lots and lots of select calls just make sure everything fits on the screen! If you want to recreate the output your self all you would do is feed it the column names on the screen!"
  },
  {
    "objectID": "slides/data-cleaning.html#filter-1",
    "href": "slides/data-cleaning.html#filter-1",
    "title": "Data Cleaning in R",
    "section": "filter()",
    "text": "filter()\n\n\n\n  filter(.data = penguins,\n         species == \"Adelie\",\n         body_mass_g < 6200,\n         bill_length_mm < 59.6)\n\n\n\n\n\n\n \n  \n    species \n    body_mass_g \n    bill_length_mm \n  \n \n\n  \n    Adelie \n    3750 \n    39.1 \n  \n  \n    Adelie \n    3800 \n    39.5 \n  \n  \n    Adelie \n    3250 \n    40.3 \n  \n  \n    Adelie \n    3450 \n    36.7 \n  \n  \n    Adelie \n    3650 \n    39.3 \n  \n  \n    Adelie \n    3625 \n    38.9 \n  \n  \n    … \n    … \n    … \n  \n\n\n\n\n\n\n\n\nThe basics of filter are that you need to feed it a set of tests. Most often what you use filter for is to subset data based on a series of tests that make sense for your data. In this case I just took the quantiles for both variables. You can string together a set of tests in filter.\nNotice that we can pass different operations with a comma. We could also plausibly use our & to only return a dataset as long as we have data that meets those tests"
  },
  {
    "objectID": "slides/data-cleaning.html#what-kind-of-tests-can-i-do",
    "href": "slides/data-cleaning.html#what-kind-of-tests-can-i-do",
    "title": "Data Cleaning in R",
    "section": "What Kind of Tests Can I Do?",
    "text": "What Kind of Tests Can I Do?\n\n\n\n\n\n\n\n\n\nTest\nMeaning\nTest\nMeaning\n\n\n\n\nx < y\nLess than\nx %in% y\nIn (group membership)\n\n\nx > y\nGreater than\nis.na(x)\nIs missing\n\n\n==\nEqual to\n!is.na(x)\nIs not missing\n\n\nx <= y\nLess than or equal to\n\n\n\n\nx >= y\nGreater than or equal to\n\n\n\n\nx != y\nNot equal to"
  },
  {
    "objectID": "slides/data-cleaning.html#the-default",
    "href": "slides/data-cleaning.html#the-default",
    "title": "Data Cleaning in R",
    "section": "The Default",
    "text": "The Default\n\n\n\nfilter(starwars, homeworld == \"Naboo\",\n                 homeworld == \"Coruscant\")\n\n\n\n\n\n\n \n  \n    name \n    height \n    homeworld \n  \n \n\n  \n\n  \n\n\n\n\n\n\n\n\nFilter is going to default to an and test. That is basically what the columns are telling it"
  },
  {
    "objectID": "slides/data-cleaning.html#these-do-the-same-thing",
    "href": "slides/data-cleaning.html#these-do-the-same-thing",
    "title": "Data Cleaning in R",
    "section": "These Do The Same Thing",
    "text": "These Do The Same Thing\n\n\n\nfilter(starwars, homeworld == \"Naboo\",\n                 mass < 84.5)\n\n\nfilter(starwars, homeworld == \"Naboo\" &\n                 mass < 84.5)\n\n\n\n\n\n\n \n  \n    name \n    mass \n    homeworld \n  \n \n\n  \n    R2-D2 \n    32 \n    Naboo \n  \n  \n    Palpatine \n    75 \n    Naboo \n  \n  \n    Jar Jar Binks \n    66 \n    Naboo \n  \n  \n    Roos Tarpals \n    82 \n    Naboo \n  \n  \n    Padmé Amidala \n    45 \n    Naboo"
  },
  {
    "objectID": "slides/data-cleaning.html#getting-multiple-things-from-the-same-column",
    "href": "slides/data-cleaning.html#getting-multiple-things-from-the-same-column",
    "title": "Data Cleaning in R",
    "section": "Getting Multiple Things From the Same Column",
    "text": "Getting Multiple Things From the Same Column\n\nTo get a subset of homeworld we chain together multiple | tests together\n\n\n\n\nfilter(starwars, homeworld == \"Naboo\" |\n                 homeworld == \"Coruscant\" |\n                 homeworld == \"Tatooine\")\n\n\n\n\n\n\n \n  \n    name \n    homeworld \n  \n \n\n  \n    Luke Skywalker \n    Tatooine \n  \n  \n    C-3PO \n    Tatooine \n  \n  \n    R2-D2 \n    Naboo \n  \n  \n    Darth Vader \n    Tatooine \n  \n  \n    Owen Lars \n    Tatooine \n  \n  \n    Beru Whitesun lars \n    Tatooine \n  \n  \n    R5-D4 \n    Tatooine \n  \n  \n    Biggs Darklighter \n    Tatooine \n  \n  \n    Anakin Skywalker \n    Tatooine \n  \n  \n    Palpatine \n    Naboo \n  \n  \n    Finis Valorum \n    Coruscant \n  \n  \n    ... \n    ... \n  \n\n\n\n\n\n\n\n\nIn this case we are just using lots of or tests to achieve what we want this is because filter defaults to using &"
  },
  {
    "objectID": "slides/data-cleaning.html#getting-multiple-things-from-the-same-columncont",
    "href": "slides/data-cleaning.html#getting-multiple-things-from-the-same-columncont",
    "title": "Data Cleaning in R",
    "section": "Getting Multiple Things From the Same Column(cont)",
    "text": "Getting Multiple Things From the Same Column(cont)\n\n\n\n filter(starwars,\n  homeworld %in% c(\"Naboo\",\n                  \"Tatooine\",\n                  \"Coruscant\"))\n\n\n\n\n\n\n \n  \n    name \n    homeworld \n  \n \n\n  \n    Luke Skywalker \n    Tatooine \n  \n  \n    C-3PO \n    Tatooine \n  \n  \n    R2-D2 \n    Naboo \n  \n  \n    Darth Vader \n    Tatooine \n  \n  \n    Owen Lars \n    Tatooine \n  \n  \n    Beru Whitesun lars \n    Tatooine \n  \n  \n    R5-D4 \n    Tatooine \n  \n  \n    Biggs Darklighter \n    Tatooine \n  \n  \n    Anakin Skywalker \n    Tatooine \n  \n  \n    Palpatine \n    Naboo \n  \n  \n    Finis Valorum \n    Coruscant \n  \n  \n    ... \n    ... \n  \n\n\n\n\n\n\n\n\nUsing lists instead of big long conditional statements will honestly make your life so much easier. Where you use set membership in filter is in place of lots of or tests. So in this case we are feeding"
  },
  {
    "objectID": "slides/data-cleaning.html#filter-mistakes-we-all-make",
    "href": "slides/data-cleaning.html#filter-mistakes-we-all-make",
    "title": "Data Cleaning in R",
    "section": "filter() mistakes we all make",
    "text": "filter() mistakes we all make\n\n\n\nfilter(penguins, species = \"Gentoo\")\n\nError in `filter()`:\n! We detected a named input.\nℹ This usually means that you've used `=` instead of `==`.\nℹ Did you mean `species == \"Gentoo\"`?\n\n\n\n\nfilter(starwars, homeworld == \"Nabo\")\n\n\n\n\n\n \n  \n    name \n    height \n    mass \n    hair_color \n    skin_color \n    eye_color \n    birth_year \n    sex \n    gender \n    homeworld \n    species \n    films \n    vehicles \n    starships \n  \n \n\n  \n\n  \n\n\n\n\n\n\n\n\nWe are likely to encounter some sets of errors. Luckily Dplyr has finally built in a friendly error message with when your tests are misspecified. The equals one is one that we all make. Which will just now throw an error.\nWhat your are likely to encounter is that you have done a test and your object is now blank. It will return your something back to you but it is blank. All this means is that something is wrong in one of your tests. R will evaluate the test. If something meets the test than it will just return those results\nIn this case nothing in our data is named Nabo with one o so it is just returning the results of the test back to us."
  },
  {
    "objectID": "slides/data-cleaning.html#your-turn",
    "href": "slides/data-cleaning.html#your-turn",
    "title": "Data Cleaning in R",
    "section": "Your Turn",
    "text": "Your Turn\n\nTry removing the missing values from bill_length_mm hint: use ! and is.na.\nReturn a data dataset that only has data for the Dream island\nUsing either the starwars or penguins data use %in% to get a set of homeworlds or islands\nSubset the penguin data where body mass is less than 4202 and not on the Dream Island.\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/data-cleaning.html#mutate-1",
    "href": "slides/data-cleaning.html#mutate-1",
    "title": "Data Cleaning in R",
    "section": "Mutate",
    "text": "Mutate\n\n\n\n mutate(penguins,\n   log_bill_length = round(log(bill_length_mm)\n    , digits = 2))\n\n\n\n\n\n\n \n  \n    species \n    bill_length_mm \n    log_bill_length \n  \n \n\n  \n    Adelie \n    39.1 \n    3.67 \n  \n  \n    Adelie \n    39.5 \n    3.68 \n  \n  \n    Adelie \n    40.3 \n    3.7 \n  \n  \n    Adelie \n    36.7 \n    3.6 \n  \n  \n    Adelie \n    39.3 \n    3.67 \n  \n  \n    Adelie \n    38.9 \n    3.66 \n  \n  \n    ... \n    ... \n    ..."
  },
  {
    "objectID": "slides/data-cleaning.html#mutate-2",
    "href": "slides/data-cleaning.html#mutate-2",
    "title": "Data Cleaning in R",
    "section": "Mutate",
    "text": "Mutate\n\n\n\nmutate(penguins,\n      bill_length_mm = bill_length_mm / bill_depth_mm) \n\n\n\n\n\n\n \n  \n    bill_length_mm \n    bill_depth_mm \n  \n \n\n  \n    2.09090909090909 \n    18.7 \n  \n  \n    2.27011494252874 \n    17.4 \n  \n  \n    2.23888888888889 \n    18 \n  \n  \n    NA \n    NA \n  \n  \n    1.90155440414508 \n    19.3 \n  \n  \n    ... \n    ... \n  \n\n\n\n\n\n\n\n\nWe can also just take the existing columns and create a variable. So in this case we can just make a column that divides things or multiplies or square roots or whatever to your column to create something that we need!"
  },
  {
    "objectID": "slides/data-cleaning.html#logical-tests",
    "href": "slides/data-cleaning.html#logical-tests",
    "title": "Data Cleaning in R",
    "section": "Logical Tests",
    "text": "Logical Tests\n\nThere are various ways to do this in mutate but they all follow the same logic\n\n\n\n\nifelse(test,\n       Value_if_True,\n       Value_if_False)\n\n\n\nTest is a logical test species == \"Chinstrap\", species == \"Wookie\", etc\nValue_if_True what does it do if the test returns true\nValue_if_FALSE what does it do if the test returns false"
  },
  {
    "objectID": "slides/data-cleaning.html#mutate-3",
    "href": "slides/data-cleaning.html#mutate-3",
    "title": "Data Cleaning in R",
    "section": "Mutate",
    "text": "Mutate\n\n\n\nmutate(penguins,\n  big_penguin = ifelse(\n  body_mass_g >= 4750,  # median(body_mass_g) aslo works\n   TRUE,\n   FALSE))\n\n\n\n\n\n\n \n  \n    body_mass_g \n    big_penguin \n  \n \n\n  \n    3750 \n    FALSE \n  \n  \n    3800 \n    FALSE \n  \n  \n    3250 \n    FALSE \n  \n  \n    NA \n    NA \n  \n  \n    3450 \n    FALSE \n  \n  \n    3650 \n    FALSE \n  \n  \n    3625 \n    FALSE \n  \n  \n    4675 \n    FALSE \n  \n  \n    3475 \n    FALSE \n  \n  \n    4250 \n    FALSE \n  \n\n\n\n\n\n\n\n\nYou can put just about anything in this true false column generally thought true or false or a 1 or a zero are standard. All the stuff that we saw with the filter tests can work in there too! So if you need a variable requires multiple tests than you can do that as well.\nYou can also other functions in there as well"
  },
  {
    "objectID": "slides/data-cleaning.html#mutate-4",
    "href": "slides/data-cleaning.html#mutate-4",
    "title": "Data Cleaning in R",
    "section": "Mutate()",
    "text": "Mutate()\n\nMutate is order aware so you don’t have to use a new mutate for each new variable you want to create\n\n\n\n\n  mutate(penguins, long_bill = bill_length_mm * 2,\n         long_bill_logical =\n           ifelse(long_bill >= 100\n                  & long_bill <= 119.20,\n                  TRUE,\n                  FALSE)) \n\n\n\n\n\n\n \n  \n    bill_length_mm \n    long_bill \n    long_bill_logical \n    species \n  \n \n\n  \n    46.1 \n    92.2 \n    FALSE \n    NA \n  \n  \n    50 \n    100 \n    TRUE \n    NA \n  \n  \n    48.7 \n    97.4 \n    FALSE \n    NA \n  \n  \n    50 \n    100 \n    TRUE \n    NA \n  \n  \n    47.6 \n    95.2 \n    FALSE \n    NA \n  \n  \n    46.5 \n    93 \n    FALSE \n    NA \n  \n  \n    ... \n    ... \n    ... \n    ..."
  },
  {
    "objectID": "slides/data-cleaning.html#your-turn-1",
    "href": "slides/data-cleaning.html#your-turn-1",
    "title": "Data Cleaning in R",
    "section": "Your Turn",
    "text": "Your Turn\n\nWrite code to\nAdd a column in your dataset that is TRUE if a penguin is an Adelie penguin\nAdd a column in the starwars dataset that says Naboo or Tatooine, and Not Naboo or Tatooine if the character is not from there\nAdd a column in your dataset that squares the body mass (hint: use ^)\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/data-cleaning.html#data-cleaning",
    "href": "slides/data-cleaning.html#data-cleaning",
    "title": "Data Cleaning in R",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\n\nOften requires lots of intermediary steps\n\nreplacing missing values\nsubsetting data\ncreating new variables\n\nWe would ideally like to do this without assigning a new object each time\nIf things start going it may be difficult to spot where it went wrong\nEnter dplyr and the pipe"
  },
  {
    "objectID": "slides/data-cleaning.html#remember-how-a-dplyr-verb-works",
    "href": "slides/data-cleaning.html#remember-how-a-dplyr-verb-works",
    "title": "Data Cleaning in R",
    "section": "Remember How A Dplyr Verb Works",
    "text": "Remember How A Dplyr Verb Works\n\n\n\nverb(.data, ...)\n\n\n\nAll dplyr stuff works along these lines\nverb is one of the Dplyr verbs\n.data is the dataset you want to manipulate\n\nthis is true for the rest of the tidyverse\n\n... is just a set of things that the verb does."
  },
  {
    "objectID": "slides/data-cleaning.html#side-by-side",
    "href": "slides/data-cleaning.html#side-by-side",
    "title": "Data Cleaning in R",
    "section": "Side By Side",
    "text": "Side By Side\n\nFilterSelectMutate\n\n\n\nfilter(.data = penguins, species == \"Gentoo\")\n\n# A tibble: 124 × 8\n   species island bill_length_mm bill_depth_mm flipper_len…¹ body_…² sex    year\n   <fct>   <fct>           <dbl>         <dbl>         <int>   <int> <fct> <int>\n 1 Gentoo  Biscoe           46.1          13.2           211    4500 fema…  2007\n 2 Gentoo  Biscoe           50            16.3           230    5700 male   2007\n 3 Gentoo  Biscoe           48.7          14.1           210    4450 fema…  2007\n 4 Gentoo  Biscoe           50            15.2           218    5700 male   2007\n 5 Gentoo  Biscoe           47.6          14.5           215    5400 male   2007\n 6 Gentoo  Biscoe           46.5          13.5           210    4550 fema…  2007\n 7 Gentoo  Biscoe           45.4          14.6           211    4800 fema…  2007\n 8 Gentoo  Biscoe           46.7          15.3           219    5200 male   2007\n 9 Gentoo  Biscoe           43.3          13.4           209    4400 fema…  2007\n10 Gentoo  Biscoe           46.8          15.4           215    5150 male   2007\n# … with 114 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n\n\n\n\n\nselect(.data = penguins, species:bill_length_mm)\n\n# A tibble: 344 × 3\n   species island    bill_length_mm\n   <fct>   <fct>              <dbl>\n 1 Adelie  Torgersen           39.1\n 2 Adelie  Torgersen           39.5\n 3 Adelie  Torgersen           40.3\n 4 Adelie  Torgersen           NA  \n 5 Adelie  Torgersen           36.7\n 6 Adelie  Torgersen           39.3\n 7 Adelie  Torgersen           38.9\n 8 Adelie  Torgersen           39.2\n 9 Adelie  Torgersen           34.1\n10 Adelie  Torgersen           42  \n# … with 334 more rows\n\n\n\n\n\nmutate(.data = penguins, bill_length_mm_sq = bill_length_mm^2)\n\n# A tibble: 344 × 9\n   species island    bill_length_mm bill_d…¹ flipp…² body_…³ sex    year bill_…⁴\n   <fct>   <fct>              <dbl>    <dbl>   <int>   <int> <fct> <int>   <dbl>\n 1 Adelie  Torgersen           39.1     18.7     181    3750 male   2007   1529.\n 2 Adelie  Torgersen           39.5     17.4     186    3800 fema…  2007   1560.\n 3 Adelie  Torgersen           40.3     18       195    3250 fema…  2007   1624.\n 4 Adelie  Torgersen           NA       NA        NA      NA <NA>   2007     NA \n 5 Adelie  Torgersen           36.7     19.3     193    3450 fema…  2007   1347.\n 6 Adelie  Torgersen           39.3     20.6     190    3650 male   2007   1544.\n 7 Adelie  Torgersen           38.9     17.8     181    3625 fema…  2007   1513.\n 8 Adelie  Torgersen           39.2     19.6     195    4675 male   2007   1537.\n 9 Adelie  Torgersen           34.1     18.1     193    3475 <NA>   2007   1163.\n10 Adelie  Torgersen           42       20.2     190    4250 <NA>   2007   1764 \n# … with 334 more rows, and abbreviated variable names ¹​bill_depth_mm,\n#   ²​flipper_length_mm, ³​body_mass_g, ⁴​bill_length_mm_sq"
  },
  {
    "objectID": "slides/data-cleaning.html#piping",
    "href": "slides/data-cleaning.html#piping",
    "title": "Data Cleaning in R",
    "section": "Piping",
    "text": "Piping\n\nA pipe takes whats on the left hand side of the pipe and evaluates it as the first argument on the right hand side\nthis leverages the common syntax effectively\n\nthis is not limited to just dplyr stuff\n\nIf you look behind the curtain of the workshop slides you will see pipes everywhere!\n\n\nRather than starting with nested parenthesis we will start with piping. because it affords us a ton of advantages"
  },
  {
    "objectID": "slides/data-cleaning.html#magrittr-piping",
    "href": "slides/data-cleaning.html#magrittr-piping",
    "title": "Data Cleaning in R",
    "section": "Magrittr Piping",
    "text": "Magrittr Piping\n\nThe tidyverse has its own pipe %>% and used to be the only game in town.\n\n%>% is just % followed by > followed by %\n\nYou can see the advantages of the pipe when we need to do multiple things to a dataset\n\n\n\n\n## These do the same thing\n filter(mutate(penguins,\n  female = ifelse(sex == \"female\",\n    TRUE, FALSE)),\n     species == \"Adelie\")\n\n\n\n\n# these do the same thing\npenguins %>%  \nfilter(species == \"Adelie\") %>% \nmutate(female = ifelse(sex == \"female\", TRUE, FALSE))\n\n\n\n\n\n\nThis example an adaptation provided by Grant McDermot"
  },
  {
    "objectID": "slides/data-cleaning.html#magrittr-pipingcont",
    "href": "slides/data-cleaning.html#magrittr-pipingcont",
    "title": "Data Cleaning in R",
    "section": "Magrittr Piping(cont)",
    "text": "Magrittr Piping(cont)\n\nWhen you use the pipe it is easier to think of the pipe as saying and then\n\n\nI %>%\n    wake_up(time = \"8.00am\") %>%\n    get_out_of_bed(side = \"correct\") %>%\n    get_dressed(pants = \"TRUE\", shirt = \"TRUE\") %>% \n    leave_house(car = TRUE, bike = FALSE, MARTA = FALSE) %>%\n    am_late(traffic = TRUE)\n\n\n\nThe first thing in our pipe sequence is the object we are manipulating. In this case the object is me\n\n\nExample derived from Andrew Heiss"
  },
  {
    "objectID": "slides/data-cleaning.html#base-r-pipe",
    "href": "slides/data-cleaning.html#base-r-pipe",
    "title": "Data Cleaning in R",
    "section": "Base R Pipe",
    "text": "Base R Pipe\n\nThe pipe caught on and the team behind R added a native pipe |>\n\nthis is just | followed by >\n\nIf you are have a version of R that is 4.2.0>= it should come with the native pipe\nThe base versus magrittr pipe differ slightly and it is worth knowing some of the differences\nThe base R pipe is pretty flexible and supports some cool computer sciency stuff for more check out this page\n\nI tend to use the base R pipe\nThe differences between the two pipes are outlined in this post\n\n\n\nIf you are on a mac the keyboard shortcut is command shift m"
  },
  {
    "objectID": "slides/data-cleaning.html#group_by",
    "href": "slides/data-cleaning.html#group_by",
    "title": "Data Cleaning in R",
    "section": "group_by()",
    "text": "group_by()\n\n\ngroup_by() simply puts rows into groups based on values of a column\nNot necessarily the most useful function because nothing really happens when called by itself\n\n\n\n\npenguins |> \n  group_by(species) |>\n  head(5)\n\n# A tibble: 5 × 8\n# Groups:   species [1]\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  <fct>   <fct>              <dbl>         <dbl>       <int>   <int> <fct> <int>\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n4 Adelie  Torgersen           NA            NA            NA      NA <NA>   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\n\n\nUnless you combine it with summarize()\nNote: you will see summarise as well in people’s code because the creator and maintainer is from New Zealand"
  },
  {
    "objectID": "slides/data-cleaning.html#section-1",
    "href": "slides/data-cleaning.html#section-1",
    "title": "Data Cleaning in R",
    "section": "",
    "text": "penguins |> \n  group_by(species) |> \n  summarize(n())\n\n\n\n\n\n \n  \n    species \n    n() \n  \n \n\n  \n    Adelie \n    152 \n  \n  \n    Chinstrap \n    68 \n  \n  \n    Gentoo \n    124"
  },
  {
    "objectID": "slides/data-cleaning.html#section-2",
    "href": "slides/data-cleaning.html#section-2",
    "title": "Data Cleaning in R",
    "section": "",
    "text": "penguins |> \n  group_by(species) |> \n  summarise(n(),\n        mean_bill_length = mean(bill_length_mm,\n                             na.rm = TRUE))\n\n\n\n\n\n\n \n  \n    species \n    mean_bill_length \n    n() \n  \n \n\n  \n    Adelie \n    38.79 \n    152 \n  \n  \n    Chinstrap \n    48.83 \n    68 \n  \n  \n    Gentoo \n    47.5 \n    124 \n  \n\n\n\n\n\n\n\n\nAs with other dplyr stuff we can chain together multiple operations with a comma. So instead of doing multiple one of these we can just add them all in one summarise call. Thankfully we can also give our variables more informative names kind of like how we use mutate."
  },
  {
    "objectID": "slides/data-cleaning.html#your-turn-2",
    "href": "slides/data-cleaning.html#your-turn-2",
    "title": "Data Cleaning in R",
    "section": "Your Turn",
    "text": "Your Turn\n\nCalculate the minimum, maximum, and median body_mass_g for each species of penguin\nWhat happens if you remove group_by()?\nCalculate the number of distinct penguin species per island\n\nhint: type n_\n\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/data-cleaning.html#joins",
    "href": "slides/data-cleaning.html#joins",
    "title": "Data Cleaning in R",
    "section": "Joins",
    "text": "Joins\n\nOften times we need to get data from another dataset\nIn Dplyr we use join operations\n\nWhat each of the below joins are doing are elaborated more in R for Data Science\n\ninner_join(df1, df2)\nleft_join(df1, df2)\nright_join(df1, df2)\nfull_join(df1, df2)\nsemi_join(df1, df2)\nanti_join(df1, df2)"
  },
  {
    "objectID": "slides/data-cleaning.html#joins-1",
    "href": "slides/data-cleaning.html#joins-1",
    "title": "Data Cleaning in R",
    "section": "Joins",
    "text": "Joins\n\nThe basic syntax for each join is the same _join(df1, df2, by = \"var I want to join on)\nThe by argument can take a list of variables or you can just let dplyr guess(bad idea)\nEach join does something different and some are more cautious than others\nI tend to use left_join the most and is handy when you are trying to fill in gaps in panel data\n\n\n\n\ndata1 = data.frame(ID = 1:2,                      ## Create first example data frame\n                    X1 = c(\"a1\", \"a2\"),\n                    stringsAsFactors = FALSE)\n\n\n\n\n\n \n  \n    ID \n    X1 \n  \n \n\n  \n    1 \n    a1 \n  \n  \n    2 \n    a2 \n  \n\n\n\n\n\n\n\ndata2 = data.frame(ID = 2:3,                      ## Create second example data frame\n                    X2 = c(\"b1\", \"b2\"),\n                    stringsAsFactors = FALSE)\n\n\n\n\n\n \n  \n    ID \n    X2 \n  \n \n\n  \n    2 \n    b1 \n  \n  \n    3 \n    b2"
  },
  {
    "objectID": "slides/data-cleaning.html#left_join",
    "href": "slides/data-cleaning.html#left_join",
    "title": "Data Cleaning in R",
    "section": "left_join()",
    "text": "left_join()\n\nleft_join(data1, data2, by = \"ID\")\n\n\n\n\n\n \n  \n    ID \n    X1 \n    X2 \n  \n \n\n  \n    1 \n    a1 \n    NA \n  \n  \n    2 \n    a2 \n    b1"
  },
  {
    "objectID": "slides/data-cleaning.html#using-real-data",
    "href": "slides/data-cleaning.html#using-real-data",
    "title": "Data Cleaning in R",
    "section": "Using “Real” Data",
    "text": "Using “Real” Data\n\n\n\n\n\n\nnational_data\n\n\n\n\n\n \n  \n    state \n    year \n    unemployment \n    inflation \n    population \n  \n \n\n  \n    GA \n    2018 \n    5.0 \n    2.0 \n    100 \n  \n  \n    GA \n    2019 \n    5.3 \n    1.8 \n    200 \n  \n  \n    GA \n    2020 \n    5.2 \n    2.5 \n    300 \n  \n  \n    NC \n    2018 \n    6.1 \n    1.8 \n    350 \n  \n  \n    NC \n    2019 \n    5.9 \n    1.6 \n    375 \n  \n  \n    NC \n    2020 \n    5.3 \n    1.8 \n    400 \n  \n  \n    CO \n    2018 \n    4.7 \n    2.7 \n    200 \n  \n  \n    CO \n    2019 \n    4.4 \n    2.6 \n    300 \n  \n  \n    CO \n    2020 \n    5.1 \n    2.5 \n    400 \n  \n\n\n\n\n\n\n\nnational_libraries\n\n\n\n\n\n \n  \n    state \n    year \n    libraries \n    schools \n  \n \n\n  \n    CO \n    2018 \n    230 \n    470 \n  \n  \n    CO \n    2019 \n    240 \n    440 \n  \n  \n    CO \n    2020 \n    270 \n    510 \n  \n  \n    NC \n    2018 \n    200 \n    610 \n  \n  \n    NC \n    2019 \n    210 \n    590 \n  \n  \n    NC \n    2020 \n    220 \n    530"
  },
  {
    "objectID": "slides/data-cleaning.html#joinscont",
    "href": "slides/data-cleaning.html#joinscont",
    "title": "Data Cleaning in R",
    "section": "Joins(cont)",
    "text": "Joins(cont)\n\nnational_combined = left_join(national_data, national_libraries, \n                                    by = c(\"state\", \"year\")) \n\nnational_combined\n\n\n\n\n\n \n  \n    state \n    year \n    unemployment \n    inflation \n    population \n    libraries \n    schools \n  \n \n\n  \n    GA \n    2018 \n    5.0 \n    2.0 \n    100 \n    NA \n    NA \n  \n  \n    GA \n    2019 \n    5.3 \n    1.8 \n    200 \n    NA \n    NA \n  \n  \n    GA \n    2020 \n    5.2 \n    2.5 \n    300 \n    NA \n    NA \n  \n  \n    NC \n    2018 \n    6.1 \n    1.8 \n    350 \n    200 \n    610 \n  \n  \n    NC \n    2019 \n    5.9 \n    1.6 \n    375 \n    210 \n    590 \n  \n  \n    NC \n    2020 \n    5.3 \n    1.8 \n    400 \n    220 \n    530 \n  \n  \n    CO \n    2018 \n    4.7 \n    2.7 \n    200 \n    230 \n    470 \n  \n  \n    CO \n    2019 \n    4.4 \n    2.6 \n    300 \n    240 \n    440 \n  \n  \n    CO \n    2020 \n    5.1 \n    2.5 \n    400 \n    270 \n    510"
  },
  {
    "objectID": "slides/data-cleaning.html#combined-data",
    "href": "slides/data-cleaning.html#combined-data",
    "title": "Data Cleaning in R",
    "section": "Combined Data",
    "text": "Combined Data\n\nnational_combined = national_data |> \n  left_join(national_libraries, by = c(\"state\", \"year\"))\n\nnational_combined\n\n\n\n\n\n \n  \n    state \n    year \n    unemployment \n    inflation \n    population \n    libraries \n    schools \n  \n \n\n  \n    GA \n    2018 \n    5.0 \n    2.0 \n    100 \n    NA \n    NA \n  \n  \n    GA \n    2019 \n    5.3 \n    1.8 \n    200 \n    NA \n    NA \n  \n  \n    GA \n    2020 \n    5.2 \n    2.5 \n    300 \n    NA \n    NA \n  \n  \n    NC \n    2018 \n    6.1 \n    1.8 \n    350 \n    200 \n    610 \n  \n  \n    NC \n    2019 \n    5.9 \n    1.6 \n    375 \n    210 \n    590 \n  \n  \n    NC \n    2020 \n    5.3 \n    1.8 \n    400 \n    220 \n    530 \n  \n  \n    CO \n    2018 \n    4.7 \n    2.7 \n    200 \n    230 \n    470 \n  \n  \n    CO \n    2019 \n    4.4 \n    2.6 \n    300 \n    240 \n    440 \n  \n  \n    CO \n    2020 \n    5.1 \n    2.5 \n    400 \n    270 \n    510"
  },
  {
    "objectID": "slides/data-cleaning.html#what-if-our-columns-have-different-names",
    "href": "slides/data-cleaning.html#what-if-our-columns-have-different-names",
    "title": "Data Cleaning in R",
    "section": "What if our Columns Have Different Names?",
    "text": "What if our Columns Have Different Names?\n\n\n\n\n\n\n \n  \n    state \n    year \n    unemployment \n    inflation \n    population \n  \n \n\n  \n    GA \n    2018 \n    5.0 \n    2.0 \n    100 \n  \n  \n    GA \n    2019 \n    5.3 \n    1.8 \n    200 \n  \n  \n    GA \n    2020 \n    5.2 \n    2.5 \n    300 \n  \n  \n    NC \n    2018 \n    6.1 \n    1.8 \n    350 \n  \n  \n    NC \n    2019 \n    5.9 \n    1.6 \n    375 \n  \n  \n    NC \n    2020 \n    5.3 \n    1.8 \n    400 \n  \n  \n    CO \n    2018 \n    4.7 \n    2.7 \n    200 \n  \n  \n    CO \n    2019 \n    4.4 \n    2.6 \n    300 \n  \n  \n    CO \n    2020 \n    5.1 \n    2.5 \n    400 \n  \n\n\n\n\n\n\n\n\n\n\n \n  \n    statename \n    year \n    libraries \n    schools \n  \n \n\n  \n    CO \n    2018 \n    230 \n    470 \n  \n  \n    CO \n    2019 \n    240 \n    440 \n  \n  \n    CO \n    2020 \n    270 \n    510 \n  \n  \n    NC \n    2018 \n    200 \n    610 \n  \n  \n    NC \n    2019 \n    210 \n    590 \n  \n  \n    NC \n    2020 \n    220 \n    530"
  },
  {
    "objectID": "slides/data-cleaning.html#renaming-columns",
    "href": "slides/data-cleaning.html#renaming-columns",
    "title": "Data Cleaning in R",
    "section": "Renaming Columns",
    "text": "Renaming Columns\n\nRenaming stuff in dplyr is easy\nwe use the same syntax as dplyr::rename()\nrename(newvarname = oldvarname)\n\n\nnational_data |> \n  left_join(national_libraries, by = c(\"state\" = \"statename\", \"year\"))\n\n\n\n\n\n \n  \n    state \n    year \n    unemployment \n    inflation \n    population \n    libraries \n    schools \n  \n \n\n  \n    GA \n    2018 \n    5.0 \n    2.0 \n    100 \n    NA \n    NA \n  \n  \n    GA \n    2019 \n    5.3 \n    1.8 \n    200 \n    NA \n    NA \n  \n  \n    GA \n    2020 \n    5.2 \n    2.5 \n    300 \n    NA \n    NA \n  \n  \n    NC \n    2018 \n    6.1 \n    1.8 \n    350 \n    200 \n    610 \n  \n  \n    NC \n    2019 \n    5.9 \n    1.6 \n    375 \n    210 \n    590"
  },
  {
    "objectID": "slides/data-cleaning.html#reshaping-data",
    "href": "slides/data-cleaning.html#reshaping-data",
    "title": "Data Cleaning in R",
    "section": "Reshaping Data",
    "text": "Reshaping Data\n\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell."
  },
  {
    "objectID": "slides/data-cleaning.html#what-does-this-look-like-in-practice",
    "href": "slides/data-cleaning.html#what-does-this-look-like-in-practice",
    "title": "Data Cleaning in R",
    "section": "What does this look like in practice?",
    "text": "What does this look like in practice?\n\n\n\n\n \n  \n    religion \n    <$10k \n    $10-20k \n    $20-30k \n    $30-40k \n    $40-50k \n    $50-75k \n    $75-100k \n    $100-150k \n    >150k \n    Don't know/refused \n  \n \n\n  \n    Agnostic \n    27 \n    34 \n    60 \n    81 \n    76 \n    137 \n    122 \n    109 \n    84 \n    96 \n  \n  \n    Atheist \n    12 \n    27 \n    37 \n    52 \n    35 \n    70 \n    73 \n    59 \n    74 \n    76 \n  \n  \n    Buddhist \n    27 \n    21 \n    30 \n    34 \n    33 \n    58 \n    62 \n    39 \n    53 \n    54 \n  \n  \n    Catholic \n    418 \n    617 \n    732 \n    670 \n    638 \n    1116 \n    949 \n    792 \n    633 \n    1489 \n  \n  \n    Don’t know/refused \n    15 \n    14 \n    15 \n    11 \n    10 \n    35 \n    21 \n    17 \n    18 \n    116 \n  \n  \n    Evangelical Prot \n    575 \n    869 \n    1064 \n    982 \n    881 \n    1486 \n    949 \n    723 \n    414 \n    1529 \n  \n  \n    Hindu \n    1 \n    9 \n    7 \n    9 \n    11 \n    34 \n    47 \n    48 \n    54 \n    37 \n  \n  \n    Historically Black Prot \n    228 \n    244 \n    236 \n    238 \n    197 \n    223 \n    131 \n    81 \n    78 \n    339 \n  \n  \n    Jehovah's Witness \n    20 \n    27 \n    24 \n    24 \n    21 \n    30 \n    15 \n    11 \n    6 \n    37 \n  \n  \n    Jewish \n    19 \n    19 \n    25 \n    25 \n    30 \n    95 \n    69 \n    87 \n    151 \n    162 \n  \n  \n    Mainline Prot \n    289 \n    495 \n    619 \n    655 \n    651 \n    1107 \n    939 \n    753 \n    634 \n    1328 \n  \n  \n    Mormon \n    29 \n    40 \n    48 \n    51 \n    56 \n    112 \n    85 \n    49 \n    42 \n    69 \n  \n  \n    Muslim \n    6 \n    7 \n    9 \n    10 \n    9 \n    23 \n    16 \n    8 \n    6 \n    22 \n  \n  \n    Orthodox \n    13 \n    17 \n    23 \n    32 \n    32 \n    47 \n    38 \n    42 \n    46 \n    73 \n  \n  \n    Other Christian \n    9 \n    7 \n    11 \n    13 \n    13 \n    14 \n    18 \n    14 \n    12 \n    18 \n  \n  \n    Other Faiths \n    20 \n    33 \n    40 \n    46 \n    49 \n    63 \n    46 \n    40 \n    41 \n    71 \n  \n  \n    Other World Religions \n    5 \n    2 \n    3 \n    4 \n    2 \n    7 \n    3 \n    4 \n    4 \n    8 \n  \n  \n    Unaffiliated \n    217 \n    299 \n    374 \n    365 \n    341 \n    528 \n    407 \n    321 \n    258 \n    597 \n  \n\n\n\n\n\n\nLets take a look at some wide data here we are looking at billboard top hits. I have truncated it a bit to fit on the slide but weeks goes to week 47. Instead of having each week be an individual column we want it to be in a tidy format where week is one column and the values from all those columns are in the week column we create"
  },
  {
    "objectID": "slides/data-cleaning.html#making-data-longer",
    "href": "slides/data-cleaning.html#making-data-longer",
    "title": "Data Cleaning in R",
    "section": "Making Data Longer",
    "text": "Making Data Longer\n\nrelig_income |> \n  pivot_longer(!religion, names_to = \"income\", values_to = \"count\" )\n\n\n\n\n\n \n  \n    religion \n    income \n    count \n  \n \n\n  \n    Agnostic \n    <$10k \n    27 \n  \n  \n    Agnostic \n    $10-20k \n    34 \n  \n  \n    Agnostic \n    $20-30k \n    60 \n  \n  \n    Agnostic \n    $30-40k \n    81 \n  \n  \n    Agnostic \n    $40-50k \n    76 \n  \n  \n    Agnostic \n    $50-75k \n    137 \n  \n  \n    Agnostic \n    $75-100k \n    122 \n  \n  \n    Agnostic \n    $100-150k \n    109 \n  \n  \n    Agnostic \n    >150k \n    84 \n  \n  \n    Agnostic \n    Don't know/refused \n    96"
  },
  {
    "objectID": "slides/data-cleaning.html#separate",
    "href": "slides/data-cleaning.html#separate",
    "title": "Data Cleaning in R",
    "section": "Separate",
    "text": "Separate\n\nSometimes we need one variable to be two variables\nEnter Separate\n\n\nlibrary(lubridate) # for working with dates \nathlete_data =   tibble(forename = c(\"Lewis\", \"Tom\", \"Michael\", \"Joshua\"),\n                      surname = c(\"Hamilton\", \"Brady\", \"Jordan\", \"Allen\"),\n                      dob = ymd(c(\"1985-01-07\", \"1977-08-03\",\"1963-02-17\", \"1996-05-21\")))\nathlete_data |>\nseparate(dob, c(\"year\", \"month\", \"day\"))\n\n# A tibble: 4 × 5\n  forename surname  year  month day  \n  <chr>    <chr>    <chr> <chr> <chr>\n1 Lewis    Hamilton 1985  01    07   \n2 Tom      Brady    1977  08    03   \n3 Michael  Jordan   1963  02    17   \n4 Joshua   Allen    1996  05    21"
  },
  {
    "objectID": "slides/data-cleaning.html#unite",
    "href": "slides/data-cleaning.html#unite",
    "title": "Data Cleaning in R",
    "section": "Unite",
    "text": "Unite\n\nOther times we need to combine multiple columns to be one column\nenter unite\n\n\nathlete_data |>\nunite(name, c(\"forename\", \"surname\"), sep = \" \")\n\n# A tibble: 4 × 2\n  name           dob       \n  <chr>          <date>    \n1 Lewis Hamilton 1985-01-07\n2 Tom Brady      1977-08-03\n3 Michael Jordan 1963-02-17\n4 Joshua Allen   1996-05-21"
  },
  {
    "objectID": "slides/data-cleaning.html#helpful-helpers",
    "href": "slides/data-cleaning.html#helpful-helpers",
    "title": "Data Cleaning in R",
    "section": "Helpful Helpers",
    "text": "Helpful Helpers\n\nDplyr comes with really useful functions that help you when there are common patterns in your variable names\nthe syntax usually goes\nselect(contains(\"pattern\"))\nselect(starts_with(\"pattern\"))\nselect(ends_with(\"pattern\"))\n\n\n\n\n  select(starwars, name,\n   ends_with(\"color\"),  -eye_color)\n\n\n\n\n\n\n \n  \n    name \n    hair_color \n    skin_color \n  \n \n\n  \n    Luke Skywalker \n    blond \n    fair \n  \n  \n    C-3PO \n    NA \n    gold \n  \n  \n    R2-D2 \n    NA \n    white, blue \n  \n  \n    Darth Vader \n    none \n    white \n  \n  \n    ... \n    ... \n    ..."
  },
  {
    "objectID": "slides/data-cleaning.html#another-helper-example",
    "href": "slides/data-cleaning.html#another-helper-example",
    "title": "Data Cleaning in R",
    "section": "Another Helper Example",
    "text": "Another Helper Example\n\n\n\npenguins |>\nselect(starts_with(\"bill\"))\n\n\n\n\n\n\n \n  \n    bill_length_mm \n    bill_depth_mm \n  \n \n\n  \n    39.1 \n    18.7 \n  \n  \n    39.5 \n    17.4 \n  \n  \n    40.3 \n    18.0 \n  \n  \n    NA \n    NA \n  \n  \n    36.7 \n    19.3"
  },
  {
    "objectID": "slides/data-cleaning.html#filter-with-regular-expressions",
    "href": "slides/data-cleaning.html#filter-with-regular-expressions",
    "title": "Data Cleaning in R",
    "section": "filter() with regular expressions",
    "text": "filter() with regular expressions\n\nregular expressions also work and can be pretty handy.\n\n\n\n\n# base R regex\nfilter(starwars, grepl(\"Sky\", name)|  # base r version\n           grepl(\"Palp\", name) |\n           grepl(\"Obi\", name))\n# tidyverse regex\nfilter(starwars, str_detect(name, \"Sky\") | \n         str_detect(name, \"Palp\") |\n        str_detect(name, \"Obi\"))\n\n\n\n\n\n\n \n  \n    name \n    height \n    mass \n    homeworld \n  \n \n\n  \n    Luke Skywalker \n    172 \n    77 \n    Tatooine \n  \n  \n    Obi-Wan Kenobi \n    182 \n    77 \n    Stewjon \n  \n  \n    Anakin Skywalker \n    188 \n    84 \n    Tatooine \n  \n  \n    Palpatine \n    170 \n    75 \n    Naboo \n  \n  \n    Shmi Skywalker \n    163 \n    NA \n    Tatooine \n  \n  \n    ... \n    ... \n    ... \n    ... \n  \n\n\n\n\n\n\n\n\nI will say that regular expressions are notoriously difficult to work with. I choose the simplest one to use. Here we are feeding filter a really simple regular expression"
  },
  {
    "objectID": "slides/data-cleaning.html#mutate-6",
    "href": "slides/data-cleaning.html#mutate-6",
    "title": "Data Cleaning in R",
    "section": "Mutate",
    "text": "Mutate\n\n\nRegular ifelse\n\nmutate(penguins,\n  big_penguin = ifelse(\n  body_mass_g >= 4750,\n   \"Dats a big penguin\",\n   \"SMOL penguin\"))\n\n\n\n# A tibble: 6 × 2\n  body_mass_g big_penguin \n        <int> <chr>       \n1        3750 SMOL penguin\n2        3800 SMOL penguin\n3        3250 SMOL penguin\n4          NA <NA>        \n5        3450 SMOL penguin\n6        3650 SMOL penguin\n\n\n\nFancy ifelse\n\njedi = c(\"Luke Skywalker\",\n \"Yoda\", \"Obi-Wan Kenobi\",\n  \"Rey\", \n  \"Mace Windu\")\n\nsith = c(\"Palpatine\",\n \"Darth Maul\",\n  \"Dooku\",\n   \"Darth Vader\")\n\nhero_villains <- filter(starwars, name %in% jedi |\n name %in% sith)  \n \nmutate(hero_villains,\n  what_are_they = case_when(\n  name %in% jedi ~ \"Hero\",\n  name %in% sith ~ \"Evil Dooer\")) \n\n\n\n# A tibble: 5 × 2\n  name           what_are_they\n  <chr>          <chr>        \n1 Luke Skywalker Hero         \n2 Darth Vader    Evil Dooer   \n3 Obi-Wan Kenobi Hero         \n4 Yoda           Hero         \n5 Palpatine      Evil Dooer"
  },
  {
    "objectID": "slides/data-cleaning.html#generating-summary-statistics",
    "href": "slides/data-cleaning.html#generating-summary-statistics",
    "title": "Data Cleaning in R",
    "section": "Generating Summary statistics",
    "text": "Generating Summary statistics\n\npenguins |> \nselect(-year) |>\ngroup_by(species) |>\nsummarise(across(where(is.numeric),\n c(Mean = mean, Min = min, Max = max), na.rm = TRUE),\n  .names = \"{.cols}_{.fn}\")\n\n# A tibble: 3 × 14\n  species   bill_lengt…¹ bill_…² bill_…³ bill_…⁴ bill_…⁵ bill_…⁶ flipp…⁷ flipp…⁸\n  <fct>            <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <int>\n1 Adelie            38.8    32.1    46      18.3    15.5    21.5    190.     172\n2 Chinstrap         48.8    40.9    58      18.4    16.4    20.8    196.     178\n3 Gentoo            47.5    40.9    59.6    15.0    13.1    17.3    217.     203\n# … with 5 more variables: flipper_length_mm_Max <int>, body_mass_g_Mean <dbl>,\n#   body_mass_g_Min <int>, body_mass_g_Max <int>, .names <chr>, and abbreviated\n#   variable names ¹​bill_length_mm_Mean, ²​bill_length_mm_Min,\n#   ³​bill_length_mm_Max, ⁴​bill_depth_mm_Mean, ⁵​bill_depth_mm_Min,\n#   ⁶​bill_depth_mm_Max, ⁷​flipper_length_mm_Mean, ⁸​flipper_length_mm_Min"
  },
  {
    "objectID": "slides/data-cleaning.html#pivoting-with-lots-of-columns",
    "href": "slides/data-cleaning.html#pivoting-with-lots-of-columns",
    "title": "Data Cleaning in R",
    "section": "Pivoting With LOTS of columns",
    "text": "Pivoting With LOTS of columns\n\ndata(\"billboard\")\n\n\n\n\n\n \n  \n    artist \n    track \n    date.entered \n    wk1 \n    wk2 \n    wk3 \n    wk4 \n    wk5 \n  \n \n\n  \n    2 Pac \n    Baby Don't Cry (Keep... \n    2000-02-26 \n    87 \n    82 \n    72 \n    77 \n    87 \n  \n  \n    2Ge+her \n    The Hardest Part Of ... \n    2000-09-02 \n    91 \n    87 \n    92 \n    NA \n    NA \n  \n  \n    3 Doors Down \n    Kryptonite \n    2000-04-08 \n    81 \n    70 \n    68 \n    67 \n    66 \n  \n  \n    3 Doors Down \n    Loser \n    2000-10-21 \n    76 \n    76 \n    72 \n    69 \n    67 \n  \n  \n    504 Boyz \n    Wobble Wobble \n    2000-04-15 \n    57 \n    34 \n    25 \n    17 \n    17 \n  \n  \n    98^0 \n    Give Me Just One Nig... \n    2000-08-19 \n    51 \n    39 \n    34 \n    26 \n    26 \n  \n  \n    A*Teens \n    Dancing Queen \n    2000-07-08 \n    97 \n    97 \n    96 \n    95 \n    100 \n  \n  \n    Aaliyah \n    I Don't Wanna \n    2000-01-29 \n    84 \n    62 \n    51 \n    41 \n    38 \n  \n  \n    Aaliyah \n    Try Again \n    2000-03-18 \n    59 \n    53 \n    38 \n    28 \n    21 \n  \n  \n    Adams, Yolanda \n    Open My Heart \n    2000-08-26 \n    76 \n    76 \n    74 \n    69 \n    68 \n  \n  \n    Adkins, Trace \n    More \n    2000-04-29 \n    84 \n    84 \n    75 \n    73 \n    73 \n  \n  \n    Aguilera, Christina \n    Come On Over Baby (A... \n    2000-08-05 \n    57 \n    47 \n    45 \n    29 \n    23 \n  \n  \n    Aguilera, Christina \n    I Turn To You \n    2000-04-15 \n    50 \n    39 \n    30 \n    28 \n    21 \n  \n  \n    Aguilera, Christina \n    What A Girl Wants \n    1999-11-27 \n    71 \n    51 \n    28 \n    18 \n    13 \n  \n  \n    Alice Deejay \n    Better Off Alone \n    2000-04-08 \n    79 \n    65 \n    53 \n    48 \n    45 \n  \n  \n    Allan, Gary \n    Smoke Rings In The D... \n    2000-01-22 \n    80 \n    78 \n    76 \n    77 \n    92 \n  \n  \n    Amber \n    Sexual \n    1999-07-17 \n    99 \n    99 \n    96 \n    96 \n    100 \n  \n  \n    Anastacia \n    I'm Outta Love \n    2000-04-01 \n    92 \n    NA \n    NA \n    95 \n    NA \n  \n  \n    Anthony, Marc \n    My Baby You \n    2000-09-16 \n    82 \n    76 \n    76 \n    70 \n    82 \n  \n  \n    Anthony, Marc \n    You Sang To Me \n    2000-02-26 \n    77 \n    54 \n    50 \n    43 \n    30 \n  \n  \n    Avant \n    My First Love \n    2000-11-04 \n    70 \n    62 \n    56 \n    43 \n    39 \n  \n  \n    Avant \n    Separated \n    2000-04-29 \n    62 \n    32 \n    30 \n    23 \n    26 \n  \n  \n    BBMak \n    Back Here \n    2000-04-29 \n    99 \n    86 \n    60 \n    52 \n    38 \n  \n  \n    Backstreet Boys, The \n    Shape Of My Heart \n    2000-10-14 \n    39 \n    25 \n    24 \n    15 \n    12 \n  \n  \n    Backstreet Boys, The \n    Show Me The Meaning ... \n    2000-01-01 \n    74 \n    62 \n    55 \n    25 \n    16 \n  \n  \n    Backstreet Boys, The \n    The One \n    2000-05-27 \n    58 \n    50 \n    43 \n    37 \n    31 \n  \n  \n    Badu, Erkyah \n    Bag Lady \n    2000-08-19 \n    67 \n    53 \n    42 \n    41 \n    48 \n  \n  \n    Baha Men \n    Who Let The Dogs Out \n    2000-07-22 \n    99 \n    92 \n    85 \n    76 \n    65 \n  \n  \n    Barenaked Ladies \n    Pinch Me \n    2000-09-09 \n    77 \n    76 \n    69 \n    45 \n    51 \n  \n  \n    Beenie Man \n    Girls Dem Sugar \n    2000-10-21 \n    72 \n    72 \n    63 \n    56 \n    62 \n  \n  \n    Before Dark \n    Monica \n    2000-05-20 \n    95 \n    87 \n    80 \n    80 \n    77 \n  \n  \n    Bega, Lou \n    Tricky Tricky \n    2000-01-29 \n    75 \n    74 \n    87 \n    NA \n    NA \n  \n  \n    Big Punisher \n    It's So Hard \n    2000-04-22 \n    96 \n    87 \n    75 \n    79 \n    81 \n  \n  \n    Black Rob \n    Whoa! \n    2000-03-04 \n    78 \n    59 \n    53 \n    52 \n    47 \n  \n  \n    Black, Clint \n    Been There \n    2000-02-19 \n    87 \n    73 \n    62 \n    58 \n    58 \n  \n  \n    Blaque \n    Bring It All To Me \n    1999-10-23 \n    73 \n    63 \n    50 \n    42 \n    24 \n  \n  \n    Blige, Mary J. \n    Deep Inside \n    1999-11-13 \n    83 \n    80 \n    80 \n    75 \n    75 \n  \n  \n    Blige, Mary J. \n    Give Me You \n    2000-04-15 \n    97 \n    94 \n    77 \n    76 \n    68 \n  \n  \n    Blink-182 \n    All The Small Things \n    1999-12-04 \n    89 \n    76 \n    69 \n    59 \n    59 \n  \n  \n    Bloodhound Gang \n    The Bad Touch \n    2000-03-18 \n    70 \n    62 \n    55 \n    55 \n    52 \n  \n  \n    Bon Jovi \n    It's My Life \n    2000-08-12 \n    64 \n    58 \n    51 \n    51 \n    48 \n  \n  \n    Braxton, Toni \n    He Wasn't Man Enough \n    2000-03-18 \n    63 \n    55 \n    48 \n    39 \n    35 \n  \n  \n    Braxton, Toni \n    Just Be A Man About ... \n    2000-07-29 \n    76 \n    69 \n    51 \n    42 \n    37 \n  \n  \n    Braxton, Toni \n    Spanish Guitar \n    2000-12-02 \n    98 \n    98 \n    98 \n    NA \n    NA \n  \n  \n    Brock, Chad \n    A Country Boy Can Su... \n    2000-01-01 \n    93 \n    75 \n    92 \n    NA \n    NA \n  \n  \n    Brock, Chad \n    Yes! \n    2000-04-08 \n    90 \n    77 \n    66 \n    61 \n    59 \n  \n  \n    Brooks & Dunn \n    You'll Always Be Lov... \n    2000-06-10 \n    95 \n    85 \n    85 \n    85 \n    83 \n  \n  \n    Brooks, Garth \n    Do What You Gotta Do \n    2000-02-19 \n    86 \n    81 \n    72 \n    70 \n    69 \n  \n  \n    Byrd, Tracy \n    Put Your Hand In Min... \n    2000-01-29 \n    81 \n    77 \n    76 \n    76 \n    86 \n  \n  \n    Cagle, Chris \n    My Love Goes On And ... \n    2000-10-21 \n    99 \n    94 \n    94 \n    87 \n    84 \n  \n  \n    Cam'ron \n    What Means The World... \n    2000-10-14 \n    94 \n    94 \n    96 \n    91 \n    84 \n  \n  \n    Carey, Mariah \n    Crybaby \n    2000-06-24 \n    28 \n    34 \n    48 \n    62 \n    77 \n  \n  \n    Carey, Mariah \n    Thank God I Found Yo... \n    1999-12-11 \n    82 \n    68 \n    50 \n    50 \n    41 \n  \n  \n    Carter, Aaron \n    Aaron's Party (Come ... \n    2000-08-26 \n    99 \n    75 \n    57 \n    35 \n    35 \n  \n  \n    Carter, Torrey \n    Take That \n    2000-06-24 \n    94 \n    88 \n    86 \n    91 \n    89 \n  \n  \n    Changing Faces \n    That Other Woman \n    2000-09-30 \n    80 \n    72 \n    66 \n    66 \n    64 \n  \n  \n    Chesney, Kenny \n    I Lost It \n    2000-10-21 \n    75 \n    67 \n    61 \n    58 \n    58 \n  \n  \n    Chesney, Kenny \n    What I Need To Do \n    2000-04-01 \n    79 \n    74 \n    68 \n    72 \n    69 \n  \n  \n    Clark Family Experience \n    Meanwhile Back At Th... \n    2000-11-18 \n    87 \n    86 \n    81 \n    92 \n    80 \n  \n  \n    Clark, Terri \n    A Little Gasoline \n    2000-12-16 \n    75 \n    82 \n    88 \n    96 \n    99 \n  \n  \n    Common \n    The Light \n    2000-08-05 \n    75 \n    55 \n    53 \n    49 \n    46 \n  \n  \n    Counting Crows \n    Hanginaround \n    1999-11-06 \n    84 \n    70 \n    66 \n    60 \n    46 \n  \n  \n    Creed \n    Higher \n    1999-09-11 \n    81 \n    77 \n    73 \n    63 \n    61 \n  \n  \n    Creed \n    With Arms Wide Open \n    2000-05-13 \n    84 \n    78 \n    76 \n    74 \n    70 \n  \n  \n    Cyrus, Billy Ray \n    You Won't Be Lonely ... \n    2000-09-23 \n    97 \n    97 \n    97 \n    92 \n    91 \n  \n  \n    D'Angelo \n    Left & Right \n    1999-12-11 \n    93 \n    77 \n    75 \n    70 \n    91 \n  \n  \n    D'Angelo \n    Untitled (How Does I... \n    2000-01-22 \n    77 \n    56 \n    35 \n    26 \n    25 \n  \n  \n    DMX \n    Party Up (Up In Here... \n    2000-02-26 \n    88 \n    74 \n    62 \n    56 \n    49 \n  \n  \n    DMX \n    What You Want \n    2000-07-01 \n    98 \n    95 \n    95 \n    87 \n    86 \n  \n  \n    DMX \n    What's My Name \n    2000-01-15 \n    98 \n    76 \n    69 \n    69 \n    67 \n  \n  \n    Da Brat \n    That's What I'm Look... \n    2000-02-26 \n    93 \n    73 \n    60 \n    60 \n    60 \n  \n  \n    Da Brat \n    What'Chu Like \n    2000-06-03 \n    71 \n    65 \n    54 \n    54 \n    51 \n  \n  \n    Davidson, Clay \n    Unconditional \n    2000-03-25 \n    97 \n    97 \n    97 \n    90 \n    83 \n  \n  \n    De La Soul \n    All Good? \n    2000-12-23 \n    96 \n    96 \n    100 \n    NA \n    NA \n  \n  \n    Destiny's Child \n    Independent Women Pa... \n    2000-09-23 \n    78 \n    63 \n    49 \n    33 \n    23 \n  \n  \n    Destiny's Child \n    Jumpin' Jumpin' \n    2000-05-13 \n    74 \n    71 \n    65 \n    62 \n    57 \n  \n  \n    Destiny's Child \n    Say My Name \n    1999-12-25 \n    83 \n    83 \n    44 \n    38 \n    16 \n  \n  \n    Diffie, Joe \n    It's Always Somethin... \n    2000-08-12 \n    81 \n    78 \n    67 \n    63 \n    60 \n  \n  \n    Diffie, Joe \n    The Quittin' Kind \n    2000-01-01 \n    98 \n    100 \n    100 \n    90 \n    93 \n  \n  \n    Dion, Celine \n    That's The Way It Is \n    1999-11-13 \n    74 \n    68 \n    65 \n    49 \n    44 \n  \n  \n    Dixie Chicks, The \n    Cold Day In July \n    2000-06-24 \n    80 \n    79 \n    76 \n    72 \n    68 \n  \n  \n    Dixie Chicks, The \n    Cowboy Take Me Away \n    1999-11-27 \n    79 \n    72 \n    70 \n    61 \n    52 \n  \n  \n    Dixie Chicks, The \n    Goodbye Earl \n    2000-03-18 \n    40 \n    29 \n    24 \n    24 \n    20 \n  \n  \n    Dixie Chicks, The \n    Without You \n    2000-10-07 \n    80 \n    70 \n    63 \n    56 \n    50 \n  \n  \n    Dr. Dre \n    Forgot About Dre \n    2000-01-22 \n    75 \n    55 \n    47 \n    36 \n    36 \n  \n  \n    Dr. Dre \n    The Next Episode \n    2000-05-27 \n    78 \n    67 \n    58 \n    53 \n    46 \n  \n  \n    Drama \n    Left, Right, Left \n    2000-02-12 \n    100 \n    98 \n    89 \n    80 \n    75 \n  \n  \n    Dream \n    He Loves U Not \n    2000-09-30 \n    99 \n    92 \n    81 \n    59 \n    47 \n  \n  \n    Eastsidaz, The \n    G'D Up \n    2000-01-08 \n    77 \n    77 \n    89 \n    64 \n    57 \n  \n  \n    Eastsidaz, The \n    Got Beef \n    2000-07-01 \n    99 \n    99 \n    NA \n    NA \n    NA \n  \n  \n    Eiffel 65 \n    Blue \n    1999-12-11 \n    67 \n    29 \n    16 \n    16 \n    13 \n  \n  \n    Elliott, Missy \"Misdemeanor\" \n    Hot Boyz \n    1999-11-27 \n    36 \n    21 \n    13 \n    9 \n    7 \n  \n  \n    Eminem \n    Stan \n    2000-11-04 \n    78 \n    67 \n    57 \n    57 \n    51 \n  \n  \n    Eminem \n    The Real Slim Shady \n    2000-05-06 \n    70 \n    32 \n    20 \n    16 \n    11 \n  \n  \n    Eminem \n    The Way I Am \n    2000-08-26 \n    87 \n    74 \n    59 \n    65 \n    59 \n  \n  \n    En Vogue \n    Riddle \n    2000-06-17 \n    92 \n    92 \n    97 \n    100 \n    NA \n  \n  \n    Estefan, Gloria \n    No Me Dejes De Quere... \n    2000-06-10 \n    77 \n    NA \n    NA \n    NA \n    NA \n  \n  \n    Evans, Sara \n    Born To Fly \n    2000-10-21 \n    77 \n    71 \n    64 \n    57 \n    55 \n  \n  \n    Eve \n    Got It All \n    2000-07-15 \n    89 \n    88 \n    88 \n    91 \n    95 \n  \n  \n    Eve \n    Love Is Blind \n    2000-01-08 \n    94 \n    91 \n    57 \n    46 \n    46 \n  \n  \n    Everclear \n    Wonderful \n    2000-07-08 \n    77 \n    69 \n    53 \n    37 \n    33 \n  \n  \n    Fabian, Lara \n    I Will Love Again \n    2000-06-10 \n    91 \n    80 \n    75 \n    61 \n    60 \n  \n  \n    Fatboy Slim \n    The Rockafeller Skan... \n    1999-11-13 \n    94 \n    94 \n    94 \n    87 \n    77 \n  \n  \n    Filter \n    Take A Picture \n    1999-11-27 \n    91 \n    74 \n    64 \n    52 \n    38 \n  \n  \n    Foo Fighters \n    Learn To Fly \n    1999-10-16 \n    80 \n    69 \n    68 \n    63 \n    60 \n  \n  \n    Fragma \n    Toca's Miracle \n    2000-10-28 \n    99 \n    NA \n    NA \n    NA \n    NA \n  \n  \n    Funkmaster Flex \n    Do You \n    2000-11-11 \n    92 \n    92 \n    95 \n    91 \n    91 \n  \n  \n    Ghostface Killah \n    Cherchez LaGhost \n    2000-08-05 \n    98 \n    NA \n    NA \n    NA \n    NA \n  \n  \n    Gill, Vince \n    Feels Like Love \n    2000-09-02 \n    82 \n    76 \n    74 \n    73 \n    73 \n  \n  \n    Gilman, Billy \n    One Voice \n    2000-06-17 \n    86 \n    86 \n    82 \n    72 \n    65 \n  \n  \n    Ginuwine \n    None Of Ur Friends B... \n    1999-12-11 \n    94 \n    84 \n    71 \n    71 \n    60 \n  \n  \n    Ginuwine \n    The Best Man I Can B... \n    2000-01-08 \n    97 \n    97 \n    83 \n    94 \n    84 \n  \n  \n    Goo Goo Dolls \n    Broadway \n    2000-04-22 \n    74 \n    58 \n    53 \n    42 \n    35 \n  \n  \n    Gray, Macy \n    I Try \n    2000-02-19 \n    68 \n    51 \n    47 \n    36 \n    30 \n  \n  \n    Griggs, Andy \n    She's More \n    2000-03-11 \n    81 \n    76 \n    69 \n    67 \n    62 \n  \n  \n    Guy \n    Dancin' \n    1999-12-18 \n    46 \n    29 \n    19 \n    22 \n    36 \n  \n  \n    Hanson \n    This Time Around \n    2000-04-22 \n    22 \n    22 \n    20 \n    45 \n    87 \n  \n  \n    Hart, Beth \n    L.A. Song \n    1999-11-27 \n    99 \n    100 \n    98 \n    99 \n    99 \n  \n  \n    Heatherly, Eric \n    Flowers On The Wall \n    2000-04-29 \n    95 \n    88 \n    88 \n    82 \n    82 \n  \n  \n    Henley, Don \n    Taking You Home \n    2000-06-24 \n    79 \n    77 \n    74 \n    73 \n    66 \n  \n  \n    Herndon, Ty \n    No Mercy \n    2000-03-18 \n    100 \n    99 \n    99 \n    NA \n    NA \n  \n  \n    Hill, Faith \n    Breathe \n    1999-11-06 \n    81 \n    68 \n    62 \n    51 \n    42 \n  \n  \n    Hill, Faith \n    Let's Make Love \n    2000-08-12 \n    83 \n    83 \n    73 \n    73 \n    67 \n  \n  \n    Hoku \n    Another Dumb Blonde \n    2000-02-19 \n    69 \n    49 \n    49 \n    34 \n    34 \n  \n  \n    Hollister, Dave \n    Can't Stay \n    2000-03-25 \n    84 \n    84 \n    93 \n    98 \n    NA \n  \n  \n    Hot Boys \n    I Need A Hot Girl \n    2000-02-19 \n    77 \n    75 \n    71 \n    65 \n    65 \n  \n  \n    Houston, Whitney \n    Could I Have This Ki... \n    2000-06-17 \n    74 \n    68 \n    68 \n    67 \n    59 \n  \n  \n    Houston, Whitney \n    I Learned From The B... \n    2000-02-19 \n    83 \n    83 \n    83 \n    40 \n    28 \n  \n  \n    Houston, Whitney \n    My Love Is Your Love \n    1999-09-04 \n    81 \n    68 \n    44 \n    16 \n    11 \n  \n  \n    Houston, Whitney \n    Same Script, Differe... \n    2000-06-17 \n    71 \n    71 \n    71 \n    71 \n    70 \n  \n  \n    IMx \n    Stay The Night \n    1999-10-09 \n    84 \n    61 \n    45 \n    43 \n    40 \n  \n  \n    Ice Cube \n    You Can Do It \n    1999-12-04 \n    86 \n    66 \n    50 \n    42 \n    42 \n  \n  \n    Ideal \n    Whatever \n    2000-06-10 \n    75 \n    75 \n    67 \n    73 \n    64 \n  \n  \n    Iglesias, Enrique \n    Be With You \n    2000-04-01 \n    63 \n    45 \n    34 \n    23 \n    17 \n  \n  \n    Iglesias, Enrique \n    Rhythm Divine \n    1999-12-04 \n    90 \n    84 \n    79 \n    67 \n    67 \n  \n  \n    J-Shin \n    One Night Stand \n    1999-12-25 \n    96 \n    96 \n    69 \n    74 \n    72 \n  \n  \n    Ja Rule \n    Between Me And You \n    2000-09-16 \n    85 \n    74 \n    61 \n    37 \n    27 \n  \n  \n    Jackson, Alan \n    It Must Be Love \n    2000-06-24 \n    76 \n    74 \n    68 \n    63 \n    57 \n  \n  \n    Jackson, Alan \n    Pop A Top \n    1999-11-13 \n    79 \n    73 \n    70 \n    64 \n    63 \n  \n  \n    Jackson, Alan \n    www.memory \n    2000-11-04 \n    75 \n    59 \n    59 \n    54 \n    50 \n  \n  \n    Jagged Edge \n    He Can't Love U \n    1999-12-11 \n    54 \n    32 \n    17 \n    17 \n    15 \n  \n  \n    Jagged Edge \n    Let's Get Married \n    2000-05-06 \n    77 \n    66 \n    55 \n    45 \n    38 \n  \n  \n    Janet \n    Doesn't Really Matte... \n    2000-06-17 \n    59 \n    52 \n    43 \n    30 \n    29 \n  \n  \n    Jay-Z \n    Anything \n    2000-02-26 \n    72 \n    58 \n    55 \n    55 \n    63 \n  \n  \n    Jay-Z \n    Big Pimpin' \n    2000-04-22 \n    69 \n    52 \n    39 \n    33 \n    28 \n  \n  \n    Jay-Z \n    Do It Again (Put Ya ... \n    2000-01-15 \n    95 \n    68 \n    65 \n    65 \n    74 \n  \n  \n    Jay-Z \n    Hey Papi \n    2000-08-12 \n    98 \n    100 \n    98 \n    94 \n    83 \n  \n  \n    Jay-Z \n    I Just Wanna Love U ... \n    2000-10-28 \n    58 \n    45 \n    35 \n    26 \n    23 \n  \n  \n    Jean, Wyclef \n    911 \n    2000-10-07 \n    77 \n    74 \n    64 \n    61 \n    53 \n  \n  \n    Joe \n    I Wanna Know \n    2000-01-01 \n    94 \n    86 \n    69 \n    50 \n    41 \n  \n  \n    Joe \n    Treat Her Like A Lad... \n    2000-08-05 \n    77 \n    75 \n    63 \n    63 \n    69 \n  \n  \n    John, Elton \n    Someday Out Of The B... \n    2000-04-22 \n    56 \n    56 \n    49 \n    59 \n    67 \n  \n  \n    Jones, Donell \n    Where I Wanna Be \n    2000-04-22 \n    81 \n    71 \n    65 \n    50 \n    41 \n  \n  \n    Jordan, Montell \n    Get It On.. Tonite \n    1999-10-23 \n    92 \n    80 \n    72 \n    69 \n    67 \n  \n  \n    Juvenile \n    U Understand \n    2000-02-05 \n    85 \n    83 \n    100 \n    98 \n    97 \n  \n  \n    Kandi \n    Don't Think I'm Not \n    2000-08-05 \n    66 \n    66 \n    66 \n    61 \n    49 \n  \n  \n    Keith, Toby \n    Country Comes To Tow... \n    2000-08-05 \n    82 \n    78 \n    75 \n    69 \n    66 \n  \n  \n    Keith, Toby \n    How Do You Like Me N... \n    2000-01-29 \n    77 \n    72 \n    59 \n    53 \n    45 \n  \n  \n    Kelis \n    Caught Out There \n    1999-12-04 \n    84 \n    68 \n    67 \n    63 \n    63 \n  \n  \n    Kenny G \n    Auld Lang Syne (The ... \n    1999-12-25 \n    89 \n    89 \n    7 \n    8 \n    66 \n  \n  \n    Kid Rock \n    Only God Knows Why \n    2000-02-19 \n    63 \n    47 \n    46 \n    39 \n    35 \n  \n  \n    Kravitz, Lenny \n    I Belong To You \n    2000-03-25 \n    78 \n    77 \n    71 \n    71 \n    71 \n  \n  \n    Kumbia Kings \n    U Don't Love Me \n    2000-03-04 \n    81 \n    64 \n    62 \n    67 \n    70 \n  \n  \n    LFO \n    I Don't Wanna Kiss Y... \n    2000-04-15 \n    63 \n    61 \n    68 \n    73 \n    91 \n  \n  \n    LFO \n    West Side Story \n    2000-08-05 \n    96 \n    84 \n    88 \n    96 \n    NA \n  \n  \n    LL Cool J \n    Imagine That \n    2000-08-12 \n    99 \n    98 \n    NA \n    NA \n    NA \n  \n  \n    Larrieux, Amel \n    Get Up \n    2000-03-04 \n    100 \n    97 \n    97 \n    NA \n    NA \n  \n  \n    Lawrence, Tracy \n    Lessons Learned \n    2000-01-29 \n    80 \n    73 \n    61 \n    61 \n    48 \n  \n  \n    Levert, Gerald \n    Baby U Are \n    2000-08-19 \n    96 \n    89 \n    92 \n    96 \n    96 \n  \n  \n    Levert, Gerald \n    Mr. Too Damn Good \n    2000-03-18 \n    84 \n    83 \n    83 \n    76 \n    86 \n  \n  \n    Lil Bow Wow \n    Bounce With Me \n    2000-08-19 \n    48 \n    35 \n    24 \n    24 \n    20 \n  \n  \n    Lil Wayne \n    Tha Block Is Hot \n    1999-12-04 \n    99 \n    89 \n    92 \n    84 \n    84 \n  \n  \n    Lil' Kim \n    How Many Licks? \n    2000-11-25 \n    79 \n    75 \n    77 \n    86 \n    86 \n  \n  \n    Lil' Kim \n    No Matter What They ... \n    2000-07-15 \n    80 \n    72 \n    67 \n    60 \n    65 \n  \n  \n    Lil' Mo \n    Ta Da \n    2000-08-12 \n    100 \n    99 \n    97 \n    97 \n    100 \n  \n  \n    Lil' Zane \n    Callin' Me \n    2000-07-29 \n    83 \n    89 \n    57 \n    40 \n    34 \n  \n  \n    Limp Bizkit \n    N 2 Gether Now \n    1999-12-04 \n    94 \n    88 \n    85 \n    78 \n    78 \n  \n  \n    Limp Bizkit \n    Re-Arranged \n    1999-12-04 \n    91 \n    91 \n    90 \n    95 \n    95 \n  \n  \n    Limp Bizkit \n    Rollin' \n    2000-11-11 \n    77 \n    73 \n    72 \n    66 \n    65 \n  \n  \n    Lonestar \n    Amazed \n    1999-06-05 \n    81 \n    54 \n    44 \n    39 \n    38 \n  \n  \n    Lonestar \n    Smile \n    1999-12-18 \n    89 \n    80 \n    80 \n    80 \n    65 \n  \n  \n    Lonestar \n    What About Now \n    2000-06-10 \n    78 \n    72 \n    66 \n    64 \n    56 \n  \n  \n    Lopez, Jennifer \n    Feelin' Good \n    2000-02-19 \n    79 \n    79 \n    66 \n    54 \n    54 \n  \n  \n    Loveless, Patty \n    That's The Kind Of M... \n    2000-09-16 \n    98 \n    93 \n    93 \n    93 \n    88 \n  \n  \n    Lox \n    Ryde or Die, Chick \n    2000-03-18 \n    86 \n    73 \n    80 \n    84 \n    91 \n  \n  \n    Lucy Pearl \n    Dance Tonight \n    2000-05-20 \n    80 \n    75 \n    63 \n    59 \n    55 \n  \n  \n    Ludacris \n    What's Your Fantasy \n    2000-09-30 \n    89 \n    83 \n    63 \n    55 \n    49 \n  \n  \n    M2M \n    Don't Say You Love M... \n    1999-11-20 \n    72 \n    53 \n    62 \n    46 \n    54 \n  \n  \n    M2M \n    Mirror Mirror \n    2000-04-01 \n    87 \n    87 \n    94 \n    91 \n    75 \n  \n  \n    Madison Avenue \n    Don't Call Me Baby \n    2000-07-08 \n    98 \n    96 \n    93 \n    93 \n    93 \n  \n  \n    Madonna \n    American Pie \n    2000-02-19 \n    43 \n    35 \n    29 \n    29 \n    33 \n  \n  \n    Madonna \n    Music \n    2000-08-12 \n    41 \n    23 \n    18 \n    14 \n    2 \n  \n  \n    Martin, Ricky \n    Private Emotion \n    2000-03-11 \n    76 \n    67 \n    71 \n    78 \n    89 \n  \n  \n    Martin, Ricky \n    Shake Your Bon-Bon \n    1999-11-20 \n    74 \n    66 \n    52 \n    39 \n    39 \n  \n  \n    Martin, Ricky \n    She Bangs \n    2000-10-07 \n    38 \n    28 \n    21 \n    21 \n    18 \n  \n  \n    Mary Mary \n    Shackles (Praise You... \n    2000-03-25 \n    90 \n    76 \n    72 \n    54 \n    43 \n  \n  \n    Master P \n    Souljas \n    2000-11-18 \n    98 \n    NA \n    NA \n    NA \n    NA \n  \n  \n    McBride, Martina \n    Love's The Only Hous... \n    2000-02-05 \n    79 \n    69 \n    65 \n    58 \n    53 \n  \n  \n    McBride, Martina \n    There You Are \n    2000-09-09 \n    79 \n    75 \n    75 \n    75 \n    73 \n  \n  \n    McEntire, Reba \n    I'll Be \n    2000-05-13 \n    89 \n    79 \n    79 \n    72 \n    69 \n  \n  \n    McEntire, Reba \n    What Do You Say \n    1999-10-30 \n    88 \n    76 \n    71 \n    71 \n    69 \n  \n  \n    McGraw, Tim \n    My Best Friend \n    1999-11-27 \n    85 \n    76 \n    71 \n    64 \n    54 \n  \n  \n    McGraw, Tim \n    My Next Thirty Years \n    2000-10-21 \n    73 \n    62 \n    56 \n    52 \n    46 \n  \n  \n    McGraw, Tim \n    Some Things Never Ch... \n    2000-05-13 \n    76 \n    66 \n    66 \n    65 \n    63 \n  \n  \n    McKnight, Brian \n    Stay Or Let It Go \n    2000-02-26 \n    95 \n    92 \n    82 \n    78 \n    76 \n  \n  \n    Messina, Jo Dee \n    Because You Love Me \n    2000-01-29 \n    83 \n    78 \n    71 \n    71 \n    66 \n  \n  \n    Messina, Jo Dee \n    That's The Way \n    2000-06-24 \n    78 \n    67 \n    63 \n    54 \n    50 \n  \n  \n    Metallica \n    I Disappear \n    2000-05-13 \n    86 \n    84 \n    88 \n    81 \n    81 \n  \n  \n    Metallica \n    No Leaf Clover (Live... \n    2000-02-12 \n    86 \n    81 \n    78 \n    76 \n    74 \n  \n  \n    Montgomery Gentry \n    Daddy Won't Sell The... \n    2000-03-04 \n    87 \n    83 \n    81 \n    79 \n    81 \n  \n  \n    Montgomery, John Michael \n    The Little Girl \n    2000-09-09 \n    81 \n    67 \n    64 \n    56 \n    45 \n  \n  \n    Moore, Chante \n    Straight Up \n    2000-10-28 \n    98 \n    98 \n    97 \n    90 \n    85 \n  \n  \n    Moore, Mandy \n    I Wanna Be With You \n    2000-06-17 \n    69 \n    63 \n    54 \n    50 \n    45 \n  \n  \n    Mumba, Samantha \n    Gotta Tell You \n    2000-09-09 \n    85 \n    72 \n    65 \n    49 \n    39 \n  \n  \n    Musiq \n    Just Friends \n    2000-10-14 \n    89 \n    83 \n    65 \n    55 \n    54 \n  \n  \n    Mya \n    Case Of The Ex (What... \n    2000-08-19 \n    72 \n    57 \n    52 \n    47 \n    42 \n  \n  \n    Mya \n    The Best Of Me \n    2000-04-15 \n    85 \n    70 \n    65 \n    62 \n    55 \n  \n  \n    Mystikal \n    Shake Ya Ass \n    2000-08-12 \n    97 \n    90 \n    65 \n    41 \n    34 \n  \n  \n    N'Sync \n    Bye Bye Bye \n    2000-01-29 \n    42 \n    20 \n    19 \n    14 \n    13 \n  \n  \n    N'Sync \n    It's Gonna Be Me \n    2000-05-06 \n    82 \n    70 \n    51 \n    39 \n    26 \n  \n  \n    N'Sync \n    This I Promise You \n    2000-09-30 \n    68 \n    31 \n    19 \n    15 \n    11 \n  \n  \n    Nas \n    You Owe Me \n    2000-03-25 \n    74 \n    72 \n    68 \n    59 \n    59 \n  \n  \n    Nelly \n    (Hot S**t) Country G... \n    2000-04-29 \n    100 \n    99 \n    96 \n    76 \n    55 \n  \n  \n    Next \n    Wifey \n    2000-05-27 \n    85 \n    61 \n    46 \n    40 \n    36 \n  \n  \n    Nine Days \n    Absolutely (Story Of... \n    2000-05-06 \n    85 \n    71 \n    59 \n    52 \n    39 \n  \n  \n    Nine Days \n    If I Am \n    2000-12-02 \n    68 \n    68 \n    81 \n    94 \n    100 \n  \n  \n    No Doubt \n    Simple Kind Of Life \n    2000-07-01 \n    50 \n    40 \n    39 \n    38 \n    38 \n  \n  \n    Nu Flavor \n    3 Little Words \n    2000-06-03 \n    97 \n    97 \n    89 \n    89 \n    94 \n  \n  \n    Offspring, The \n    Original Prankster \n    2000-11-25 \n    74 \n    71 \n    70 \n    70 \n    77 \n  \n  \n    Paisley, Brad \n    Me Neither \n    2000-05-13 \n    87 \n    85 \n    90 \n    92 \n    NA \n  \n  \n    Paisley, Brad \n    We Danced \n    2000-10-14 \n    71 \n    68 \n    52 \n    52 \n    45 \n  \n  \n    Papa Roach \n    Last Resort \n    2000-07-29 \n    75 \n    71 \n    69 \n    69 \n    66 \n  \n  \n    Pearl Jam \n    Nothing As It Seems \n    2000-05-13 \n    49 \n    70 \n    84 \n    89 \n    93 \n  \n  \n    Pink \n    Most Girls \n    2000-08-12 \n    85 \n    70 \n    52 \n    36 \n    27 \n  \n  \n    Pink \n    There U Go \n    2000-03-04 \n    25 \n    15 \n    12 \n    11 \n    11 \n  \n  \n    Price, Kelly \n    As We Lay \n    2000-07-15 \n    82 \n    69 \n    69 \n    64 \n    71 \n  \n  \n    Price, Kelly \n    Love Sets You Free \n    2000-05-13 \n    92 \n    91 \n    98 \n    100 \n    NA \n  \n  \n    Price, Kelly \n    You Should've Told M... \n    2000-09-23 \n    91 \n    91 \n    91 \n    87 \n    86 \n  \n  \n    Profyle \n    Liar \n    2000-09-16 \n    52 \n    32 \n    25 \n    17 \n    16 \n  \n  \n    Puff Daddy \n    Best Friend \n    2000-02-12 \n    65 \n    59 \n    62 \n    79 \n    99 \n  \n  \n    Q-Tip \n    Breathe And Stop \n    2000-01-22 \n    71 \n    71 \n    81 \n    82 \n    96 \n  \n  \n    R.E.M. \n    The Great Beyond \n    1999-12-25 \n    79 \n    79 \n    70 \n    62 \n    60 \n  \n  \n    Rascal Flatts \n    Prayin' For Daylight \n    2000-05-06 \n    87 \n    78 \n    72 \n    68 \n    66 \n  \n  \n    Raye, Collin \n    Couldn't Last A Mome... \n    2000-03-18 \n    91 \n    85 \n    75 \n    73 \n    67 \n  \n  \n    Red Hot Chili Peppers \n    Californication \n    2000-07-29 \n    72 \n    72 \n    72 \n    77 \n    79 \n  \n  \n    Red Hot Chili Peppers \n    Otherside \n    2000-02-12 \n    80 \n    72 \n    65 \n    52 \n    51 \n  \n  \n    Rimes, LeAnn \n    Big Deal \n    1999-10-16 \n    71 \n    52 \n    51 \n    51 \n    51 \n  \n  \n    Rimes, LeAnn \n    Can't Fight The Moon... \n    2000-09-09 \n    82 \n    71 \n    79 \n    83 \n    96 \n  \n  \n    Rimes, LeAnn \n    I Need You \n    2000-05-27 \n    77 \n    68 \n    67 \n    63 \n    59 \n  \n  \n    Rogers, Kenny \n    Buy Me A Rose \n    2000-03-11 \n    79 \n    72 \n    65 \n    65 \n    54 \n  \n  \n    Ruff Endz \n    No More \n    2000-07-01 \n    76 \n    38 \n    19 \n    17 \n    12 \n  \n  \n    Sammie \n    I Like It \n    2000-01-29 \n    85 \n    68 \n    58 \n    44 \n    40 \n  \n  \n    Santana \n    Maria, Maria \n    2000-02-12 \n    15 \n    8 \n    6 \n    5 \n    2 \n  \n  \n    Savage Garden \n    Crash And Burn \n    2000-04-08 \n    75 \n    58 \n    51 \n    36 \n    33 \n  \n  \n    Savage Garden \n    I Knew I Loved You \n    1999-10-23 \n    71 \n    48 \n    43 \n    31 \n    20 \n  \n  \n    SheDaisy \n    Deck The Halls \n    1999-12-25 \n    97 \n    61 \n    NA \n    NA \n    NA \n  \n  \n    SheDaisy \n    I Will.. But \n    2000-07-15 \n    78 \n    74 \n    70 \n    61 \n    59 \n  \n  \n    SheDaisy \n    This Woman Needs \n    2000-02-05 \n    82 \n    70 \n    70 \n    67 \n    57 \n  \n  \n    Sheist, Shade \n    Where I Wanna Be \n    2000-11-11 \n    96 \n    95 \n    99 \n    99 \n    100 \n  \n  \n    Shyne \n    Bad Boyz \n    2000-09-09 \n    94 \n    87 \n    90 \n    90 \n    82 \n  \n  \n    Simpson, Jessica \n    I Think I'm In Love ... \n    2000-07-01 \n    63 \n    52 \n    44 \n    29 \n    25 \n  \n  \n    Simpson, Jessica \n    Where You Are \n    2000-04-01 \n    73 \n    66 \n    62 \n    62 \n    76 \n  \n  \n    Sisqo \n    Got To Get It \n    1999-11-20 \n    92 \n    76 \n    73 \n    58 \n    48 \n  \n  \n    Sisqo \n    Incomplete \n    2000-06-24 \n    77 \n    66 \n    61 \n    61 \n    61 \n  \n  \n    Sisqo \n    Thong Song \n    2000-01-29 \n    74 \n    63 \n    35 \n    26 \n    26 \n  \n  \n    Sister Hazel \n    Change Your Mind \n    2000-07-15 \n    75 \n    67 \n    66 \n    59 \n    63 \n  \n  \n    Smash Mouth \n    Then The Morning Com... \n    1999-10-30 \n    83 \n    59 \n    56 \n    46 \n    27 \n  \n  \n    Smith, Will \n    Freakin' It \n    2000-02-12 \n    99 \n    99 \n    99 \n    99 \n    NA \n  \n  \n    Son By Four \n    A Puro Dolor (Purest... \n    2000-04-08 \n    80 \n    80 \n    80 \n    79 \n    72 \n  \n  \n    Sonique \n    It Feels So Good \n    2000-01-22 \n    67 \n    52 \n    30 \n    23 \n    19 \n  \n  \n    SoulDecision \n    Faded \n    2000-07-08 \n    94 \n    90 \n    81 \n    64 \n    56 \n  \n  \n    Spears, Britney \n    From The Bottom Of M... \n    2000-01-29 \n    76 \n    59 \n    52 \n    52 \n    14 \n  \n  \n    Spears, Britney \n    Lucky \n    2000-08-12 \n    61 \n    41 \n    28 \n    26 \n    23 \n  \n  \n    Spears, Britney \n    Oops!.. I Did It Aga... \n    2000-04-22 \n    67 \n    38 \n    26 \n    19 \n    15 \n  \n  \n    Spencer, Tracie \n    Still In My Heart \n    2000-03-04 \n    95 \n    88 \n    98 \n    NA \n    NA \n  \n  \n    Splender \n    I Think God Can Expl... \n    2000-06-10 \n    71 \n    66 \n    62 \n    62 \n    62 \n  \n  \n    Sting \n    Desert Rose \n    2000-05-13 \n    98 \n    88 \n    72 \n    59 \n    55 \n  \n  \n    Stone Temple Pilots \n    Sour Girl \n    2000-07-08 \n    79 \n    79 \n    79 \n    78 \n    78 \n  \n  \n    Stone, Angie \n    No More Rain (In Thi... \n    1999-12-25 \n    86 \n    86 \n    74 \n    66 \n    56 \n  \n  \n    Strait, George \n    Go On \n    2000-08-26 \n    71 \n    67 \n    63 \n    56 \n    53 \n  \n  \n    Strait, George \n    The Best Day \n    2000-01-29 \n    73 \n    64 \n    54 \n    45 \n    44 \n  \n  \n    Sugar Ray \n    Falls Apart \n    2000-01-15 \n    70 \n    64 \n    54 \n    40 \n    34 \n  \n  \n    TLC \n    Dear Lie \n    2000-02-12 \n    63 \n    55 \n    52 \n    51 \n    56 \n  \n  \n    Tamar \n    If You Don't Wanna L... \n    2000-03-25 \n    98 \n    98 \n    92 \n    89 \n    92 \n  \n  \n    Tamia \n    Can't Go For That \n    2000-09-16 \n    90 \n    86 \n    84 \n    88 \n    97 \n  \n  \n    Third Eye Blind \n    Deep Inside Of You \n    2000-09-02 \n    80 \n    73 \n    70 \n    70 \n    70 \n  \n  \n    Third Eye Blind \n    Never Let You Go \n    2000-01-22 \n    65 \n    32 \n    25 \n    24 \n    23 \n  \n  \n    Thomas, Carl \n    Emotional \n    2000-11-25 \n    77 \n    63 \n    61 \n    58 \n    54 \n  \n  \n    Thomas, Carl \n    I Wish \n    2000-03-25 \n    75 \n    64 \n    48 \n    39 \n    32 \n  \n  \n    Thomas, Carl \n    Summer Rain \n    2000-09-23 \n    82 \n    82 \n    86 \n    80 \n    82 \n  \n  \n    Tippin, Aaron \n    Kiss This \n    2000-08-26 \n    74 \n    72 \n    66 \n    53 \n    52 \n  \n  \n    Train \n    Meet Virginia \n    1999-10-09 \n    76 \n    67 \n    59 \n    54 \n    48 \n  \n  \n    Trick Daddy \n    Shut Up \n    2000-05-20 \n    99 \n    95 \n    87 \n    87 \n    83 \n  \n  \n    Trina \n    Pull Over \n    2000-09-09 \n    97 \n    93 \n    96 \n    100 \n    NA \n  \n  \n    Tritt, Travis \n    Best Of Intentions \n    2000-08-19 \n    97 \n    86 \n    79 \n    70 \n    63 \n  \n  \n    Tuesday \n    I Know \n    2000-12-30 \n    98 \n    98 \n    NA \n    NA \n    NA \n  \n  \n    Urban, Keith \n    Your Everything \n    2000-07-15 \n    81 \n    80 \n    73 \n    73 \n    67 \n  \n  \n    Usher \n    Pop Ya Collar \n    2000-11-04 \n    68 \n    64 \n    60 \n    60 \n    62 \n  \n  \n    Vassar, Phil \n    Carlene \n    2000-03-04 \n    75 \n    67 \n    64 \n    64 \n    57 \n  \n  \n    Vassar, Phil \n    Just Another Day In ... \n    2000-09-30 \n    81 \n    81 \n    76 \n    67 \n    53 \n  \n  \n    Vertical Horizon \n    Everything You Want \n    2000-01-22 \n    70 \n    61 \n    53 \n    46 \n    40 \n  \n  \n    Vertical Horizon \n    You're A God \n    2000-08-26 \n    64 \n    55 \n    43 \n    41 \n    37 \n  \n  \n    Vitamin C \n    Graduation (Friends ... \n    2000-04-15 \n    81 \n    64 \n    54 \n    54 \n    46 \n  \n  \n    Vitamin C \n    The Itch \n    2000-12-02 \n    86 \n    48 \n    45 \n    52 \n    57 \n  \n  \n    Walker, Clay \n    Live, Laugh, Love \n    1999-12-04 \n    95 \n    95 \n    94 \n    94 \n    94 \n  \n  \n    Walker, Clay \n    The Chain Of Love \n    2000-04-15 \n    73 \n    65 \n    57 \n    57 \n    51 \n  \n  \n    Wallflowers, The \n    Sleepwalker \n    2000-10-28 \n    73 \n    73 \n    74 \n    80 \n    90 \n  \n  \n    Westlife \n    Swear It Again \n    2000-04-01 \n    96 \n    82 \n    66 \n    55 \n    55 \n  \n  \n    Williams, Robbie \n    Angels \n    1999-11-20 \n    85 \n    77 \n    69 \n    69 \n    62 \n  \n  \n    Wills, Mark \n    Back At One \n    2000-01-15 \n    89 \n    55 \n    51 \n    43 \n    37 \n  \n  \n    Worley, Darryl \n    When You Need My Lov... \n    2000-06-17 \n    98 \n    88 \n    93 \n    92 \n    85 \n  \n  \n    Wright, Chely \n    It Was \n    2000-03-04 \n    86 \n    78 \n    75 \n    72 \n    71 \n  \n  \n    Yankee Grey \n    Another Nine Minutes \n    2000-04-29 \n    86 \n    83 \n    77 \n    74 \n    83 \n  \n  \n    Yearwood, Trisha \n    Real Live Woman \n    2000-04-01 \n    85 \n    83 \n    83 \n    82 \n    81 \n  \n  \n    Ying Yang Twins \n    Whistle While You Tw... \n    2000-03-18 \n    95 \n    94 \n    91 \n    85 \n    84 \n  \n  \n    Zombie Nation \n    Kernkraft 400 \n    2000-09-02 \n    99 \n    99 \n    NA \n    NA \n    NA \n  \n  \n    matchbox twenty \n    Bent \n    2000-04-29 \n    60 \n    37 \n    29 \n    24 \n    22"
  },
  {
    "objectID": "slides/data-cleaning.html#using-your-helpers",
    "href": "slides/data-cleaning.html#using-your-helpers",
    "title": "Data Cleaning in R",
    "section": "Using Your Helpers",
    "text": "Using Your Helpers\n\nbillboard |> \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\",\n    names_prefix = \"wk\",\n    values_to = \"rank\",\n    values_drop_na = TRUE\n  ) |>\n  head(4)\n\n# A tibble: 4 × 5\n  artist track                   date.entered week   rank\n  <chr>  <chr>                   <date>       <chr> <dbl>\n1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   1        87\n2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   2        82\n3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   3        72\n4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   4        77"
  },
  {
    "objectID": "project-description.html",
    "href": "project-description.html",
    "title": "Project description",
    "section": "",
    "text": "Topic ideas due Fri, Feb 18\nProposal due Fri, Mar 18\nDraft report due Fri, Apr 8\nPeer review due Fri, Apr 15\nFinal report due Mon, Apr 25\nVideo presentation + slides and final GitHub repo due Thu, Apr 28\nPresentation comments due Sat, Apr 30"
  },
  {
    "objectID": "project-description.html#introduction",
    "href": "project-description.html#introduction",
    "title": "Project description",
    "section": "Introduction",
    "text": "Introduction\nTL;DR: Pick a data set and do a regression analysis. That is your final project.\nThe goal of the final project is for you to use regression analysis to analyze a data set of your own choosing. The data set may already exist or you may collect your own data by scraping the web.\nChoose the data based on your group’s interests or work you all have done in other courses or research projects. The goal of this project is for you to demonstrate proficiency in the techniques we have covered in this class (and beyond, if you like!) and apply them to a data set to analyze it in a meaningful way.\nAll analyses must be done in RStudio, and all components of the project must be reproducible (with the exception of the presentation).\n\nLogistics\nYou will work on the project with your lab groups.\nThe four primary deliverables for the final project are\n\nA written, reproducible report detailing your analysis\nA GitHub repository corresponding to your report\nSlides + a video presentation\nFormal peer review on another team’s project"
  },
  {
    "objectID": "project-description.html#topic-ideas",
    "href": "project-description.html#topic-ideas",
    "title": "Project description",
    "section": "Topic ideas",
    "text": "Topic ideas\nIdentify 2-3 data sets you’re interested in potentially using for the final project. If you’re unsure where to find data, you can use the list of potential data sources in the Tips + Resources section as a starting point. It may also help to think of topics you’re interested in investigating and find data sets on those topics.\nThe purpose of submitting project ideas is to give you time to find data for the project and to make sure you have a data set that can help you be successful in the project. Therefore, you must use one of the data sets submitted as a topic idea, unless otherwise notified by the teaching team.\nThe data sets should meet the following criteria:\n\nAt least 500 observations\nAt least 10 columns\nAt least 6 of the columns must be useful and unique predictor variables.\n\nIdentifier variables such as “name”, “social security number”, etc. are not useful predictor variables.\nIf you have multiple columns with the same information (e.g. “state abbreviation” and “state name”), then they are not unique predictors.\n\nAt least one variable that can be identified as a reasonable response variable.\n\nThe response variable can be quantitative or categorical.\n\nA mix of quantitative and categorical variables that can be used as predictors.\nObservations should reasonably meet the independence condition. Therefore, avoid data with repeated measures, data collected over time, etc.\nYou may not use data that has previously been used in any course materials, or any derivation of data that has been used in course materials.\n\nPlease ask a member of the teaching team if you’re unsure whether your data set meets the criteria.\nFor each data set, include the following:\n\nIntroduction and data\n\nState the source of the data set.\nDescribe when and how it was originally collected (by the original data curator, not necessarily how you found the data)\nDescribe the observations and the general characteristics being measured in the data\n\n\n\nResearch question\n\nDescribe a research question you’re interested in answering using this data.\n\n\n\nGlimpse of data\n\nUse the glimpse function to provide an overview of each data set\n\nSubmit the PDF of the topic ideas to Gradescope. Mark all pages associated with each data set."
  },
  {
    "objectID": "project-description.html#project-proposal",
    "href": "project-description.html#project-proposal",
    "title": "Project description",
    "section": "Project proposal",
    "text": "Project proposal\nThe purpose of the project proposal is to help you think about your analysis strategy early.\nInclude the following in the proposal:\n\nSection 1 - Introduction\nThe introduction section includes\n\nan introduction to the subject matter you’re investigating\nthe motivation for your research question (citing any relevant literature)\nthe general research question you wish to explore\nyour hypotheses regarding the research question of interest.\n\n\n\nSection 2 - Data description\nIn this section, you will describe the data set you wish to explore. This includes\n\ndescription of the observations in the data set,\ndescription of how the data was originally collected (not how you found the data but how the original curator of the data collected it).\n\n\n\nSection 3 - Analysis approach\nIn this section, you will provide a brief overview of your analysis approach. This includes:\n\nDescription of the response variable.\nVisualization and summary statistics for the response variable.\nList of variables that will be considered as predictors\nRegression model technique (multiple linear regression and logistic regression)\n\n\n\nData dictionary (aka code book)\nSubmit a data dictionary for all the variables in your data set in the README of your project repo, in the data folder. Link to this file from your proposal writeup.\n\n\nSubmission\nPush all of your final changes to the GitHub repo, and submit the PDF of your proposal to Gradescope.\n\n\nProposal grading\n\n\n\nTotal\n10 pts\n\n\n\n\nIntroduction\n3 pts\n\n\nData description\n2 pts\n\n\nAnalysis plan\n4 pts\n\n\nData dictionary\n1 pts\n\n\n\nEach component will be graded as follows:\n\nMeets expectations (full credit): All required elements are completed and are accurate. The narrative is written clearly, all tables and visualizations are nicely formatted, and the work would be presentable in a professional setting.\nClose to expectations (half credit): There are some elements missing and/or inaccurate. There are some issues with formatting.\nDoes not meet expectations (no credit): Major elements missing. Work is not neatly formatted and would not be presentable in a professional setting."
  },
  {
    "objectID": "project-description.html#draft-report",
    "href": "project-description.html#draft-report",
    "title": "Project description",
    "section": "Draft report",
    "text": "Draft report\nThe purpose of the draft and peer review is to give you an opportunity to get early feedback on your analysis. Therefore, the draft and peer review will focus primarily on the exploratory data analysis, modeling, and initial interpretations.\nWrite the draft in the written-report.qmd file in your project repo. You do not need to submit the draft on Gradescope.\nBelow is a brief description of the sections to focus on in the draft:\n\nIntroduction and data\nThis section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA won’t fit in the paper, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\n\nMethodology\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model you’re fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, any variable transformations (if needed), and any other relevant considerations that were part of the model fitting process.\n\n\nResults\nIn this section, you will output the final model and include a brief discussion of the model assumptions, diagnostics, and any relevant model fit statistics.\nThis section also includes initial interpretations and conclusions drawn from the model."
  },
  {
    "objectID": "project-description.html#peer-review",
    "href": "project-description.html#peer-review",
    "title": "Project description",
    "section": "Peer review",
    "text": "Peer review\nCritically reviewing others’ work is a crucial part of the scientific process, and STA 210 is no exception. Each lab team will be assigned two other teams’s projects to review. Each team should push their draft to their GitHub repo by the due date. One lab in the following week will be dedicated to the peer review, and all reviews will be due by the end of that lab session.\nDuring the peer review process, you will be provided read-only access to your partner teams’ GitHub repos. Provide your review in the form of GitHub issues to your partner team’s GitHub repo using the issue template provided. The peer review will be graded on the extent to which it comprehensively and constructively addresses the components of the partner team’s report: the research context and motivation, exploratory data analysis, modeling, interpretations, and conclusions.\n\nPairings\n\nSection 1 - M 1:45PM - 3:00PM\n\n\n\nTeam being reviewed\nReviewer 1\nReviewer 2\n\n\n\n\nchaa_chaa_chaa\nyay_stats\nstat_over_flow\n\n\ndekk\nchaa_chaa_chaa\nyay_stats\n\n\neight\ndekk\nchaa_chaa_chaa\n\n\nhousecats\neight\ndekk\n\n\nkrafthouse\nhousecats\neight\n\n\nrrawr\nkrafthouse\nhousecats\n\n\nstat_over_flow\nrrawr\nkrafthouse\n\n\nyay_stats\nstat_over_flow\nrrawr\n\n\n\n\n\nSection 2 - M 3:30PM - 4:45PM\n\n\n\nTeam being reviewed\nReviewer 1\nReviewer 2\n\n\n\n\na_plus_plus_plus\nwe_r\ntina\n\n\npredictors\na_plus_plus_plus\nwe_r\n\n\nsixers\npredictors\na_plus_plus_plus\n\n\nsoy_nuggets\nsixers\npredictors\n\n\ntina\nsoy_nuggets\nsixers\n\n\nwe_r\ntina\nsoy_nuggets\n\n\n\n\n\nSection 3 - M 5:15PM - 6:30PM\n\n\n\n\n\n\n\n\nTeam being reviewed\nReviewer 1\nReviewer 2\n\n\n\n\ndown_to_earth_goats\nthe_three_musketeers\nteam_five\n\n\nginger_and_stats\ndown_to_earth_goats\nthe_three_musketeers\n\n\npineapple_wedge_and_diced_papaya\nginger_and_stats\ndown_to_earth_goats\n\n\nstatchelorettes\npineapple_wedge_and_diced_papaya\nginger_and_stats\n\n\nstatisix\nstatchelorettes\npineapple_wedge_and_diced_papaya\n\n\nstats_squad\nstatisix\nstatchelorettes\n\n\nteam_five\nstats_squad\nstatisix\n\n\nthe_three_musketeers\nteam_five\nstats_squad\n\n\n\n\n\n\nProcess and questions\nSpend ~30 mins to review each team’s project.\n\nFind your team name on the Reviewer 1 and Reviewer 2 columns.\nFor each of the columns, find the name of the team to review in the Team being reviewed column. You should already have access to this team’s repo.\nOpen the repo of the team you’re reviewing, read their project draft, and browser around the rest of their repo.\nThen, go to the Issues tab in that repo, click on New issue, and click on Get started for the Peer review issue. Fill out this issue, answering the following questions:\n\nPeer review by: [NAME OF TEAM DOING THE REVIEW]\nNames of team members that participated in this review: [FULL NAMES OF TEAM MEMBERS DOING THE REVIEW]\nDescribe the goal of the project.\nDescribe the data used or collected, if any. If the proposal does not include the use of a specific dataset, comment on whether the project would be strengthened by the inclusion of a dataset.\nDescribe the approaches, tools, and methods that will be used.\nIs there anything that is unclear from the proposal?\nProvide constructive feedback on how the team might be able to improve their project. Make sure your feedback includes at least one comment on the statistical modeling aspect of the project, but do feel free to comment on aspects beyond the modeling.\nWhat aspect of this project are you most interested in and would like to see highlighted in the presentation.\nProvide constructive feedback on any issues with file and/or code organization.\n(Optional) Any further comments or feedback?"
  },
  {
    "objectID": "project-description.html#written-report",
    "href": "project-description.html#written-report",
    "title": "Project description",
    "section": "Written report",
    "text": "Written report\nYour written report must be completed in the written-report.qmd file and must be reproducible. All team members should contribute to the GitHub repository, with regular meaningful commits.\nBefore you finalize your write up, make sure the printing of code chunks is off with the option echo = FALSE.\nYou will submit the PDF of your final report on Gradescope.\nThe PDF you submit must match the files in your GitHub repository exactly. The mandatory components of the report are below. You are free to add additional sections as necessary. The report, including visualizations, should be no more than 10 pages long. is no minimum page requirement; however, you should comprehensively address all of the analysis and report.\nBe selective in what you include in your final write-up. The goal is to write a cohesive narrative that demonstrates a thorough and comprehensive analysis rather than explain every step of the analysis.\nYou are welcome to include an appendix with additional work at the end of the written report document; however, grading will largely be based on the content in the main body of the report. You should assume the reader will not see the material in the appendix unless prompted to view it in the main body of the report. The appendix should be neatly formatted and easy for the reader to navigate. It is not included in the 10-page limit.\nThe written report is worth 40 points, broken down as follows\n\n\n\nTotal\n40 pts\n\n\n\n\nIntroduction/data\n6 pts\n\n\nMethodology\n10 pts\n\n\nResults\n14 pts\n\n\nDiscussion + conclusion\n6 pts\n\n\nOrganization + formatting\n4 pts\n\n\n\nClick here for a PDF of the written report rubric.\n\nIntroduction and data\nThis section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA won’t fit in the paper, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\nGrading criteria\nThe research question and motivation are clearly stated in the introduction, including citations for the data source and any external research. The data are clearly described, including a description about how the data were originally collected and a concise definition of the variables relevant to understanding the report. The data cleaning process is clearly described, including any decisions made in the process (e.g., creating new variables, removing observations, etc.) The explanatory data analysis helps the reader better understand the observations in the data along with interesting and relevant relationships between the variables. It incorporates appropriate visualizations and summary statistics.\n\n\n\nMethodology\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model you’re fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, interactions considered, variable transformations (if needed), assessment of conditions and diagnostics, and any other relevant considerations that were part of the model fitting process.\n\nGrading criteria\nThe analysis steps are appropriate for the data and research question. The group used a thorough and careful approach to select the final model; the approach is clearly described in the report. The model selection process took into account potential interaction effects and addressed any violations in model conditions. The model conditions and diagnostics are thoroughly and accurately assessed for their model. If violations of model conditions are still present, there was a reasonable attempt to address the violations based on the course content.\n\n\n\nResults\nThis is where you will output the final model with any relevant model fit statistics.\nDescribe the key results from the model. The goal is not to interpret every single variable in the model but rather to show that you are proficient in using the model output to address the research questions, using the interpretations to support your conclusions. Focus on the variables that help you answer the research question and that provide relevant context for the reader.\n\nGrading criteria\nThe model fit is clearly assessed, and interesting findings from the model are clearly described. Interpretations of model coefficients are used to support the key findings and conclusions, rather than merely listing the interpretation of every model coefficient. If the primary modeling objective is prediction, the model’s predictive power is thoroughly assessed.\n\n\n\nDiscussion + Conclusion\nIn this section you’ll include a summary of what you have learned about your research question along with statistical arguments supporting your conclusions. In addition, discuss the limitations of your analysis and provide suggestions on ways the analysis could be improved. Any potential issues pertaining to the reliability and validity of your data and appropriateness of the statistical analysis should also be discussed here. Lastly, this section will include ideas for future work.\n\nGrading criteria\nOverall conclusions from analysis are clearly described, and the model results are put into the larger context of the subject matter and original research question. There is thoughtful consideration of potential limitations of the data and/or analysis, and ideas for future work are clearly described.\n\n\n\nOrganization + formatting\nThis is an assessment of the overall presentation and formatting of the written report.\n\nGrading criteria\nThe report neatly written and organized with clear section headers and appropriately sized figures with informative labels. Numerical results are displayed with a reasonable number of digits, and all visualizations are neatly formatted. All citations and links are properly formatted. If there is an appendix, it is reasonably organized and easy for the reader to find relevant information. All code, warnings, and messages are suppressed. The main body of the written report (not including the appendix) is no longer than 10 pages."
  },
  {
    "objectID": "project-description.html#video-presentation-slides",
    "href": "project-description.html#video-presentation-slides",
    "title": "Project description",
    "section": "Video presentation + slides",
    "text": "Video presentation + slides\n\nSlides\nIn addition to the written report, your team will also create presentation slides and record a video presentation that summarize and showcase your project. Introduce your research question and data set, showcase visualizations, and discuss the primary conclusions. These slides should serve as a brief visual addition to your written report and will be graded for content and quality.\nFor submission, convert these slides to a .pdf document, and submit the PDF of the slides on Gradescope.\nThe slide deck should have no more than 6 content slides + 1 title slide. Here is a suggested outline as you think through the slides; you do not have to use this exact format for the 6 slides.\n\nTitle Slide\nSlide 1: Introduce the topic and motivation\nSlide 2: Introduce the data\nSlide 3: Highlights from EDA\nSlide 4: Final model\nSlide 5: Interesting findings from the model\nSlide 6: Conclusions + future work\n\n\n\nVideo presentation\nFor the video presentation, you can speak over your slide deck, similar to the lecture content videos. The video presentation must be no longer than 8 minutes. It is fine if the video is shorter than 8 minutes, but it cannot exceed 8 minutes. You may use can use any platform that works best for your group to record your presentation. Below are a few resources on recording videos:\n\nRecording presentations in Zoom\nApple Quicktime for screen recording\nWindows 10 built-in screen recording functionality\nKap for screen recording\n\nOnce your video is ready, upload the video to Warpwire, then embed the video in an new discussion post on Conversations.\n\nTo upload your video to Warpwire:\n\nClick the Warpwire tab in the course Sakai site.\nClick the “+” and select “Upload files”.\nLocate the video on your computer and click to upload.\nOnce you’ve uploaded the video to Warpwire, click to share the video and copy the video’s URL. You will need this when you post the video in the discussion forum.\n\n\n\nTo post the video to the discussion forum\n\nClick the Presentations tab in the course Sakai site.\nClick the Presentations topic.\nClick “Start a new conversation”.\nMake the title “Your Team Name: Project Title”. For example, “Teaching Team: Our Awesome Presentation”.\nClick the Warpwire icon (between the table and shopping cart icons).\nSelect your video, then click “Insert 1 item.” This will embed your video in the conversation.\nUnder the video, paste the URL to your video.\nYou’re done!"
  },
  {
    "objectID": "project-description.html#presentation-comments",
    "href": "project-description.html#presentation-comments",
    "title": "Project description",
    "section": "Presentation comments",
    "text": "Presentation comments\nEach student will be assigned 2 presentations to watch. Your viewing assignments will be posted later in the semester.\nWatch the group’s video, then click “Reply” to post a question for the group. You may not post a question that’s already been asked on the discussion thread. Additionally, the question should be (i) substantive (i.e. it shouldn’t be “Why did you use a bar plot instead of a pie chart”?), (ii) demonstrate your understanding of the content from the course, and (iii) relevant to that group’s specific presentation, i.e demonstrating that you’ve watched the presentation.\nThis portion of the project will be assessed individually.\n\nPairings\nFind your team name in the first column, watch videos from teams in the second column and leave comments.\n\n\n\n\n\n\n\n\nReviewer\nFirst video to review\nSecond video to review\n\n\n\n\nGinger and Stats\nEight\nWe R\n\n\nKrafthouse\nGinger and Stats\nEight\n\n\nSoy Nuggets\nKrafthouse\nGinger and Stats\n\n\nDown To Earth Goats\nSoy Nuggets\nKrafthouse\n\n\nA+++\nDown To Earth Goats\nSoy Nuggets\n\n\nTeam Five\nA+++\nDown To Earth Goats\n\n\nRrawr\nTeam Five\nA+++\n\n\nHousecats\nRrawr\nTeam Five\n\n\nDekk\nHousecats\nRrawr\n\n\nStat OverFlow\nDekk\nHousecats\n\n\nThe Three Musketeers\nStat OverFlow\nDekk\n\n\nPredictors\nThe Three Musketeers\nStat OverFlow\n\n\nStats Squad\nPredictors\nThe Three Musketeers\n\n\nStatisix\nStats Squad\nPredictors\n\n\nSixers\nStatisix\nStats Squad\n\n\nYay Stats\nSixers\nStatisix\n\n\nTINA\nYay Stats\nSixers\n\n\nStatchelorettes\nTINA\nYay Stats\n\n\nPineapple Wedge and Diced Papaya\nStatchelorettes\nTINA\n\n\nChaa Chaa Chaa\nPineapple Wedge and Diced Papaya\nStatchelorettes\n\n\nWe R\nChaa Chaa Chaa\nPineapple Wedge and Diced Papaya\n\n\nEight\nWe R\nChaa Chaa Chaa"
  },
  {
    "objectID": "project-description.html#reproducibility-organization",
    "href": "project-description.html#reproducibility-organization",
    "title": "Project description",
    "section": "Reproducibility + organization",
    "text": "Reproducibility + organization\nAll written work (with exception of presentation slides) should be reproducible, and the GitHub repo should be neatly organized.\nThe GitHub repo should have the following structure:\n\nREADME: Short project description and data dictionary\nwritten-report.qmd & written-report.pdf: Final written report\n/data: Folder that contains the data set for the final project.\n/previous-work: Folder that contains the topic-ideas and project-proposal files.\n/presentation: Folder with the presentation slides.\n\nIf your presentation slides are online, you can put a link to the slides in a README.md file in the presentation folder.\n\n\nPoints for reproducibility + organization will be based on the reproducibility of the written report and the organization of the project GitHub repo. The repo should be neatly organized as described above, there should be no extraneous files, all text in the README should be easily readable."
  },
  {
    "objectID": "project-description.html#peer-teamwork-evaluation",
    "href": "project-description.html#peer-teamwork-evaluation",
    "title": "Project description",
    "section": "Peer teamwork evaluation",
    "text": "Peer teamwork evaluation\nYou will be asked to fill out a survey where you rate the contribution and teamwork of each team member by assigning a contribution percentage for each team member. Filling out the survey is a prerequisite for getting credit on the team member evaluation. If you are suggesting that an individual did less than half the expected contribution given your team size (e.g., for a team of four students, if a student contributed less than 12.5% of the total effort), please provide some explanation. If any individual gets an average peer score indicating that this was the case, their grade will be assessed accordingly.\nIf you have concerns with the teamwork and/or contribution from any team members, please email me by the project video deadline. You only need to email me if you have concerns. Otherwise, I will assume everyone on the team equally contributed and will receive full credit for the teamwork portion of the grade."
  },
  {
    "objectID": "project-description.html#overall-grading",
    "href": "project-description.html#overall-grading",
    "title": "Project description",
    "section": "Overall grading",
    "text": "Overall grading\nThe grade breakdown is as follows:\n\n\n\nTotal\n100 pts\n\n\n\n\nTopic ideas\n5 pts\n\n\nProject proposal\n10 pts\n\n\nPeer review\n10 pts\n\n\nWritten report\n40 pts\n\n\nSlides + video presentation\n20 pts\n\n\nReproducibility + organization\n5 pts\n\n\nVideo comments\n5 pts\n\n\nPeer teamwork evaluation\n5 pts\n\n\n\nNote: No late project reports or videos are accepted.\n\nGrading summary\nGrading of the project will take into account the following:\n\nContent - What is the quality of research and/or policy question and relevancy of data to those questions?\nCorrectness - Are statistical procedures carried out and explained correctly?\nWriting and Presentation - What is the quality of the statistical presentation, writing, and explanations?\nCreativity and Critical Thought - Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\n\nA general breakdown of scoring is as follows:\n\n90%-100%: Outstanding effort. Student understands how to apply all statistical concepts, can put the results into a cogent argument, can identify weaknesses in the argument, and can clearly communicate the results to others.\n80%-89%: Good effort. Student understands most of the concepts, puts together an adequate argument, identifies some weaknesses of their argument, and communicates most results clearly to others.\n70%-79%: Passing effort. Student has misunderstanding of concepts in several areas, has some trouble putting results together in a cogent argument, and communication of results is sometimes unclear.\n60%-69%: Struggling effort. Student is making some effort, but has misunderstanding of many concepts and is unable to put together a cogent argument. Communication of results is unclear.\nBelow 60%: Student is not making a sufficient effort.\n\n\n\nLate work policy\nThere is no late work accepted on this project. Be sure to turn in your work early to avoid any technological mishaps."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Georgia State Research Data Services R Workshop",
    "section": "",
    "text": "Date\nLocation & Time\nTitle\nMaterials\n\n\n\n\nSeptember 7\nOnline: 11:00 - 1:00 pm\nGetting Started in R\n1\n\n\nSeptember 21\nOnline: 11:00-1:00 pm\nTidyverse and Manipulating Data\n2\n\n\nSeptember 30\nOnline: 11:00-1:00 pm\nData Visualization and Mapping\n3\n\n\nOctober 7\nOnline: 11:00-1:00 pm\nStatistical Modelling in R\n4\n\n\nOctober 20\nClassroom 2, Library North 2 12:00-2:00pm\nGetting Started in R\n1\n\n\nOctober 27\nClassroom 2, Library North 2 12:00-2:00pm\nTidyverse and Manipulating Data\n2\n\n\nNovember 3\nClassroom 2, Library North 2 12:00-2:00pm\nData Visualization and Mapping\n3\n\n\nNovember 10\nClassroom 2, Library North 2 12:00-2:00pm\nStatistical Modelling in R\n4"
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Installing R",
    "section": "",
    "text": "Many students starting out for the first time ask \"Which stats software should I use? R or Stata?\" The debates have gotten stale so I hired the couple on TikTok again to come in and give their two cents. pic.twitter.com/Enl9jz8KHx\n\n— scott cunningham (@causalinf) August 19, 2022"
  },
  {
    "objectID": "computing-access.html#installing-r-and-rstudio",
    "href": "computing-access.html#installing-r-and-rstudio",
    "title": "Installing R",
    "section": "Installing R and RStudio",
    "text": "Installing R and RStudio\n\nInstalling R\nFirst we need to install R.\n\nGo to the CRAN(Collective R Archive Network) website https://cran.r-project.org/\nYou should see download R for your operating system at the top of the page under “Download and Install R”\n\n\nIf you use macOS make sure you choose one of the notarized and signed versions. Right now it is 4.2.1. If you have an intel based mac choose the first one. If you have an ARM mac than choose the ARM one.\n\nIf you use Windows, click “base” (or click on the on the bolded one that says “install R for the first time and download it).\n\n\n\nDouble click on the downloaded file(check your Downloads folder). Click yes on all the prompts\nIf you are on a Mac you also need to download and install XQuartz. If you are on a windows machine you do not need to install XQuartz\n\n\n\nInstalling RStudio\nNext, you need to install RStudio, the nicer integrated devloper environment (IDE) for R (the dashboard). Once R and RStudio are both installed, you can ignore R and only use RStudio. RStudio will use R automatically and you won’t ever have to interact with it directly.\n\nGo to the free download location on RStudio’s website: https://www.rstudio.com/products/rstudio/download/#download\nThe Website should detect what operating system you are on and show a button that says *“Download RStudio for __* If not scroll and select the Rstudio for your operating system.\nDouble click on the downloaded file(check your Downloads folderr). Click yes for each prompt."
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "Research Data Services R Workshop",
    "section": "",
    "text": "via GIPHY\n\nHere you will find the materials produced for the Research Data Services R workshops. The website and workshop slides were made using quarto which is an awesome tool which we will 🤞🏽 have a workshop on some time in the future. The content of the workshop are a mashup of lots of resources that helped me learn R in particular and will post links to all the wonderful and free materials I use. I strongly recommend you check them out!"
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Research Data Services R Workshops at Georgia State University",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "project-tips-resources.html",
    "href": "project-tips-resources.html",
    "title": "Project tips + resources",
    "section": "",
    "text": "R Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\n\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "project-tips-resources.html#tips",
    "href": "project-tips-resources.html#tips",
    "title": "Project tips + resources",
    "section": "Tips",
    "text": "Tips\n\nAsk questions if any of the expectations are unclear.\nCode: In your write up your code should be hidden (echo = FALSE) so that your document is neat and easy to read. However your document should include all your code such that if I re-knit your qmd file I should be able to obtain the results you presented.\n\nException: If you want to highlight something specific about a piece of code, you’re welcome to show that portion.\n\nMerge conflicts will happen, issues will arise, and that’s fine! Commit and push often, and ask questions when stuck.\nMake sure each team member is contributing, both in terms of quality and quantity of contribution (we will be reviewing commits from different team members).\nAll team members are expected to contribute equally to the completion of this assignment and group assessments will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works."
  },
  {
    "objectID": "project-tips-resources.html#formatting-communication-tips",
    "href": "project-tips-resources.html#formatting-communication-tips",
    "title": "Project tips + resources",
    "section": "Formatting + communication tips",
    "text": "Formatting + communication tips\n\nSuppress Code, Warnings, & Messages\n\nInclude the following code in a code chunk at the top of your .qmd file to suppress all code, warnings, and other messages. Use the code chunk header {r set-up, include = FALSE} to suppress this set up code.\n\nknitr::opts_chunk$set(echo = FALSE,\n                      warning = FALSE, \n                      message = FALSE)\n\n\nHeaders\n\nUse headers to clearly label each section.\nInspect the document outline to review your headers and sub-headers.\n\n\n\nReferences\n\nInclude all references in a section called “References” at the end of the report.\nThis course does not have specific requirements for formatting citations and references.\n\n\n\nAppendix\n\nIf you have additional work that does not fit or does not belong in the body of the report, you may put it at the end of the document in section called “Appendix”.\nThe items in the appendix should be properly labeled.\nThe appendix should only be for additional material. The reader should be able to fully understand your report without viewing content in the appendix.\n\n\n\nResize figures\nResize plots and figures, so you have more space for the narrative.\n\n\nArranging plots\nArrange plots in a grid, instead of one after the other. This is especially useful when displaying plots for exploratory data analysis and to check assumptions.\nIf you’re using ggplot2 functions, the patchwork package makes it easy to arrange plots in a grid. See the documentation and examples here.\n\n\nPlot titles and axis labels\nBe sure all plot titles and axis labels are visible and easy to read.\n\nUse informative titles, not variable names, for titles and axis labels.\n\n❌ NO! The x-axis is hard to read because the names overlap.\n\nggplot(data = mpg, aes(x = manufacturer)) +\n  geom_bar()\n\n\n\n\n✅ YES! Names are readable\n\nggplot(data = mpg, aes(y = manufacturer)) +\n  geom_bar()\n\n\n\n\n\n\nDo a little more to make the plot look professional!\n\nInformative title and axis labels\nFlipped coordinates to make names readable\nArranged bars based on count\nCapitalized manufacturer names\nOptional: Added color - Use a coordinated color scheme throughout paper / presentation\nOptional: Applied a theme - Use same theme throughout paper / presentation\n\n\nmpg %>%\n  count(manufacturer) %>%\n  mutate(manufacturer = str_to_title(manufacturer)) %>%\n  ggplot(aes(y = fct_reorder(manufacturer,n), x = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(x = \"Manufacturer\", \n       y = \"Count\", \n       title = \"The most common manufacturer is Dodge\") +\n  theme_minimal() \n\n\n\n\n\n\nTables and model output\n\nUse the kable function from the knitr package to neatly output all tables and model output. This will also ensure all model coefficients are displayed.\n\nUse the digits argument to display only 3 or 4 significant digits.\nUse the caption argument to add captions to your table.\n\n\n\nmodel <- lm(mpg ~ hp, data = mtcars)\ntidy(model) %>%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n30.099\n1.634\n18.421\n0\n\n\nhp\n-0.068\n0.010\n-6.742\n0\n\n\n\n\n\n\n\nGuidelines for communicating results\n\nDon’t use variable names in your narrative! Use descriptive terms, so the reader understands your narrative without relying on the data dictionary.\n\n❌ There is a negative linear relationship between mpg and hp.\n✅ There is a negative linear relationship between a car’s fuel economy (in miles per gallon) and its horsepower.\n\nKnow your audience: Your report should be written for a general audience who has an understanding of statistics at the level of STA 210.\nAvoid subject matter jargon: Don’t assume the audience knows all of the specific terminology related to your subject area. If you must use jargon, include a brief definition the first time you introduce a term.\nTell the “so what”: Your report and presentation should be more than a list of interpretations and technical definitions. Focus on what the results mean, i.e. what you want the audience to know about your topic after reading your report or viewing your presentation.\n\n❌ For every one unit increase in horsepower, we expect the miles per gallon to decrease by 0.068 units, on average.\n✅ If the priority is to have good fuel economy, then one should choose a car with lower horsepower. Based on our model, the fuel economy is expected to decrease, on average, by 0.68 miles per gallon for every 10 additional horsepower.\n\nTell a story: All visualizations, tables, model output, and narrative should tell a cohesive story!\nUse one voice: Though multiple people are writing the report, it should read as if it’s from a single author. At least one team member should read through the report before submission to ensure it reads like a cohesive document."
  },
  {
    "objectID": "project-tips-resources.html#additional-resources",
    "href": "project-tips-resources.html#additional-resources",
    "title": "Project tips + resources",
    "section": "Additional resources",
    "text": "Additional resources\n\nR for Data Science\nQuarto Documentation\nData visualization\n\nggplot2 Reference\nggplot2: Elegant Graphics for Data Analysis\nData Visualization: A Practice Introduction\nPatchwork R Package"
  },
  {
    "objectID": "computing-troubleshooting.html",
    "href": "computing-troubleshooting.html",
    "title": "Troubleshooting Errors and Warnings",
    "section": "",
    "text": "We frequently run into error messages and warnings in these workshops! No need to fret this happens to the best R users all the time. It may be helpful to go over them quickly."
  },
  {
    "objectID": "computing-troubleshooting.html#file-does-not-exist-or-no-such-file-or-directory",
    "href": "computing-troubleshooting.html#file-does-not-exist-or-no-such-file-or-directory",
    "title": "Troubleshooting Errors and Warnings",
    "section": "File does not exist, or No such file or directory",
    "text": "File does not exist, or No such file or directory\nThis generally means one of two things. You misspelled the filename or you forgot to tell R where to look. To fix it you should check a few things\n\nCheck to make sure that the name of the file is spelled correctly\nCheck to make sure that your working directory is set correctly\n\nIf the file is living in a folder in the working directory specify that folder with the correct slash\nThis is probably the most common ones\n\nIf the file is open in Excel close it. Sometimes Excel will hold things hostage so that could be your issue."
  },
  {
    "objectID": "computing-troubleshooting.html#could-not-find-function-functionineedtouse",
    "href": "computing-troubleshooting.html#could-not-find-function-functionineedtouse",
    "title": "Troubleshooting Errors and Warnings",
    "section": "Could Not find function “functionineedtouse”",
    "text": "Could Not find function “functionineedtouse”\nThis is just R saying that it does not know what you are trying to do to the object because it can’t figure what the function is.\n\nIs the function spelled correctly(my most common mistake)\nIf so did you run library(packagewithfunctionname)\nDo you need to install the package that function is living in?"
  },
  {
    "objectID": "computing-troubleshooting.html#object-not-found",
    "href": "computing-troubleshooting.html#object-not-found",
    "title": "Troubleshooting Errors and Warnings",
    "section": "Object Not found",
    "text": "Object Not found\nIf you get this error than this is R telling you that you are trying to reference an object that does not exist.\n\nMake sure the object name is spelled right\nMake sure that you have in fact assigned the said object\n\nDid you do something with all the neccessary R code, but forgot to assign it before running the code in question?"
  },
  {
    "objectID": "computing-troubleshooting.html#column-does-not-exist",
    "href": "computing-troubleshooting.html#column-does-not-exist",
    "title": "Troubleshooting Errors and Warnings",
    "section": "Column Does Not Exist",
    "text": "Column Does Not Exist\nIf you get his error than this means you are trying to reference a column that does not exist.\n\nIs the name of the variable spelled correctly?\n\nThis can also include white space which requires you to use Name of Var\n\nMake sure you are referencing the right object\n\ni.e. Did you create a new variable and assign it to a different object?\n\nWhen referencing the column did you drop it along the way?"
  },
  {
    "objectID": "hw/hw-5.html",
    "href": "hw/hw-5.html",
    "title": "HW 5 - Statistics Experience",
    "section": "",
    "text": "The world of statistics and data science is vast and continually growing! The goal of the statistics experience assignments is to help you engage with the statistics and data science communities outside of the classroom.\nYou may submit the statistics experience assignment anytime between now and the deadline.\nEach experience has two parts:\n1️⃣ Have a statistics experience\n2️⃣ Make a slide summarizing on your experience\nYou must complete both parts to receive credit."
  },
  {
    "objectID": "hw/hw-5.html#part-1-experience-statistics-outside-of-the-classroom",
    "href": "hw/hw-5.html#part-1-experience-statistics-outside-of-the-classroom",
    "title": "HW 5 - Statistics Experience",
    "section": "Part 1: Experience statistics outside of the classroom",
    "text": "Part 1: Experience statistics outside of the classroom\nComplete an activity in one of the categories below. Under each category are suggested activities. You do not have to do one these suggested activities. You are welcome to find other activities as long as they are related to statistics/data science and they fit in one of the six categories. If there is an activity you’d like to do but you’re not sure if it qualifies for the statistics experience, just ask!\n\nCategory 1: Attend a talk or conference\nAttend an talk, panel, or conference related to statistics or data science. If you are attending a single talk or panel, it must be at least 30 minutes to count towards the statistics experience. The event can be in-person or online.\n\n\nCategory 2: Talk with a statistician/ data scientist\nTalk with someone who uses statistics in their daily work. This could include a professor, professional in industry, graduate student, etc.\n\n\nCategory 3: Listen to a podcast / watch video\nListen to a podcast or watch a video about statistics and data science. The podcast or video must be at least 30 minutes to count towards the statistics experience. A few suggestions are below:\n\nStats + Stories Podcast\nCausal Inference Podcast\nFiveThirtyEight Model Talk\nrstudio::global 2021 talks\nrstudio::conf 2020 talks\n\nThis list is not exhaustive. You may listen to other podcasts or watch other statistics/data science videos not included on this list. Ask your professor if you are unsure whether a particular podcast or video will count towards the statistics experience.\n\n\nCategory 4: Participate in a data science competition or challenge\nParticipate in a statistics or data science competition. You can participate individually or with a team. One option is DataFest, which will take place over the April 1-3, 2022 weekend. More information to follow here.\n\n\nCategory 5: Read a book on statistics/data science\nThere are a lot of books about statistics, data science, and related topics. A few suggestions are below. If you decide to read a book that isn’t on this list, ask your professor to make sure it counts toward the experience. Many of these books are available through Duke library.\n\nWeapons of Math Destruction by Cathy O’Neil\nHow Charts Lie: Getting Smarter about Visual Information by Alberto Cairo\nThe Theory that Would Not Die by Sharon Bertsch McGrayne\nThe Art of Statistics: How to learn from data by David Spiegelhalter\nThe Signal and the Noise: Why so many predictions fail - but some don’t by Nate Silver\nHow Charts Lie by Alberto Cairo\nList of books about data science ethics\n\n\n\nCategory 6: TidyTuesday\nYou may also participate in a TidyTuesday challenge. New data sets are announced on Monday afternoons.You can find more information about TidyTuesday and see the data in the TidyTuesday GitHub repo.\nA few guidelines:\n✅ Create a GitHub repo for your TidyTuesday submission. Your repo should include\n\nThe R Markdown file with all the code needed to reproduce your visualization.\nA README that includes an image of your final visualization and a short summary (~ 1 paragraph) about your visualization.\n\n✅ The visualization should include features or customization that are beyond what we’ve done in class .\n✅ Include the link to your GitHub repo in the slide summarizing your experience.\n\n\nCategory 7: Coding out loud\nWatch an episode of Coding out loud (either live or pre-recorded) and work through the project.\nA few guidelines:\n✅ Create a GitHub repo for your Coding out loud submission. Your repo should include\n\nThe Quarto file with all the code needed to reproduce your visualization.\nA README that includes an image of your final visualization and a short summary (~ 1 paragraph) about your visualization.\n\n✅ The final product (visualuzation, table, etc.) should include features or customization that are beyond what was achieved in the Coding out loud episode.\n✅ Include the link to your GitHub repo in the slide summarizing your experience."
  },
  {
    "objectID": "hw/hw-5.html#part-2-summarize-your-experience",
    "href": "hw/hw-5.html#part-2-summarize-your-experience",
    "title": "HW 5 - Statistics Experience",
    "section": "Part 2: Summarize your experience",
    "text": "Part 2: Summarize your experience\nMake one slide summarizing your experience. Submit the slide as a PDF on Gradescope.\nInclude the following on your slide:\n\nName and brief description of the event/podcast/competition/etc.\nSomething you found new, interesting, or unexpected\nHow the event/podcast/competition/etc. connects to something we’ve done in class.\nCitation or link to web page for event/competition/etc.\n\nClick here to see a template to help you get started on your slide. Your slide does not have to follow this exact format; it just needs to include the information mentioned above and be easily readable (i.e. use a reasonable font size!). Creativity is encouraged!"
  },
  {
    "objectID": "hw/hw-5.html#submission",
    "href": "hw/hw-5.html#submission",
    "title": "HW 5 - Statistics Experience",
    "section": "Submission",
    "text": "Submission\nSubmit the reflection as a PDF under the HW 5 - Statistics Experience assignment on Gradescope by Fri, Apr 15 at 5 pm ET. It must be submitted by the deadline on Gradescope to be considered for grading."
  },
  {
    "objectID": "hw/hw-4.html",
    "href": "hw/hw-4.html",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "",
    "text": "In this assignment, you’ll get to put into practice the logistic regression skills you’ve developed.\n\n\nIn this assignment, you will…\n\nFit and interpret multinomial logistic regression models.\nEvaluate model conditions\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-4. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(patchwork)"
  },
  {
    "objectID": "hw/hw-4.html#data",
    "href": "hw/hw-4.html#data",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Data",
    "text": "Data\nFor this assignment, we will analyze data from the eye witness identification experiment in Carlson and Carlson (2014). In this experiment, participants were asked to watch a video of a mock crime (from the first person perspective), spend a few minutes completing a random task, and then identify the perpetrator of the mock crime from a line up shown on the screen. Every lineup in this analysis included the true perpetrator from the video. After viewing the line-up , each participant could make one of the following decisions (id):\n\ncorrect: correctly identified the true perpetrator\nfoil: incorrectly identified the “foil”, i.e. a person who looks very similar to the perpetrator\nreject: incorrectly concluded the true perpetrator is not in the lineup\n\nThe main objective of the analysis is to understand how different conditions of the mock crime and suspect lineup affect the decision made by the participant. We will consider the following conditions to describe the decisions:\n\nlineup: How potential suspects are shown to the participants\n\nSimultaneous Lineup: Participants were shown photos of all 6 potential suspects at the same time and were required to make a single decision (identify someone from the lineup or reject the lineup).\nSequential 5 Lineup: Photos of the 6 suspects were shown one at a time. The participant was required to make a decision (choose or don’t choose) as each photo was shown. Once a decision was made, participants were not allowed to reexamine a photo. If the participant made an identification, the remaining photos were not shown. In each of these lineups the true perpetrator was always the 5th photo in the lineup.\n\nweapon: Whether or not a weapon was present in the video of the mock crime.\nfeature: Whether or not the perpetrator had a distinctive marking on his face. In this experiment, the distinctive feature was a large “N” sticker on one cheek. (The letter “N” was chosen to represent the first author’s alma mater - University of Nebraska.)\n\nThe data may be found in eyewitness.csv in the data folder.\n\new <- read_csv(here::here(\"hw\", \"data/eyewitness.csv\"))\new <- ew %>%\n  mutate(id = as_factor(id))"
  },
  {
    "objectID": "hw/hw-4.html#exercises",
    "href": "hw/hw-4.html#exercises",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Exercises",
    "text": "Exercises\n\nLet’s begin by doing some exploratory data analysis. The univariate (single variable) plots for each of the predictor variables and the response variable are shown below.\n\n\n\n\n\n\nComplete the exploratory data analysis by creating the plots and/or summary statistics to examine the relationship between the response variable (id) and each of the explanatory variables (lineup, weapon, and feature).\n\nUsing the plots/tables from Exercise 1:\n\n\nWhat is one thing you learn about the data from the univariate plots?\nBased on the bivariate plots, do any of the predictors appear to have a significant effect on the id? Briefly explain.\n\n\nBriefly explain why you should use a multinomial logistic regression model to predict id using lineup, weapon and feature.\nFit the multinomial logistic model that only includes main effects. Display the model output.\n\n\nWhat is the baseline category for the response variable?\nInterpret the intercepts for each part of the model in terms of the odds.\nInterpret the coefficients of lineup for each part of the model in terms of the odds.\n\n\nYou want to consider all possible first-order interaction effects (interaction effects between two variables) for the model.\n\n\nUse the appropriate test to determine if there is at least one significant interaction effect.\nBased on your test, is there evidence of any significant interaction effects?\n\nRegardless of your answer to Question 5, use the model that includes the interaction terms for the remainder of the assignment.\n\nAccording to the model,\n\n\nIf there was no weapon but the perpetrator had a distinctive feature in the mock crime, how do the log-odds of reject vs. a correct ID change when there is a simultaneous lineup vs. a sequential lineup?\nIf there was no weapon but the perpetrator had a distinctive feature in the mock crime, how do the odds of reject vs. a correct ID change when there is a simultaneous lineup vs. a sequential lineup?\nWhich group of participants (i.e., which set of experimental conditions) is described by the intercept?\n\n\nAre the conditions inference met? List of the conditions, and, if relevant, create visualizations to check the conditions and evaluate whether each condition is met. Include an assessment about each condition and a brief explanation about your conclusion.\nUse the model to predict the decision made by each participant. Make a table of the predicted vs. the actual decisions.\n\n\nBriefly describe how the predicted decision is determined for each participant.\nWhat is the misclassification rate?"
  },
  {
    "objectID": "hw/hw-4.html#submission",
    "href": "hw/hw-4.html#submission",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "hw/hw-4.html#grading",
    "href": "hw/hw-4.html#grading",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nExercises\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "hw/hw-1.html",
    "href": "hw/hw-1.html",
    "title": "HW 1 - In-person voting trends",
    "section": "",
    "text": "In this assignment, you’ll use simple linear regression to explore the percent of votes cast in-person in the 2020 U.S. election based on the county’s political leanings.\n\n\nIn this assignment, you will…\n\nFit and interpret simple linear regression models\nAssess the conditions for simple linear regression.\nCreate and interpret spatial data visualizations using R.\nContinue developing a workflow for reproducible data analysis."
  },
  {
    "objectID": "hw/hw-1.html#getting-started",
    "href": "hw/hw-1.html#getting-started",
    "title": "HW 1 - In-person voting trends",
    "section": "Getting started",
    "text": "Getting started\n\nLog in to RStudio\n\nGo to https://vm-manage.oit.duke.edu/containers and login with your Duke NetID and Password.\nClick STA210 to log into the Docker container. You should now see the RStudio environment.\n\n\n\nClone the repo & start new RStudio project\n\nGo to the course organization at github.com/sta210-s22 organization on GitHub. Click on the repo with the prefix hw-1. It contains the starter documents you need to complete the lab.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\nIn RStudio, go to File ➛ New Project ➛Version Control ➛ Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick hw-1-voting.qmd to open the template R Markdown file. This is where you will write up your code and narrative for the lab."
  },
  {
    "objectID": "hw/hw-1.html#packages",
    "href": "hw/hw-1.html#packages",
    "title": "HW 1 - In-person voting trends",
    "section": "Packages",
    "text": "Packages\nThe following packages will be used in this assignment:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(scales)"
  },
  {
    "objectID": "hw/hw-1.html#data-2020-election",
    "href": "hw/hw-1.html#data-2020-election",
    "title": "HW 1 - In-person voting trends",
    "section": "Data: 2020 Election",
    "text": "Data: 2020 Election\nThere are multiple data sets for this assignment. Use the code below to load the data.\n\nelection_nc <- read_csv(\"data/nc-election-2020.csv\") %>%\n  mutate(fips = as.integer(FIPS))\ncounty_map_data <-  read_csv(\"data/nc-county-map-data.csv\")\nelection_sample <- read_csv(\"data/us-election-2020-sample.csv\")\n\nThe county-level election data in election_nc and election_sample are from The Economist GitHub repo. The data were originally analyzed in the July 2021 article In-person voting really did accelerate covid-19’s spread in America. For this analysis, we will focus on the following variables:\n\ninperson_pct: The proportion of a county’s votes cast in-person in the 2020 election\npctTrump_2016: The proportion of a county’s votes cast for Donald Trump in the 2016 election\n\nThe data in county_map_data were obtained from the maps package in R. We will not analyze any of the variables in this data set but will use it to help create maps in the assignment. Click here to see the documentation for the maps package. Click here for code examples."
  },
  {
    "objectID": "hw/hw-1.html#exercises",
    "href": "hw/hw-1.html#exercises",
    "title": "HW 1 - In-person voting trends",
    "section": "Exercises",
    "text": "Exercises\nDue to COVID-19 pandemic, many states made alternatives in-person voting, such as voting by mail, more widely available for the 2020 U.S. election. The general consensus was that voters who were more Democratic leaning would be more likely to vote by mail, while more Republican leaning voters would largely vote in-person. This was supported by multiple surveys, including this survey conducted by Pew Research.\nThe goal of this analysis is to use regression analysis to explore the relationship between a county’s political leanings and the proportion of votes cast in-person in 2020. The ultimate question we want to answer is “Did counties with more Republican leanings have a larger proportion of votes cast in-person in the 2020 election?”\nWe will use the proportion of votes cast for Donald Trump in 2016 (pctTrump_2016) as a measure of a county’s political leaning. Counties with a higher proportion of votes for Trump in 2016 are considered to have more Republican leanings.\n\n\n\n\n\n\nNote\n\n\n\nAll narrative should be written in complete sentences, and all visualizations should have informative titles and axis labels.\n\n\n\nPart 1: Counties in North Carolina\nFor this part of the analysis, we will focus on counties in North Carolina. We will use the data sets election_nc and county_map_data.\n\nVisualize the distribution of the response variable inperson_pct and calculate appropriate summary statistics. Use the visualization and summary statistics to describe the distribution. Include an informative title and axis labels on the plot.\nLet’s view the data in another way. Use the code below to make a map of North Carolina with the color of each county filled in based on the percentage of votes cast in-person in the 2020 election. Fill in title and axis labels.\nThen use the plot answer the following:\n\nWhat are 2 - 3 observations you have from the plot?\nWhat is a feature that is apparent in the map that wasn’t apparent from the histogram in the previous exercise? What is a feature that is apparent in the histogram that is not apparent in the map?\n\n\n\nelection_map_data <- left_join(election_nc, county_map_data)\n\nggplot() +\n  geom_polygon(data = county_map_data,\n    mapping = aes(x = long, y = lat, group = group),\n    fill = \"lightgray\", color = \"white\"\n    ) +\n  geom_polygon(data = election_map_data, \n    mapping = aes(x = long, y = lat, group = group,\n    fill = inperson_pct)\n    ) +\n  labs(\n    x = \"___\",\n    y = \"___\",\n    fill = \"___\",\n    title = \"___\"\n  ) +\n  scale_fill_viridis_c(labels = label_percent(scale = 1)) +\n  coord_quickmap()\n\n\nCreate a visualization of the relationship between inperson_pct and pctTrump_2016. Use the visualization to describe the relationship between the two variables.\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you haven’t yet done so, now is a good time to render your document and commit (with a meaningful commit message) and push all updates.\n\n\n\nWe can use a linear regression model to better quantify the relationship between the variables.\n\nFit the linear model to understand variability in the percent of in-person votes based on the percent of votes for Trump in the 2016 election. Neatly display the model output with 3 digits.\nWrite the regression equation using mathematical notation.\n\nNow let’s use the model coefficients to describe the relationship.\n\nInterpret the slope. The interpretation should be written in a way that is meaningful in the context of the data.\nDoes it make sense to interpret the intercept? If so, write the interpretation in the context of the data. Otherwise, briefly explain why not.\n\nIf the linear model is a good fit to these data, there should be no structure left in the residuals and the residuals should have constant variance. Augment the data with the model to obtain the residuals and predicted values for each observation, and call the augmented data frame nc_election_aug (You will use this name in Exercise 8). Then, make a plot of the residuals vs. the fitted values, and based on this plot, and provide a brief explanation for whether these two conditions are met. Hint: Zoom out on the plot by extending the limits of the y-axis.\n\n\n\n\n\n\n\nWarning\n\n\n\nNow is a good time to render your document again if you haven’t done so recently and commit (with a meaningful commit message) and push all updates.\n\n\n\nWe might also be interested in our observations being independent, particularly if we are to use these data for inference. To evaluate whether the independence condition is met, we will examine a map of the counties in North Carolina with the color filled based on the value of the residuals.\n\nBriefly explain why we may want to view the residuals on a map to assess independence.\nBriefly explain what pattern (if any) we would expect to observe on the map if the independence condition is satisfied.\n\nFill in the name of your model in the code below to calculate the residuals and add them to election_map_data. Then, a map with the color of each county filled in based on the value of the residual. Hint: Start with the code from Exercise 2.\nIs the independence condition satisfied? Briefly explain based on what you observe from the plot.\n\nnc_election_aug <- nc_election_aug %>% \n  bind_cols(fips = election_nc$fips)\n\nelection_map_data <- left_join(election_map_data, nc_election_aug)\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBefore moving on to the next part, make sure you render your document and commit (with a meaningful commit message) and push all updates.\n\n\n\n\nPart 2: Inference for the U.S.\nTo get a better understanding of the trend across the entire United States, we analyze data from a random sample of 200 counties. This data is in the election_sample data frame. Because these counties were randomly selected out of the 3,006 counties in the United States, we can reasonably treat the counties as independent observations.\n\nFit the linear model to these sample data to understand variability in the percent of in-person votes based on the percent of votes for Trump in the 2016 election. Neatly display the model output with 3 digits.\nConduct a hypothesis test for the slope using a permutation test. In your response, state the null and alternative hypotheses in words, and state the conclusion in the context of the data.\nNext, construct a 95% confidence interval for the slope using bootstrapping. Interpret the confidence interval in the context of the data.\nComment on whether the hypothesis test and confidence interval support the general consensus that Republican voters were more likely to vote in-person in the 2020 election? A brief explanation is sufficient but it should be based on your conclusions from Exercises 10 and 11.\n\n\n\n\n\n\n\nWarning\n\n\n\nBefore submitting, make sure you render your document and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw-1.html#submission",
    "href": "hw/hw-1.html#submission",
    "title": "HW 1 - In-person voting trends",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "hw/hw-1.html#grading",
    "href": "hw/hw-1.html#grading",
    "title": "HW 1 - In-person voting trends",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "hw/hw-2.html",
    "href": "hw/hw-2.html",
    "title": "HW 2 - Multiple linear regression",
    "section": "",
    "text": "In this assignment, you’ll get to put into practice the multiple linear regression skills you’ve developed.\n\n\nIn this assignment, you will…\n\nFit and interpret multiple linear regression models with main and interaction effects.\nCompare multiple linear regression models.\nReason around multiple linear regression concepts.\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-2. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "hw/hw-2.html#part-1---conceptual",
    "href": "hw/hw-2.html#part-1---conceptual",
    "title": "HW 2 - Multiple linear regression",
    "section": "Part 1 - Conceptual",
    "text": "Part 1 - Conceptual\n\nDealing with categorical predictors. Two friends, Elliott and Adrian, want to build a model predicting typing speed (average number of words typed per minute) from whether the person wears glasses or not. Before building the model they want to conduct some exploratory analysis to evaluate the strength of the association between these two variables, but they’re in disagreement about how to evaluate how strongly a categorical predictor is associated with a numerical outcome. Elliott claims that it is not possible to calculate a correlation coefficient to summarize the relationship between a categorical predictor and a numerical outcome, however they’re not sure what a better alternative is. Adrian claims that you can recode a binary predictor as a 0/1 variable (assign one level to be 0 and the other to be 1), thus converting it to a numerical variable. According to Adrian, you can then calculate the correlation coefficient between the predictor and the outcome. Who is right: Elliott or Adrian? If you pick Elliott, can you suggest a better alternative for evaluating the association between the categorical predictor and the numerical outcome?\nHigh correlation, good or bad? Two friends, Frances and Annika, are in disagreement about whether high correlation values are always good in the context of regression. Frances claims that it’s desirable for all variables in the dataset to be highly correlated to each other when building linear models. Annika claims that while it’s desirable for each of the predictors to be highly correlated with the outcome, it is not desirable for the predictors to be highly correlated with each other. Who is right: Frances, Annika, both, or neither? Explain your reasoning using appropriate terminology.\nTraining for the 5K. Nico signs up for a 5K (a 5,000 metre running race) 30 days prior to the race. They decide to run a 5K every day to train for it, and each day they record the following information: days_since_start (number of days since starting training), days_till_race (number of days left until the race), mood (poor, good, awesome), tiredness (1-not tired to 10-very tired), and time (time it takes to run 5K, recorded as mm:ss). Top few rows of the data they collect is shown below.\n\n\n\ndays_since_start\ndays_till_race\nmood\ntiredness\ntime\n\n\n\n\n1\n29\ngood\n3\n25:45\n\n\n2\n28\npoor\n5\n27:13\n\n\n3\n27\nawesome\n4\n24:13\n\n\n…\n…\n…\n…\n…\n\n\n\nUsing these data Nico wants to build a model predicting time from the other variables. Should they include all variables shown above in their model? Why or why not?\nMultiple regression fact checking. Determine which of the following statements are true and false. For each statement that is false, explain why it is false.\n\nIf predictors are colinear, then removing one variable will have no influence on the point estimate of another variable’s coefficient.\nSuppose a numerical predictor \\(x\\) has a coefficient of \\(\\hat{\\beta}_1 = 2.5\\) in a multiple regression model. Suppose also that the first observation has \\(x_{1,1} = 7.2\\), the second observation has a value of \\(x_{2,1} = 8.2\\), and these two observations have the same values for all other predictors. Then the predicted value of the second observation will be 2.5 higher than the prediction of the first observation based on the multiple regression model.\nIf a regression model’s first predictor has a coefficient of \\(\\hat{\\beta}_1 = 5.7\\) and if we are able to influence the data so that an observation will have its \\(x_1\\) be 1 larger than it would otherwise, the value \\(\\hat{y}_1\\) for this observation would increase by 5.7."
  },
  {
    "objectID": "hw/hw-2.html#part-2---palmer-penguins",
    "href": "hw/hw-2.html#part-2---palmer-penguins",
    "title": "HW 2 - Multiple linear regression",
    "section": "Part 2 - Palmer penguins",
    "text": "Part 2 - Palmer penguins\nData were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. (Gorman, Williams, and Fraser 2014)\n\n\n\nArtwork by @allison_horst\n\n\nThese data can be found in the palmerpenguins package. We’re going to be working with the penguins dataset from this package. The dataset contains data for 344 penguins. There are 3 different species of penguins in this dataset, collected from 3 islands in the Palmer Archipelago, Antarctica.\n\nBody mass. Our first goal is to fit a model predicting body mass (which is more difficult to measure) from bill length, bill depth, flipper length, species, and sex.\n\nFit a model predicting body mass (which is more difficult to measure) from the other variables listed above.\nWrite the equation of the regression model.\nInterpret each one of the slopes in this context.\nCalculate the residual for a male Adelie penguin that weighs 3750 grams with the following body measurements: bill_length_mm = 39.1, bill_depth_mm = 18.7, flipper_length_mm = 181. Does the model overpredict or underpredict this penguin’s weight?\nFind the \\(R^2\\) of this model and interpret this value in context of the data and the model.\n\n\n\n\nBill depth. Next we’ll be focusing on bill depth and bill length and also considering species.\n\nFit a model predicting bill depth from bill length. Find the adjusted R-squared, AIC, and BIC for this model.\nThen, add a new predictor: species. Fit another model predicting bill depth from bill length and species. Find the adjusted R-squared, AIC, and BIC for this model.\nFinally, add one more predictor: the interaction between bill length and species. Find the adjusted R-squared, AIC, and BIC for this model.\nUsing the three criteria you recorded for these three models, and with the goal of parsimony, which model is the “best” for predicting bill depth from bill length and/or species. Explain your reasoning.\nCreate a visualization representing your model from part a. Hint: Make a scatterplot of bill depth vs. bill length and add the linear model.\nCreate a visualization representing your model from part b. Hint: Same as part (e), but think about how many lines to plot and whether their slopes should be the same or different.\nCreate a visualization representing your model from part c. Hint: Same as part (f), but think about how many lines to plot and whether their slopes should be the same or different.\nBased on your visualizations from parts e - g, and with the goal of parsimony, is your answer for which model is the “best” for predicting bill depth from bill length and/or species the same as your answer in part d? Explain your reasoning."
  },
  {
    "objectID": "hw/hw-2.html#part-3---perceived-threat-of-covid-19",
    "href": "hw/hw-2.html#part-3---perceived-threat-of-covid-19",
    "title": "HW 2 - Multiple linear regression",
    "section": "Part 3 - Perceived threat of Covid-19",
    "text": "Part 3 - Perceived threat of Covid-19\nGarbe, Rau, and Toppe (2020), published in June 2020, aims to examine the relationship between personality traits, perceived threat of Covid-19 and stockpiling toilet paper. For this study titled Influence of perceived threat of Covid-19 and HEXACO personality traits on toilet paper stockpiling, researchers conducted an online survey March 23 - 29, 2020 and used the results to fit multiple linear regression models to draw conclusions about their research questions. From their survey, they collected data on adults across 35 countries. Given the small number of responses from people outside of the United States, Canada, and Europe, only responses from people in these three locations were included in the regression analysis.\nLet’s consider their results for the model looking at the effect on perceived threat of Covid-19. The model can be found on page 6 of the paper. The perceived threat of Covid was quantified using the responses to the following survey question:\n\nHow threatened do you feel by Coronavirus? [Users select on a 10-point visual analogue scale (Not at all threatened to Extremely Threatened)]\n\n\nInterpret the coefficient of Age (0.072) in the context of the analysis.\nInterpret the coefficient of Place of residence in the context of the analysis.\nThe model includes an interaction between Place of residence and Emotionality (capturing differential tendencies in to worry and be anxious).\n\nWhat does the coefficient for the interaction (0.101) mean in the context of the data?\nInterpret the estimated effect of Emotionality for a person who lives in the US/Canada.\nInterpret the estimated effect of Emotionality for a person who lives in Europe."
  },
  {
    "objectID": "hw/hw-2.html#submission",
    "href": "hw/hw-2.html#submission",
    "title": "HW 2 - Multiple linear regression",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "hw/hw-2.html#grading",
    "href": "hw/hw-2.html#grading",
    "title": "HW 2 - Multiple linear regression",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 9\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "hw/hw-3.html",
    "href": "hw/hw-3.html",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "",
    "text": "In this assignment, you’ll get to put into practice the logistic regression skills you’ve developed.\n\n\nIn this assignment, you will…\n\nFit and interpret logistic regression models.\nFit and interpret multiple linear regression models with log transformed outcomes.\nReason around log transformations of various types.\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-3. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "hw/hw-3.html#part-1---palmer-penguins",
    "href": "hw/hw-3.html#part-1---palmer-penguins",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Part 1 - Palmer penguins",
    "text": "Part 1 - Palmer penguins\nIn this part we’ll go back to the Palmer penguins dataset from HW 2.\nWe will use the following variables:\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nspecies\ninteger\nPenguin species (Adelie, Gentoo, Chinstrap)\n\n\nisland\ninteger\nIsland where recorded (Biscoe, Dream, Torgersen)\n\n\nflipper_length_mm\ninteger\nFlipper length in mm\n\n\n\nThe goal of this analysis is to use logistic regression to understand the relationship between flipper length, island, and whether a penguin is from the Adelie species. First, we need to create a new response variable to identify whether a penguin is from the Adelie species.\n\npenguins <- penguins %>%\n  mutate(adelie = factor(if_else(species == \"Adelie\", 1, 0)))\n\nAnd let’s check to make sure the new variable looks right before we continue with the analysis.\n\npenguins %>%\n  count(adelie, species)\n\n# A tibble: 3 × 3\n  adelie species       n\n  <fct>  <fct>     <int>\n1 0      Chinstrap    68\n2 0      Gentoo      124\n3 1      Adelie      152\n\n\nLet’s start by looking at the relationship between island and whether a penguin is from the Adelie species.\n\nWhat does the values_fill argument do in the following chunk? The documentation for the function will be helpful in answering this question.\n\npenguins %>%\n  count(island, adelie) %>%\n  pivot_wider(names_from = adelie, values_from = n, values_fill = 0)\n\n# A tibble: 3 × 3\n  island      `0`   `1`\n  <fct>     <int> <int>\n1 Biscoe      124    44\n2 Dream        68    56\n3 Torgersen     0    52\n\n\nCalculate the odds ratio of a penguin being from the Adelie species for those recorded on Dream compared to those recorded on Biscoe.\nYou want to fit a model using island to predict the odds of being from the Adelie species. Let \\(\\pi\\) be the probability a penguin is from the Adelie species. The model has the following form. What do you expect the value of \\(\\hat{\\beta}_1\\), the estimated coefficient for Dream, to be? Explain your reasoning.\n\n\\[\n\\log\\Big(\\frac{\\pi}{1-\\pi}\\Big) = \\beta_0 + \\beta_1 ~ Dream + \\beta_2 ~ Torgersen\n\\]\n\nFit a model predicting adelie from island and display the model output. For the following exercise, use this model.\nBased on this model, what are the odds of a penguin being from the Adelie species if it was recorded on Biscoe island? on Dream island?\nNext, add flipper length to the model so that there are two predictors. Display the model output. For the following exercises, use this model.\nWrite the regression equation for the model.\nInterpret the coefficient of flipper_length_mm in terms of the log-odds of being from the Adelie species.\nInterpret the coefficient of flipper_length_mm in terms of the odds of being from the Adelie species.\nInterpret the coefficient of Dream in terms of the odds of being from the Adelie species.\nHow do you expect the log-odds of being from the Adelie species to change when going from a penguin with flipper length 185 mm to a penguin with flipper length 200 mm? Assume both penguins were recorded on the Dream island.\nHow do you expect the odds of being from the Adelie species to change when going from a penguin with flipper length 185 mm to a penguin with flipper length 200 mm? Assume both penguins were recorded on the Dream island."
  },
  {
    "objectID": "hw/hw-3.html#part-2---gdp-and-urban-population",
    "href": "hw/hw-3.html#part-2---gdp-and-urban-population",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Part 2 - GDP and Urban population",
    "text": "Part 2 - GDP and Urban population\nData on countries’ Gross Domestic Product (GDP) and percentage of urban population was collected and made available by The World Bank in 2020. A description of the variables as defined by The World Bank are provided below.\n\nGDP: “GDP per capita is gross domestic product divided by midyear population. GDP is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products. It is calculated without making deductions for depreciation of fabricated assets or for depletion and degradation of natural resources. Data are in current U.S. dollars.”\nUrban Population (% of total): “Urban population refers to people living in urban areas as defined by national statistical offices. It is calculated using World Bank population estimates and urban ratios from the United Nations World Urbanization Prospects.”\n\nThe data can be found in the data folder of your repository. Read the data and name it gdp_2020.\n\nFit a model predicting GDP from urban population. Then make a plot of residuals vs. fitted for this model. Does the linear model seem appropriate for modeling this relationship? Explain your reasoning.\nAdd a new column to the gdp_2020 dataset called gdp_log which is the (natural) log of gdp.\nFit a new model, predicting the log of GDP from urban population. Then make a plot of residuals vs. fitted for this model. Does the model predicting logged GDP or original GDP appear to be a better fit? Explain your reasoning.\n\nThe model output for predicting logged GDP.\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n6.107\n0.202\n30.291\n0\n\n\nurban\n0.042\n0.003\n13.769\n0\n\n\n\n\n\nThe linear model for predicting log of GDP can be expressed as follows:\n\\[\n\\widehat{\\log(GDP)} = 6.11 + 0.042 \\times urban\n\\]\nTherefore, the coefficient of urban (0.042) can be interpreted as the change in logged GDP associated with 1 percentage point increase in urban population. The problem is, logged GDP is not a very informative value to talk about. So we need to undo the transformation we’ve done.\nTo do so, let’s do a quick review of some properties of logs.\n\nSubtraction and logs: \\(log(a) − log(b) = log(\\frac{a}{b})\\)\nNatural logarithm: \\(e^{log(x)} = x\\)\n\nBased on the interpretation of the slope above, the difference between the predicted values of logged GDP for a given value of urban and a value that is 1 percentage point higher is 0.0425. Let’s write this out mathematically, and then use the properties we’ve listed above to work through the equation.\n\\[\n\\begin{aligned}\nlog(\\text{GDP for urban } x + 1) - log(\\text{GDP for urban } x) &= 0.042 \\\\\nlog\\Big( \\frac{\\text{GDP for urban } x + 1}{\\text{GDP for urban } x} \\Big) &= 0.042 \\\\\ne^{log\\Big( \\frac{\\text{GDP for urban } x + 1}{\\text{GDP for urban } x} \\Big)} &= e^{0.042}\\\\\n\\frac{\\text{GDP for urban } x + 1}{\\text{GDP for urban } x} &= e^{0.042}\n\\end{aligned}\n\\]\n\nBased on the derivation above, fill in the blanks in the following sentence for an alternative (and more useful interpretation) of the slope of urban.\n\nFor each additional percentage point the urban population is higher, the GDP of a country is expected to be ___, on average, by a factor of ___."
  },
  {
    "objectID": "hw/hw-3.html#submission",
    "href": "hw/hw-3.html#submission",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "hw/hw-3.html#grading",
    "href": "hw/hw-3.html#grading",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 9\n45\n\n\nWorkflow & formatting\n51"
  }
]